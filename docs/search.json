[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! My name is Liam and I’m a senior at Middlebury College studying geography and mathematics. This blog documents my work in CSCI 0451: Machine Learning."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset contains anatomical measurements for \\(344\\) Penguins of \\(3\\) different species and is commonly used when introducing students to new data science concepts. In this blog post, we construct our first classification model in Python, attempting to accurately predict the species of a given observation in the Palmer Penguins dataset. Constrained to one qualitative and two quantitative predictors, we create our model using the scikit-learn package’s out-of-the-box logistic regression functionality. We select features by conducting 5-fold cross-validation on logistic regression models constructed from every possible combination of one qualitative and two quantitative variables. We implement our two models with the highest cross-validation score, selecting one that achieves 100% test data accuracy as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#abstract",
    "href": "posts/palmer-penguins/index.html#abstract",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset contains anatomical measurements for \\(344\\) Penguins of \\(3\\) different species and is commonly used when introducing students to new data science concepts. In this blog post, we construct our first classification model in Python, attempting to accurately predict the species of a given observation in the Palmer Penguins dataset. Constrained to one qualitative and two quantitative predictors, we create our model using the scikit-learn package’s out-of-the-box logistic regression functionality. We select features by conducting 5-fold cross-validation on logistic regression models constructed from every possible combination of one qualitative and two quantitative variables. We implement our two models with the highest cross-validation score, selecting one that achieves 100% test data accuracy as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-preparation",
    "href": "posts/palmer-penguins/index.html#data-preparation",
    "title": "Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nMost of the code in this section was conveniently provided by Professor Chodrow.\n\n# Load packages \nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport warnings\n\n# Change pd display parameter\npd.set_option('display.max_colwidth', 100)\n\n# Supress warnings to improve web aesthetics\nwarnings.filterwarnings(\"ignore\")\n\n# Load training data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n# Data preparation\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  x = df.drop([\"Species\"], axis = 1)\n  x = pd.get_dummies(x)\n  return x, y, df\n\nX_train, y_train, viz_train = prepare_data(train)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-exploration",
    "href": "posts/palmer-penguins/index.html#data-exploration",
    "title": "Palmer Penguins",
    "section": "Data Exploration",
    "text": "Data Exploration\nBefore constructing a classification model, it is often helpful to get a better feel for the data you are working with. For this reason, we begin our analysis by creating and analyzing a few figures.\n\n# Display plots\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\n\nsns.scatterplot(viz_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", ax = ax[0]);\nsns.move_legend(ax[0], \"lower right\")\nsns.scatterplot(viz_train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\", hue = \"Species\", ax = ax[1]);\nsns.move_legend(ax[1], \"lower right\")\n\n\n\n\n\n\n\n\nIn the first scatterplot, we have almost perfectly separated the three species of penguin using just two variables: flipper length and culmen length. The Adelie penguins tend to have low values of both variables, the Chinstrap penguins tend to have large culmen lengths but small flipper lengths, and the Gentoo penguins tend to have medium culmen lengths and large flipper lengths. One can imagine drawing straight lines in this graph that almost perfectly separate the three species of penguin. The fact that we can visually distinguish between the different species of penguins makes me very confident about our prospects for classification.\nIn the second scatterplot, the Gentoo penguins tend to have small culmen depths and large body masses, separating them from the other two species with some white-space to spare. However, the distributions of Adelie and Chinstrap points look almost identical, occupying the same lower right corner of the plot. If our goal was simply to predict whether a penguin is a Gentoo penguin, then I would imagine that these two variables could help make a strong classifier. However, since we seek to distinguish between all three species, these variables might not be the best choice.\nWhile body mass (or body mass and culmen depth) alone is insufficient to distinguish between the species, perhaps accounting for a categorical variable like sex will reveal some patterns. To explore this possibility, we include the following table of median body mass.\n\n# Display table\nviz_train.groupby(['Sex', 'Species'])[['Body Mass (g)']].median().unstack()\n\n\n\n\n\n\n\n\nBody Mass (g)\n\n\nSpecies\nAdelie\nChinstrap\nGentoo\n\n\nSex\n\n\n\n\n\n\n\nFEMALE\n3350.0\n3525.0\n4700.0\n\n\nMALE\n4025.0\n4050.0\n5500.0\n\n\n\n\n\n\n\nFrom this table, it is clear that the median body masses of all three species vary by sex, with the males tending to weigh more than the females. Additionally, while we already knew from our scatterplot that the Gentoo penguins tend to weigh more than the other species, this table reinforces that conclusion and shows it to be true for both males and females. Unfortunately, accounting for sex appears to do little to further distinguish between Adelie and Chinstrap penguins. There is roughly a 200 gram difference between the median mass of female Adelie and Chinstrap penguins, so perhaps sex in can help distinguish between the two species given that a penguin is female. However, the median mass of male Adelie and Chinstrap penguins differs only by 25 grams. A classifier would almost certainly need more information in order to make accurate predictions."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-construction",
    "href": "posts/palmer-penguins/index.html#model-construction",
    "title": "Palmer Penguins",
    "section": "Model Construction",
    "text": "Model Construction\nSince the Palmer Penguins dataset is relatively small and we are given constraints on the number of features we may include in our model, we will select features by fitting a classifier for every possible combination of one qualitative and two quantitative variables. There may be more robust and less computationally expensive methods of feature selection, but for our purposes, this works.\nWe implement logistic regression using scikit-learn’s out-of-the-box logistic-regression classifier.\n\n# Choosing features\nfrom itertools import combinations\n\n# Distinguish between qualitative and quantitative variables\nall_qual_cols = ['Clutch Completion', 'Sex', 'Island', 'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n# Define classifier\nLR = LogisticRegression(max_iter=100000000)\n\n# Initialize dataframe to store data\nmodels = pd.DataFrame(columns = [\"CV Score\", \"Columns\"])\n\n# Find best-performing classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    cv_score = cv_scores_LR.mean()\n    \n    model = pd.DataFrame({\"CV Score\": [cv_score], \"Columns\": [cols]})\n\n    models = pd.concat([models, model])\n\n\n# Sort by CV Score and display best-performing models\nmodels = models.sort_values(\"CV Score\", ascending=False)\nmodels.head()\n\n\n\n\n\n\n\n\nCV Score\nColumns\n\n\n\n\n0\n0.988311\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\n\n\n0\n0.988311\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\n\n\n0\n0.980543\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Body Mass (g)]\n\n\n0\n0.968854\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Flipper Length (mm)]\n\n\n0\n0.968778\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Delta 13 C (o/oo)]\n\n\n\n\n\n\n\nLet’s fit the model with the highest cross-validation score! There are actually two models with equal performance, so we will try both.\n\n# Test the model\n\n# Import data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Adjust species label\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n# Data preparation\nX_test, y_test, viz_test = prepare_data(test)\n\n# Fit and output the performance of the model\nmodel1 = LR.fit(X_train[models.iloc[[0]]['Columns'][0]], y_train)\nmodel1_score = LR.score(X_test[models.iloc[[0]]['Columns'][0]], y_test)\nmodel1_score\n\n1.0\n\n\nVoila! Logistic regression using the Culmen Length (mm), Culmen Depth (mm), and Island features correctly predicts every observation in our test data, achieving our goal.\n\n# Test the next model\nmodel2 = LR.fit(X_train[models.iloc[[1]]['Columns'][0]], y_train)\nmodel2_score = LR.score(X_test[models.iloc[[1]]['Columns'][0]], y_test)\nmodel2_score\n\n0.9852941176470589\n\n\nOn the other hand, logistic regression using the Culmen Length (mm), Culmen Depth (mm), and Sex features does not achieve 100% accuracy on the test data. Since our first model achieved 100% test accuracy, we select it as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-evaluation",
    "href": "posts/palmer-penguins/index.html#model-evaluation",
    "title": "Palmer Penguins",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nAt this point, we have constructed a well-performing model, so we proceed to model evaluation. First, we visualize our model’s decision regions using the plot_regions() function provided by Professor Chodrow.\n\n# Plotting decision regions function, generously provided by Professor Chodrow\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (10, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout(rect = (0,0,.95,.95))\n\n\n# Reverse the order of columns for the purposes of visualization\nmodels.iloc[[0]]['Columns'][0].reverse()\n\n# Refit model\nLR.fit(X_train[models.iloc[[0]]['Columns'][0]], y_train)\n\n# Plot decision regions on training data\nplot_regions(LR, X_train[models.iloc[[0]]['Columns'][0]], y_train)\nplt.suptitle(\"Decision Regions and Training Data\", fontsize = 13)\n\n# # Plot decision regions on test data\nplot_regions(LR, X_test[models.iloc[[0]]['Columns'][0]], y_test)\nplt.suptitle(\"Decision Regions and Test Data\", fontsize = 13);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the upper row of figures, we plot the training data in our decision regions, while in the lower row, we plot the test data in the decision regions. We trained a model on our training data and implemented it on our test data, so the decision regions are identical in the two rows of figures. The only difference between the rows is the points within the decision regions. Since most observations are part of the training data, there are many more points in the first row of figures. It is interesting to note that Torgersen Island only contains Gentoo penguins, Dream Island does not contain any Adelie penguins, and Biscoe Island does not contain any Chinstrap penguins. These patterns makes it substantially easier to predict a given penguin’s species. One could imagine that the model would always predict Gentoo penguins on Torgersen Island. Similarly, one could imagine the model never predicting Adelie penguins on Dream Island and never predicting Chinstrap penguins on Biscoe Island. Interestingly, the model actually fits decision regions for all three penguin varieties in each island. The decision region for a given species tends to be larger on islands where we would expect to find them, but the model allows for the possibility that there might be a penguin on an island where we would not expect one. Overall, the logistic regression model does a great job of incorporating both qualitative and quantitative variables into its decision regions.\n\n# Predict\nmodel1_pred = model1.predict(X_test[models.iloc[[0]]['Columns'][0]])\n\n# Confusion matrix\nC = confusion_matrix(y_test, model1_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nSince all of the predictions are on the main diagonal of the confusion matrix, we conclude that we did not misclassify any observations – but we already knew this, since our model achieved 100% test accuracy! If our model had not perfectly classified the observations in our test data, a confusion matrix would be more helpful for assessing our model’s misclassification tendencies."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#discussion",
    "href": "posts/palmer-penguins/index.html#discussion",
    "title": "Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, we began by exploring the Palmer Penguins dataset. Our scatterplots and table illustrated that for some variables, there is substantial variation between different species of penguin, while for others, there is very little. Determining which variables would be most valuable for classification required more than simple visualizations. In order to robustly determine the best combination of one qualitative and two quantitative varibles, I fit a logistic regression model on every possible combination. This process was relatively slow and may not work at scale, but it was successful in generating a highly accurate model in this scenario. My final model used the Culmen Length (mm), Culmen Depth (mm), and Island features, achieving 100% accurate predictions on the test data.\nThis assignment furthered my understanding of the entire model-construction procedure, including data preparation, data exploration, feature selection, model fitting, and model evaluation. After completing this blog post, I feel more confident about my ability to implement standard machine learning procedures from Python packages like scikit-learn. While it is relatively easy to implement out-of-the-box classifiers from scikit-learn, it may be more difficult to understand their theoretical underpinnings. As I finish this assignment, I am eager to learn more about the mathematics happening under the hood."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Perceptron\n\n\n\n\n\nImplementing one of the oldest machine learning models from scratch.\n\n\n\n\n\nApr 1, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nBlack box classification with the Palmer Penguins dataset.\n\n\n\n\n\nMar 7, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "In this blog post, we implement the perceptron, one of the first machine learning models ever invented, and perform a few experiments to highlight the model’s strengths and weaknesses. The perceptron algorithm is used to predict a binary outcome on data with a continuous and finite-dimensional feature space. Through our experiments, we visually illustrate that perceptron converges to a solution that completely differentiates between the outcome classes on linearly separable data. However, the algorithm does not converge to a solution on non-linearly separable data. We then implement a generalization of perceptron known as minibatch perceptron, which uses \\(k\\) observations in each iteration rather than \\(1\\). We perform experiments that illustrate that minibatch perceptron behaves similarly to perceptron when \\(k = 1\\) and continues to find a decision boundary on linearly separable data as we increase \\(k\\) towards \\(n\\). Finally, we show that when \\(k = n\\), minibatch perceptron can converge to a solution on non-linearly separable data, although this solution will not correctly classify every data point. To see my implementation of the two algorithms, please visit perceptron.py and minibatch_perceptron.py."
  },
  {
    "objectID": "posts/perceptron/index.html#create-linearly-separable-data",
    "href": "posts/perceptron/index.html#create-linearly-separable-data",
    "title": "Implementing Perceptron",
    "section": "Create Linearly Separable Data",
    "text": "Create Linearly Separable Data\nThe perceptron algorithm only converges to solutions on linearly separable data. To demonstrate that my implementation works, I create and display linearly separable data below. Thank you to Professor Chodrow for providing this code.\n\n\nCode\n# define function to create data\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n# define function to plot data\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# create linearly separable data\nX, y = perceptron_data(n_points = 50, noise = 0.3)\n\n# plot linearly separable data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nFigure 1: 300 data points in the 2d plane, each of which has one of two labels."
  },
  {
    "objectID": "posts/perceptron/index.html#part-a-implement-perceptron-and-test-with-minimal-training-loop",
    "href": "posts/perceptron/index.html#part-a-implement-perceptron-and-test-with-minimal-training-loop",
    "title": "Implementing Perceptron",
    "section": "Part A: Implement Perceptron and Test with Minimal Training Loop",
    "text": "Part A: Implement Perceptron and Test with Minimal Training Loop\nWe implement the perceptron algorithm in perceptron.py (LINK IT?), and here we use the algorithm to classify our data. Since the provided minimal training loop terminates, our model must converge to a solution with a loss of 0.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X, y)"
  },
  {
    "objectID": "posts/perceptron/index.html#part-b-experiments",
    "href": "posts/perceptron/index.html#part-b-experiments",
    "title": "Implementing Perceptron",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments\n\nPart B.1: Evolution of the Model on Linearly Separable Data\nIn this section, we seek to investigate the behavior and performance of our model. First, we plot the change in loss over the iterations of the minimal training loop.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAs you can see, our implementation of the perceptron algorithm converges to a solution with zero loss after 49 iterations. Notice that this process only involves 6 changes to the weight vector – the other 43 iterations leave it unchanged.\nTo gain some insight as to the location and orientation of the weight vector throughout this process, we plot the changes to weight vector below. Below, we include one subplot for every change to the weight vector, letting the dashed lines represent the previous weight vector and the solid lines represent the current weight vector. In each subplot, the circled point corresponds to the point \\(i\\) that was misclassified leading to the update in the weight vector.\nThank you to Professor Chodrow for providing this code for this visualization.\n\n\nCode\n# define line function\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# set seed\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i, local_loss = opt.step(X, y)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nDISCUSS?\n\n\nPart B.2: Evolution of the Model on Non-Linearly Separable Data\nUnfortunately, the perceptron algorithm will not converge to a weight vector separating the two classes on data that is not linearly separable. To illustrate this, we first need to create data that cannot be perfectly divided using one separating line.\nTo create our non-linearly separable data, we use the same method as before, but double the amount of noise. With our chosen seed, this results in data where the two categories have clear tendencies towards different regions of the plot, but with enough noise such that no straight line could perfectly differentiate them.\n\n# set seed\ntorch.manual_seed(1234)\n\n# create non-linearly separable data\nX, y = perceptron_data(n_points = 50, noise = 0.6)\n\n# plot non-linearly separable data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nNow that we have created non-linearly separable data, we fit the perceptron model on our data! Since the perceptron will not converge to a solution on our data, we modify our code from above slightly such that the model terminates after 1000 iterations. This is needed to allow our code to terminate.\n\n\nCode\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\n# for recording iteration number\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X, y)\n\n    # update iter\n    iter += 1\n\n    # maxiter condition\n    if iter &gt;= 1000:\n        break\n\n\nAs before, we begin by inspecting the change in loss over the iterations of our algorithm. In the figure below, notice how the loss fluctuates greatly throughout the 1000 iterations we allowed the model to perform. There is no sense of convergence towards a more acceptable solution. The model is so sensitive to the individual data point under consideration at any given moment that the loss fluctuates between almost 0 and 0.5 throughout the entire process.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nEach change in loss corresponds to a change in the weight vector. In the linearly separable case, we visually inspected every change to the weight vector, but because there are so many changes on our non-linearly separable data, it would be burdensome to inspect every change in this case. Instead, we opt to display the decision boundary in just the final iteration of our model.\n\n# Plot final decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n\n\n\n\n\n\n\n\n\n\nPart B.3: A 5-Dimensional Example\nUp until this point, we have trained perceptron models exclusively on 2-dimensional data. Working with 2-dimensional data facilitates understanding and communication surrounding the model, as we can easily visualize data in two dimensions. However, we wrote our model to work in more than 2 dimensions! In this section, to demonstrate that our implementation works on multidimensional data, we fit a perceptron model on data with 5 features.\n\n# set seed\ntorch.manual_seed(1234)\n\n# create data\nX, y = perceptron_data(n_points = 50, noise = 0.3, p_dims = 5)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\n# for recording iteration number\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X, y)\n\n    iter += 1\n    if iter &gt;= 1000:\n        break\n\nWhile we cannot visualize our 5 dimensional data and weight vector, we can inspect the changes to our loss function over time, using the plot below!\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nSince we achieved zero loss and the code terminated before completing 1000 iterations, I know that my data is linearly separable."
  },
  {
    "objectID": "posts/perceptron/index.html#part-c-minibatch-perceptron",
    "href": "posts/perceptron/index.html#part-c-minibatch-perceptron",
    "title": "Implementing Perceptron",
    "section": "Part C: Minibatch Perceptron",
    "text": "Part C: Minibatch Perceptron\nIn this section, I implemented an extension to perceptron known as minibatch perceptron. The minibatch perceptron algorithm differs from the perceptron algorithm in that instead of updating the decision boundary using \\(1\\) random point, it updates the decision boundary using \\(k\\) random points. In more detail, the algorithm involves the following process:\n\nRandomly select an initial decision boundary \\(\\mathbf{w}^{(0)}\\).\nIteratively:\n\nSample \\(k\\) random integers \\(i_1, i_2, ..., i_k \\in \\{1,\\ldots,n\\}\\) without replacement.\nUpdate the decision boundary: \\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{\\alpha}{k} \\sum_{j=1}^k \\mathbb{1} [\\langle \\mathbf{w}^{(t)}, \\mathbf{x}_{i_j} \\rangle y_{i_j} &lt; 0] y_{i_j} \\mathbf{x}_{i_j} \\]\n\n\nYou can view my implementation of the algorithm at minibatch_perceptron.py, but I will not discuss all of the implementation details here. From the user’s perspective, fitting a minibatch perceptron model is exactly the same as fitting the perceptron model, with the addition of two parameters: \\(k\\) and \\(\\alpha\\). The \\(k\\) parameter simply refers to the number of points used in each iteration, while the \\(\\alpha\\) parameter is a learning rate that affects how much \\(\\mathbf{w}\\) changes when updated.\nIn the rest of this section, we perform a few experiments to illustrate the functionality of our algorithm and its similarities and differences in comparison to the regular perceptron algorithm. The experiments we perform in parts C.1, C.2, and C.3 all involve relatively similar operations. To reduce the volume of code in each section, we define the experiment() function below.\n\ndef experiment(X, y, k, alpha):  \n    # set seed\n    torch.manual_seed(1234567)\n\n    # instantiate a model and an optimizer\n    mb_p = MinibatchPerceptron() \n    mb_opt = MinibatchPerceptronOptimizer(mb_p)\n\n    # define loss variable\n    mb_loss = 1.0\n\n    # for keeping track of loss values\n    mb_loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while mb_loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        mb_loss = mb_p.loss(X, y) \n        mb_loss_vec.append(mb_loss)\n        \n        # perform a perceptron update using the random data point\n        mb_opt.step(X, y, k, alpha)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    # set seed\n    torch.manual_seed(1234567)\n\n    # instantiate a model and an optimizer\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    # define loss variable\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # perform a perceptron update using the random data point\n        opt.step(X, y)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    return loss_vec, mb_loss_vec, p.w, mb_p.w\n\n\nPart C.1: When k = 1\nTo illustrate the differences between the perceptron and minibatch perceptron algorithms, we begin by fitting the regular perceptron and the minibatch perceptron with \\(k = \\alpha = 1\\) on our linearly separable data.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_ls, y = y_ls, k = 1, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels \nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 1, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nThe two algorithms generated incredibly similar results! In fact, if the random point considered at iteration had been the same, these graphs would be identical. In the regular perceptron algorithm, we generate a random number between \\(0\\) and \\(n\\), but in the minibatch perceptron algorithm, we generate a random permutation of the numbers \\(0\\) through \\(n-1\\) and select the first element of that permutation. This difference appears to result in different points being considered at each iteration, explaining why these graphs differ slightly.\n\n\nPart C.2: When k = 10\nOne advantage of the minibatch perceptron is that we can tune our model to different values of \\(k\\) and \\(\\alpha\\). As our first adjustment, let’s try changing \\(k\\) to \\(10\\).\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_ls, y = y_ls, k = 10, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 10, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nIt appears that this adjustment allowed our model to converge to a decision boundary in far fewer iterations! While our regular perceptron model requires 49 iterations to achieve perfect classification, the minibatch model with \\(k = 10\\) requires only 8 iterations.\n\n\nPart C.2: Convergence on Non-Linearly Separable Data\nAnother advantage of the minibatch perceptron model is that it can converge to a decision threshold on data that is not linearly separable! This does not mean that the model will perfectly classify every data point, but rather that the model will converge to a loss value that is less than 0.5.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_nls, y = y_nls, k = 50, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 50, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nOh my! That certainly didn’t work. The minibatch perceptron model seems to have an even more chaotic loss function than the regular one! This is why we have the hyperparameter \\(\\alpha\\). Let’s try decreasing \\(\\alpha\\) to 0.01.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_nls, y = y_nls, k = 50, alpha = 0.01)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 50, \\\\alpha = 0.01$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nPhew, that looks much better! Whereas the loss function of the regular perceptron algorithm would oscillate for an eternity, the loss function of the minibatch algorithm converged to a value between 0.1 and 0.15 by the 600th iteration. Let’s take a look at the resulting decision boundary.\n\n# Plot final decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\ndraw_line(mb_p_w, x_min = -1.5, x_max = 2.5, ax = ax, color = \"black\")\n# ax.set(xlim = (-1, 2), ylim = (-1, 2))\nax.set_title(\"Decision Boundary on Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nThis decision boundary looks great! Obviously, it is impossible to perfectly separate the two categories of data in this example with a straight line. The decision boundary that our minibatch perceptron algorithm converged to appears to be about as good as we can do."
  },
  {
    "objectID": "posts/perceptron/index.html#abstract",
    "href": "posts/perceptron/index.html#abstract",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "In this blog post, we implement the perceptron, one of the first machine learning models ever invented, and perform a few experiments to highlight the model’s strengths and weaknesses. The perceptron algorithm is used to predict a binary outcome on data with a continuous and finite-dimensional feature space. Through our experiments, we visually illustrate that perceptron converges to a solution that completely differentiates between the outcome classes on linearly separable data. However, the algorithm does not converge to a solution on non-linearly separable data. We then implement a generalization of perceptron known as minibatch perceptron, which uses \\(k\\) observations in each iteration rather than \\(1\\). We perform experiments that illustrate that minibatch perceptron behaves similarly to perceptron when \\(k = 1\\) and continues to find a decision boundary on linearly separable data as we increase \\(k\\) towards \\(n\\). Finally, we show that when \\(k = n\\), minibatch perceptron can converge to a solution on non-linearly separable data, although this solution will not correctly classify every data point. To see my implementation of the two algorithms, please visit perceptron.py and minibatch_perceptron.py."
  },
  {
    "objectID": "posts/perceptron/index.html#part-a-implementing-perceptron",
    "href": "posts/perceptron/index.html#part-a-implementing-perceptron",
    "title": "Implementing Perceptron",
    "section": "Part A: Implementing Perceptron",
    "text": "Part A: Implementing Perceptron\nFirst, we implemented the perceptron algorithm in perceptron.py. Recall from Professor Chodrow’s lecture notes that the perceptron algorithm involves the following process:\n\nRandomly select an initial decision boundary \\(\\mathbf{w}^{(0)}\\).\nIteratively:\n\nPick a random integer \\(i \\in \\{1,\\ldots,n\\}\\).\nCompute the score for the point \\(i\\): \\(s_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\).\nUpdate the decision boundary: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbb{1} [s_i y_i &lt; 0] y_i \\mathbf{x}_i\\)\n\n\nThis entire process is implemented in perceptron.py, but for the sake of brevity, I will not discuss every line of code in this blog post. Instead, I focus exclusively on my implementation of the grad() function, which calculates \\(\\mathbb{1} [s_i y_i &lt; 0] y_i \\mathbf{x}_i\\).\n\n# grad() function from perceptron.py\ndef grad(self, X, y):\n        s = X@self.w\n        return (s*y &lt; 0)*X*y\n\nBy taking advantage of our knowledge of linear algebra, the implementation of grad() becomes quite short! In the first line of grad(), we calculate the inner product \\(s_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\). The tensors X and self.w are shaped appropriately for us to compute this inner product with torch’s @ operator. In the second and final line of grad(), we use our result from the first line to calculate \\([s_i y_i &lt; 0] y_i \\mathbf{x}_i\\).\nTo verify that our implementation of perceptron was succussful, we run the “minimal training loop” provided in the assignment instructions. First, we need data to run the loop on, so we use some code generously provided by Professor Chodrow to generate and display our linearly separable data.\n\n# import packages\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom minibatch_perceptron import MinibatchPerceptron, MinibatchPerceptronOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nimport ipywidgets as wdg\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n# define function to create data\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n# define function to plot data\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# create linearly separable data\nX_ls, y_ls = perceptron_data(n_points = 50, noise = 0.3)\n\n# plot linearly separable data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X_ls, y_ls, ax)\nax.set_title(\"Our Linearly Separable Data\");\n\n\n\n\n\n\n\n\nVisually, it is clear that one could draw a line between the two different colors of points, so our data is linearly separable. This is important, because the perceptron algorithm will only converge to a solution with a loss of zero on linearly separable data. Since we have our data, we are now prepared to run the minimal training loop.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_ls, y_ls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_ls, y_ls)\n\nThe minimal training loop terminates, so our model must have converged to a solution with a loss of 0."
  },
  {
    "objectID": "posts/perceptron/index.html#part-b-fitting-and-evaluating-perceptron-models",
    "href": "posts/perceptron/index.html#part-b-fitting-and-evaluating-perceptron-models",
    "title": "Implementing Perceptron",
    "section": "Part B: Fitting and Evaluating Perceptron Models",
    "text": "Part B: Fitting and Evaluating Perceptron Models\nNow that we have a functional implementation of perceptron, we perform experiments and generate illustrations to illustrate the strengths and weaknesses of the perceptron model.\n\nPart B.1: Evolution of the Model on Linearly Separable Data\nFirst, we illustrate how the loss function changes between iterations of the minimal training loop.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nAs you can see, our implementation of the perceptron algorithm converges to a solution with zero loss after 49 iterations. Notice that throughout this process, the weight vector only changes 6 times. In all 43 other iterations, the randomly selected point was correctly classified by the model at that stage.\nTo gain some insight as to the location and orientation of the decision boundary throughout this process, we plot the changes to weight vector in the figure below. We include a subplot for every change to the weight vector, letting the dashed lines represent the previous weight vector and the solid lines represent the current weight vector. In each subplot, the circled point corresponds to the point \\(i\\) that was misclassified leading to the update in the weight vector.\nThank you to Professor Chodrow for providing the code for creating this visualization.\n\n# define line function\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# set seed\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X_ls, y_ls)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i, local_loss = opt.step(X_ls, y_ls)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X_ls, y_ls, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X_ls, y_ls).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X_ls[i,0],X_ls[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y_ls[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThese figures illustrate how the model responds to the random selection of an incorrectly classified point. Generally, the correction slightly overcompensates for the misclassified point, leading the decision boundary to fluctuate between which class it incorrectly classifies, gradually adjusting until it classifies all points correctly.\n\n\nPart B.2: Evolution of the Model on Non-Linearly Separable Data\nUnfortunately, the perceptron algorithm will not converge to a decision boundary on data that is not linearly separable. To illustrate this, we first need to create data that cannot be perfectly divided using one separating line.\nTo create our non-linearly separable data, we use the same method as before but increase the amount of noise. As you can see in the graph below, the two classes have tendencies towards different regions in our resulting data, but it is impossible to draw a straight line that perfectly separates them.\n\n# set seed\ntorch.manual_seed(1234)\n\n# create non-linearly separable data\nX_nls, y_nls = perceptron_data(n_points = 50, noise = 0.8)\n\n# plot non-linearly separable data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\nax.set_title(\"Our Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nNow that we have created non-linearly separable data, we fit the perceptron model on our data. Since the perceptron will not converge to a solution on our data, we modify our code so the model terminates after 1000 iterations.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\n# for recording iteration number\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_nls, y_nls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_nls, y_nls)\n\n    # update iter\n    iter += 1\n\n    # maxiter condition\n    if iter &gt;= 1000:\n        break\n\nAs before, we begin by inspecting the change in loss over the iterations of our algorithm.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn the figure above, our model clearly does not converge to a solution. Rather, the perceptron algorithm is so sensitive to the individual data point under consideration at any given moment that the loss fluctuates wildly throughout the entire process, ranging from less than 0.1 to more than 0.7.\nIn the linearly separable case, we visually inspected every change to the weight vector, but because there are so many changes to the model in this case, it would be burdensome to inspect every change. Instead, we display the decision boundary in the final iteration of our model.\n\n# Plot final decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\ndraw_line(p.w, x_min = -1.5, x_max = 1.5, ax = ax, color = \"black\")\nax.set_title(\"Decision Boundary on Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nThe decision boundary could be worse, but it is far from perfect. The inadequate performance on non-linearly separable data is a major weakness of the perceptron algorithm.\n\n\nPart B.3: A 5-Dimensional Example\nUp until this point, we have only trained perceptron models on 2-dimensional data. Working with 2-dimensional data facilitates understanding and communication surrounding the model, as we can easily visualize data in two dimensions. However, we wrote our model to work in any finite number of dimensions. To demonstrate that our implementation works on multidimensional data, we now fit a perceptron model on data with 5 features.\n\n# set seed\ntorch.manual_seed(1234)\n\n# create data\nX_5d, y_5d = perceptron_data(n_points = 50, noise = 0.3, p_dims = 5)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\n# for recording iteration number\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_5d, y_5d) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_5d, y_5d)\n\n    iter += 1\n    if iter &gt;= 1000:\n        break\n\nWhile we cannot visualize our 5 dimensional data and weight vector, we can inspect the changes to our loss function over time using the plot below.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn the figure above, it appears that the perceptron algorithm terminated after 60 iterations, achieving a loss value of zero. The perceptron algorithm terminates if and only if its training data is linearly separable; thus, our data must be linearly separable."
  },
  {
    "objectID": "posts/perceptron/index.html#part-d-runtime-complexity",
    "href": "posts/perceptron/index.html#part-d-runtime-complexity",
    "title": "Implementing Perceptron",
    "section": "Part D: Runtime Complexity",
    "text": "Part D: Runtime Complexity\nWhat is the runtime complexity of perceptron and minibatch perceptron? To answer this question, let us consider the three lines of code that we run in each iteration of the model:\nloss = p.loss(X, y) \nloss_vec.append(loss)\nopt.step(X, y)\nThe first two lines store a record of the loss function for tracking purposes. They are not actually required for fitting the perceptron, so we exclude them from our consideration of the algorithm’s runtime complexity. Thus evaluating the runtime complexity of a single iteration of perceptron amounts to evaluating the runtime complexity of opt.step(X, y).\nIn our implementation of perceptron (see perceptron.py), opt.step() involves the following operations: - n = X.size()[0]: determines the number of rows in X in constant time - i = torch.randint(n, size = (1,)): selects a random integer in constant time - x_i = X[[i],:] and y_i = y[i]: subsets X and y in constant time - current_loss = self.model.loss(X, y): calculates the current loss, which involves a dot product between two \\(1 \\times p\\) vectors, an operation with linear time complexity O(\\(p\\)) - self.model.w += torch.reshape(self.model.grad(x_i, y_i),(self.model.w.size()[0],)): updates \\(\\mathbf{w}\\) in linear time due to a dot product in the grad function - new_loss = self.model.loss(X, y): calculates the updated loss in linear time due to a dot product - return i, abs(current_loss - new_loss): returns values for visualization in constant time\nSince the operations with the largest time complexity were O(\\(p\\)), the overall runtime complexity of a single iteration of the perceptron algorithm is O(\\(p\\)). This runtime depends on the number of features \\(p\\) rather than the number of observations \\(n\\), because the dot product is between the weight vector \\(\\mathbf{w}\\) and a single row of \\(X\\).\nIn our implementation of minibatch perceptron (see minibatch_perceptron.py), opt.step() involves largely similar operations. The only difference in runtime occurs in the grad() function. Whereas in the regular perceptron, this involved computing a single dot product between two \\(1 \\times p\\) vectors, in the minibatch perceptron, this involves computing a matrix product between \\(X^{k \\times p}\\) and \\(w^{p \\times 1}\\). Calculating this matrix product is equivalent to calculating \\(k\\) dot products, each of which are O(\\(p\\)). Thus the runtime of the matrix product, and therefore the big O runtime of the minibatch perceptron algorithm, is O(\\(kp\\))."
  },
  {
    "objectID": "posts/perceptron/index.html#conclusion",
    "href": "posts/perceptron/index.html#conclusion",
    "title": "Implementing Perceptron",
    "section": "Conclusion",
    "text": "Conclusion\nIn this assignment, we investigated the perceptron algorithm, implementing it from scratch within the object-oriented framework provided by Professor Chodrow. We illustrated that the algorithm converges to a solution on linearly separable data of any finite number of dimensions but fails to converge to a solution on non-linearly separable data. To address this shortcoming, we implemented the minibatch perceptron algorithm. We found that when \\(k = 1\\), minibatch perceptron is similar to regular perceptron, and as we increase \\(k\\), the algorithm continues to find decision boundaries on linearly separable data. Furthermore, we discovered that when \\(k = n\\) and our learning rate \\(\\alpha\\) is adjusted appropriately, minibatch perceptron can converge to a (albeit imperfect) solution on data that is not linearly separable. Overall, this assignment provided me with a great opportunity to write up my first machine learning model from scratch and investigate its strengths and weaknesses."
  }
]