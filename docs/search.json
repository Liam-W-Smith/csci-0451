[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! My name is Liam and I’m a senior at Middlebury College studying geography and mathematics. This blog documents my work in CSCI 0451: Machine Learning."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset contains anatomical measurements for \\(344\\) Penguins of \\(3\\) different species and is commonly used when introducing students to new data science concepts. In this blog post, we construct our first classification model in Python, attempting to accurately predict the species of a given observation in the Palmer Penguins dataset. Constrained to one qualitative and two quantitative predictors, we create our model using the scikit-learn package’s out-of-the-box logistic regression functionality. We select features by conducting 5-fold cross-validation on logistic regression models constructed from every possible combination of one qualitative and two quantitative variables. We implement our two models with the highest cross-validation score, selecting one that achieves 100% test data accuracy as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#abstract",
    "href": "posts/palmer-penguins/index.html#abstract",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset contains anatomical measurements for \\(344\\) Penguins of \\(3\\) different species and is commonly used when introducing students to new data science concepts. In this blog post, we construct our first classification model in Python, attempting to accurately predict the species of a given observation in the Palmer Penguins dataset. Constrained to one qualitative and two quantitative predictors, we create our model using the scikit-learn package’s out-of-the-box logistic regression functionality. We select features by conducting 5-fold cross-validation on logistic regression models constructed from every possible combination of one qualitative and two quantitative variables. We implement our two models with the highest cross-validation score, selecting one that achieves 100% test data accuracy as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-preparation",
    "href": "posts/palmer-penguins/index.html#data-preparation",
    "title": "Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nMost of the code in this section was conveniently provided by Professor Chodrow.\n\n# Load packages \nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport warnings\n\n# Change pd display parameter\npd.set_option('display.max_colwidth', 100)\n\n# Supress warnings to improve web aesthetics\nwarnings.filterwarnings(\"ignore\")\n\n# Load training data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n# Data preparation\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  x = df.drop([\"Species\"], axis = 1)\n  x = pd.get_dummies(x)\n  return x, y, df\n\nX_train, y_train, viz_train = prepare_data(train)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-exploration",
    "href": "posts/palmer-penguins/index.html#data-exploration",
    "title": "Palmer Penguins",
    "section": "Data Exploration",
    "text": "Data Exploration\nBefore constructing a classification model, it is often helpful to get a better feel for the data you are working with. For this reason, we begin our analysis by creating and analyzing a few figures.\n\n# Display plots\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\n\nsns.scatterplot(viz_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", ax = ax[0]);\nsns.move_legend(ax[0], \"lower right\")\nsns.scatterplot(viz_train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\", hue = \"Species\", ax = ax[1]);\nsns.move_legend(ax[1], \"lower right\")\n\n\n\n\n\n\n\n\nIn the first scatterplot, we have almost perfectly separated the three species of penguin using just two variables: flipper length and culmen length. The Adelie penguins tend to have low values of both variables, the Chinstrap penguins tend to have large culmen lengths but small flipper lengths, and the Gentoo penguins tend to have medium culmen lengths and large flipper lengths. One can imagine drawing straight lines in this graph that almost perfectly separate the three species of penguin. The fact that we can visually distinguish between the different species of penguins makes me very confident about our prospects for classification.\nIn the second scatterplot, the Gentoo penguins tend to have small culmen depths and large body masses, separating them from the other two species with some white-space to spare. However, the distributions of Adelie and Chinstrap points look almost identical, occupying the same lower right corner of the plot. If our goal was simply to predict whether a penguin is a Gentoo penguin, then I would imagine that these two variables could help make a strong classifier. However, since we seek to distinguish between all three species, these variables might not be the best choice.\nWhile body mass (or body mass and culmen depth) alone is insufficient to distinguish between the species, perhaps accounting for a categorical variable like sex will reveal some patterns. To explore this possibility, we include the following table of median body mass.\n\n# Display table\nviz_train.groupby(['Sex', 'Species'])[['Body Mass (g)']].median().unstack()\n\n\n\n\n\n\n\n\nBody Mass (g)\n\n\nSpecies\nAdelie\nChinstrap\nGentoo\n\n\nSex\n\n\n\n\n\n\n\nFEMALE\n3350.0\n3525.0\n4700.0\n\n\nMALE\n4025.0\n4050.0\n5500.0\n\n\n\n\n\n\n\nFrom this table, it is clear that the median body masses of all three species vary by sex, with the males tending to weigh more than the females. Additionally, while we already knew from our scatterplot that the Gentoo penguins tend to weigh more than the other species, this table reinforces that conclusion and shows it to be true for both males and females. Unfortunately, accounting for sex appears to do little to further distinguish between Adelie and Chinstrap penguins. There is roughly a 200 gram difference between the median mass of female Adelie and Chinstrap penguins, so perhaps sex in can help distinguish between the two species given that a penguin is female. However, the median mass of male Adelie and Chinstrap penguins differs only by 25 grams. A classifier would almost certainly need more information in order to make accurate predictions."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-construction",
    "href": "posts/palmer-penguins/index.html#model-construction",
    "title": "Palmer Penguins",
    "section": "Model Construction",
    "text": "Model Construction\nSince the Palmer Penguins dataset is relatively small and we are given constraints on the number of features we may include in our model, we will select features by fitting a classifier for every possible combination of one qualitative and two quantitative variables. There may be more robust and less computationally expensive methods of feature selection, but for our purposes, this works.\nWe implement logistic regression using scikit-learn’s out-of-the-box logistic-regression classifier.\n\n# Choosing features\nfrom itertools import combinations\n\n# Distinguish between qualitative and quantitative variables\nall_qual_cols = ['Clutch Completion', 'Sex', 'Island', 'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n# Define classifier\nLR = LogisticRegression(max_iter=100000000)\n\n# Initialize dataframe to store data\nmodels = pd.DataFrame(columns = [\"CV Score\", \"Columns\"])\n\n# Find best-performing classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    cv_score = cv_scores_LR.mean()\n    \n    model = pd.DataFrame({\"CV Score\": [cv_score], \"Columns\": [cols]})\n\n    models = pd.concat([models, model])\n\n\n# Sort by CV Score and display best-performing models\nmodels = models.sort_values(\"CV Score\", ascending=False)\nmodels.head()\n\n\n\n\n\n\n\n\nCV Score\nColumns\n\n\n\n\n0\n0.988311\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\n\n\n0\n0.988311\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\n\n\n0\n0.980543\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Body Mass (g)]\n\n\n0\n0.968854\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Flipper Length (mm)]\n\n\n0\n0.968778\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Delta 13 C (o/oo)]\n\n\n\n\n\n\n\nLet’s fit the model with the highest cross-validation score! There are actually two models with equal performance, so we will try both.\n\n# Test the model\n\n# Import data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Adjust species label\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n# Data preparation\nX_test, y_test, viz_test = prepare_data(test)\n\n# Fit and output the performance of the model\nmodel1 = LR.fit(X_train[models.iloc[[0]]['Columns'][0]], y_train)\nmodel1_score = LR.score(X_test[models.iloc[[0]]['Columns'][0]], y_test)\nmodel1_score\n\n1.0\n\n\nVoila! Logistic regression using the Culmen Length (mm), Culmen Depth (mm), and Island features correctly predicts every observation in our test data, achieving our goal.\n\n# Test the next model\nmodel2 = LR.fit(X_train[models.iloc[[1]]['Columns'][0]], y_train)\nmodel2_score = LR.score(X_test[models.iloc[[1]]['Columns'][0]], y_test)\nmodel2_score\n\n0.9852941176470589\n\n\nOn the other hand, logistic regression using the Culmen Length (mm), Culmen Depth (mm), and Sex features does not achieve 100% accuracy on the test data. Since our first model achieved 100% test accuracy, we select it as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-evaluation",
    "href": "posts/palmer-penguins/index.html#model-evaluation",
    "title": "Palmer Penguins",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nAt this point, we have constructed a well-performing model, so we proceed to model evaluation. First, we visualize our model’s decision regions using the plot_regions() function provided by Professor Chodrow.\n\n# Plotting decision regions function, generously provided by Professor Chodrow\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (10, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout(rect = (0,0,.95,.95))\n\n\n# Reverse the order of columns for the purposes of visualization\nmodels.iloc[[0]]['Columns'][0].reverse()\n\n# Refit model\nLR.fit(X_train[models.iloc[[0]]['Columns'][0]], y_train)\n\n# Plot decision regions on training data\nplot_regions(LR, X_train[models.iloc[[0]]['Columns'][0]], y_train)\nplt.suptitle(\"Decision Regions and Training Data\", fontsize = 13)\n\n# # Plot decision regions on test data\nplot_regions(LR, X_test[models.iloc[[0]]['Columns'][0]], y_test)\nplt.suptitle(\"Decision Regions and Test Data\", fontsize = 13);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the upper row of figures, we plot the training data in our decision regions, while in the lower row, we plot the test data in the decision regions. We trained a model on our training data and implemented it on our test data, so the decision regions are identical in the two rows of figures. The only difference between the rows is the points within the decision regions. Since most observations are part of the training data, there are many more points in the first row of figures. It is interesting to note that Torgersen Island only contains Gentoo penguins, Dream Island does not contain any Adelie penguins, and Biscoe Island does not contain any Chinstrap penguins. These patterns makes it substantially easier to predict a given penguin’s species. One could imagine that the model would always predict Gentoo penguins on Torgersen Island. Similarly, one could imagine the model never predicting Adelie penguins on Dream Island and never predicting Chinstrap penguins on Biscoe Island. Interestingly, the model actually fits decision regions for all three penguin varieties in each island. The decision region for a given species tends to be larger on islands where we would expect to find them, but the model allows for the possibility that there might be a penguin on an island where we would not expect one. Overall, the logistic regression model does a great job of incorporating both qualitative and quantitative variables into its decision regions.\n\n# Predict\nmodel1_pred = model1.predict(X_test[models.iloc[[0]]['Columns'][0]])\n\n# Confusion matrix\nC = confusion_matrix(y_test, model1_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nSince all of the predictions are on the main diagonal of the confusion matrix, we conclude that we did not misclassify any observations – but we already knew this, since our model achieved 100% test accuracy! If our model had not perfectly classified the observations in our test data, a confusion matrix would be more helpful for assessing our model’s misclassification tendencies."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#discussion",
    "href": "posts/palmer-penguins/index.html#discussion",
    "title": "Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, we began by exploring the Palmer Penguins dataset. Our scatterplots and table illustrated that for some variables, there is substantial variation between different species of penguin, while for others, there is very little. Determining which variables would be most valuable for classification required more than simple visualizations. In order to robustly determine the best combination of one qualitative and two quantitative varibles, I fit a logistic regression model on every possible combination. This process was relatively slow and may not work at scale, but it was successful in generating a highly accurate model in this scenario. My final model used the Culmen Length (mm), Culmen Depth (mm), and Island features, achieving 100% accurate predictions on the test data.\nThis assignment furthered my understanding of the entire model-construction procedure, including data preparation, data exploration, feature selection, model fitting, and model evaluation. After completing this blog post, I feel more confident about my ability to implement standard machine learning procedures from Python packages like scikit-learn. While it is relatively easy to implement out-of-the-box classifiers from scikit-learn, it may be more difficult to understand their theoretical underpinnings. As I finish this assignment, I am eager to learn more about the mathematics happening under the hood."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In this blog post, we implement logistic regression via empirical risk minimization and perform a few experiments to highlight the model’s strengths and weaknesses. As a part of our experimentation, we implement gradient descent with momentum and investigate the evolution of our loss function in comparison to ordinary gradient descent. We also test our model on data with more dimensions than observations in order to illustrate the perils of overfitting. To see my implementation of logistic regression, please visit logistic.py."
  },
  {
    "objectID": "posts/logistic-regression/index.html#abstract",
    "href": "posts/logistic-regression/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In this blog post, we implement logistic regression via empirical risk minimization and perform a few experiments to highlight the model’s strengths and weaknesses. As a part of our experimentation, we implement gradient descent with momentum and investigate the evolution of our loss function in comparison to ordinary gradient descent. We also test our model on data with more dimensions than observations in order to illustrate the perils of overfitting. To see my implementation of logistic regression, please visit logistic.py."
  },
  {
    "objectID": "posts/logistic-regression/index.html#generating-training-data",
    "href": "posts/logistic-regression/index.html#generating-training-data",
    "title": "Implementing Logistic Regression",
    "section": "Generating Training Data",
    "text": "Generating Training Data\nFirst, we import the packages we will need for this assignment.\n\n# Import packages\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nNext, we generate the data that we will use to train our model. Thank you to Professor Chodrow for providing the functions for generating and visualizing training data.\n\n# Define function to generate data\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n# define function to plot data\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -0.5, vmax = 1.5, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# generate data\nX, y = classification_data(n_points = 500, noise = 0.6)\n\n# plot data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_data(X, y, ax)\nax.set_title(\"Training Data\");"
  },
  {
    "objectID": "posts/logistic-regression/index.html#vanilla-gradient-descent",
    "href": "posts/logistic-regression/index.html#vanilla-gradient-descent",
    "title": "Implementing Logistic Regression",
    "section": "Vanilla Gradient Descent",
    "text": "Vanilla Gradient Descent\nTo begin evaluating the efficacy of our model, we fit logistic regression on our training data with regular gradient descent. We actually only implemented gradient descent with momentum in logistic.py, but if we set \\(\\beta = 0\\), as we do below, this is equivalent to regular gradient descent.\n\n# set seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec_vanilla = []\n\n# fit model\nfor _ in range(100):\n\n    # update model\n    opt.step(X, y, alpha = 0.07, beta = 0)\n\n    # calculate and record loss\n    loss = LR.loss(X, y) \n    loss_vec_vanilla.append(loss)\n\nNow that we have fit our model, let’s inspect our decision boundary in the context of our training data.\n\n# define function to draw line\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# plot decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_data(X, y, ax)\ndraw_line(LR.w, x_min = -1.25, x_max = 2.25, ax = ax, color = \"black\")\nax.set_title(\"Training Data and Decision Boundary\");\n\n\n\n\n\n\n\n\nVisually, this line appears to be an intelligent choice. There are some misclassified points on either side of the line, but our data is not linearly separable, so it is impossible to correctly classify all points using a linear decision boundary. This visual check leads me to believe that our classifier is performing well. As another check, we plot the evolution of our loss function below to verify that it decreases monotonically.\n\n# plot the changes in loss \nplt.plot(loss_vec_vanilla, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_vanilla)), loss_vec_vanilla, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn this graph, every iteration has a lower loss value than the iteration that precedes it. In other words, our loss function decreases monotonically, as guaranteed by gradient descent.\n\nThe Benefits of Momentum\nOur model appears to be behaving as expected, leading us to the natural question: can we do better if we use gradient descent with momentum? More specifically, can the model converge to an appropriate weight vector in fewer iterations under gradient descent with momentum? To address this question, we fit a logistic regression model using the same values for \\(\\alpha\\) and our random seed, but with the modification that \\(\\beta = 0.9\\) rather than \\(0\\).\n\n# set seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec_momentum = []\n\n# fit model\nfor _ in range(100):\n\n    # update model\n    opt.step(X, y, alpha = 0.07, beta = 0.9)\n\n    # calculate and record loss\n    loss = LR.loss(X, y) \n    loss_vec_momentum.append(loss)\n\nTo identify any improvements due to gradient descent with momentum, we plot the evolution of the loss function under both scenarios on the same graph.\n\n# plot the changes in loss \nplt.plot(loss_vec_vanilla, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_vanilla)), loss_vec_vanilla, color = \"slategrey\", label = \"Gradient Descent\")\nplt.plot(loss_vec_momentum, color = \"#A37933\")\nplt.scatter(torch.arange(len(loss_vec_momentum)), loss_vec_momentum, color = \"#A37933\", label = \"Gradient Descent with Momentum\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\")\nplt.legend(loc = \"lower left\");\n\n\n\n\n\n\n\n\nIn this scenario, the loss function of gradient descent with momentum decreased slightly more rapidly than regular gradient descent. To be fully transparent, producing a scenario with this improvement involved some fishing for appropriate data and \\(\\alpha\\). In this case, gradient descent with momentum performed slightly better than regular gradient descent, but this is by no means a guarantee.\n\n\nThe Perils of Overfitting\nIn this section, we construct a scenario where logistic regression overfits to the training data. To accomplish this task, we fit a model on data with substantially more dimensions than observations. Specifically, we generate training data and test data which both contain 30 dimensions and 20 observations, using a function for generating classification data generously provided by Professor Chodrow. Then we fit our logistic regression model on our training data, resulting in the evolution of our loss function as illustrated below.\n\n# set seed\ntorch.manual_seed(1234)\n\n# generate data\nX_train, y_train = classification_data(n_points = 20, noise = 0.5, p_dims = 30)\nX_test, y_test = classification_data(n_points = 20, noise = 0.5, p_dims = 30)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec_overfit = []\n\n# fit model\nfor _ in range(100):\n\n    # update model\n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\n    # calculate and record loss\n    loss = LR.loss(X_train, y_train) \n    loss_vec_overfit.append(loss)\n    if loss == 0:\n        break\n\n# plot the changes in loss \nplt.plot(loss_vec_overfit, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_overfit)), loss_vec_overfit, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn this figure, we find that our loss function decreased in the same monotonic manner as before, converging to a decision boundary with a loss value of less than 0.1. Below, we calculate the training accuracy of our model.\n\n# Compute training accuracy\ny_hat = LR.predict(X_train)\ntrain_accuracy = (1.0*(y_hat == y_train)).mean().item()\ntrain_accuracy\n\n1.0\n\n\nAt 100% training accuracy, our model is correctly predicting the outcome variable of every observation in our training data. But what about testing accuracy?\n\n# Compute testing accuracy\ny_hat = LR.predict(X_test)\ntest_accuracy = (1.0*(y_hat == y_test)).mean().item()\ntest_accuracy\n\n0.6499999761581421\n\n\nAt roughly 65%, our testing accuracy is not nearly as high as our training accuracy. Our model appears to have been overfit to our training data, failing to generalize to test data that was generated in a similar manner. Why might this have happened?\nI suspect that 20 observations of training points is simply insufficient for training a model in 30 dimensional space. There are so many possibilities for where a point can be located in 30 dimensional space, and 20 observations barely scratches the surface of these possibilities. Our model would need substantially more training data in order to have been exposed to enough of these possibilities. In the absence of sufficient training data, our model reflects the noise of our training data rather than the underlying pattern."
  },
  {
    "objectID": "posts/logistic-regression/index.html#conclusion",
    "href": "posts/logistic-regression/index.html#conclusion",
    "title": "Implementing Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nIn this assignment, we implemented logistic regression via empirical risk minimization within the object-oriented framework provided by Professor Chodrow. We investigated the differences between regular gradient descent and gradient descent with momentum, discovering that the latter option converges to a solution more rapidly in some cases. We also illustrated that logistic regression tends to overfit when the training data has more dimensions than observations. Implementing logistic regression and experimenting with the model’s parameters furthered my understanding of logistic regression in particular and gradient descent more broadly."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "In this blog post, we implement the perceptron, one of the first machine learning models ever invented, and perform a few experiments to highlight the model’s strengths and weaknesses. The perceptron algorithm is used to predict a binary outcome on data with a continuous and finite-dimensional feature space. Through our experiments, we visually illustrate that perceptron converges to a solution that completely differentiates between the outcome classes on linearly separable data. However, the algorithm does not converge to a solution on non-linearly separable data. We then implement a generalization of perceptron known as minibatch perceptron, which uses \\(k\\) observations in each iteration rather than \\(1\\). We perform experiments that illustrate that minibatch perceptron behaves similarly to perceptron when \\(k = 1\\) and continues to find a decision boundary on linearly separable data as we increase \\(k\\) towards \\(n\\). Finally, we show that when \\(k = n\\), minibatch perceptron can converge to a solution on non-linearly separable data, although this solution will not correctly classify every data point. To see my implementation of the two algorithms, please visit perceptron.py and minibatch_perceptron.py."
  },
  {
    "objectID": "posts/perceptron/index.html#abstract",
    "href": "posts/perceptron/index.html#abstract",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "In this blog post, we implement the perceptron, one of the first machine learning models ever invented, and perform a few experiments to highlight the model’s strengths and weaknesses. The perceptron algorithm is used to predict a binary outcome on data with a continuous and finite-dimensional feature space. Through our experiments, we visually illustrate that perceptron converges to a solution that completely differentiates between the outcome classes on linearly separable data. However, the algorithm does not converge to a solution on non-linearly separable data. We then implement a generalization of perceptron known as minibatch perceptron, which uses \\(k\\) observations in each iteration rather than \\(1\\). We perform experiments that illustrate that minibatch perceptron behaves similarly to perceptron when \\(k = 1\\) and continues to find a decision boundary on linearly separable data as we increase \\(k\\) towards \\(n\\). Finally, we show that when \\(k = n\\), minibatch perceptron can converge to a solution on non-linearly separable data, although this solution will not correctly classify every data point. To see my implementation of the two algorithms, please visit perceptron.py and minibatch_perceptron.py."
  },
  {
    "objectID": "posts/perceptron/index.html#part-a-implementing-perceptron",
    "href": "posts/perceptron/index.html#part-a-implementing-perceptron",
    "title": "Implementing Perceptron",
    "section": "Part A: Implementing Perceptron",
    "text": "Part A: Implementing Perceptron\nRecall from Professor Chodrow’s lecture notes that the perceptron algorithm involves the following process:\n\nRandomly select an initial decision boundary \\(\\mathbf{w}^{(0)}\\).\nIteratively:\n\nPick a random integer \\(i \\in \\{1,\\ldots,n\\}\\).\nCompute the score for the point \\(i\\): \\(s_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\).\nUpdate the decision boundary: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbb{1} [s_i y_i &lt; 0] y_i \\mathbf{x}_i\\)\n\n\nI implement this entire process in perceptron.py, but for the sake of brevity, I will not discuss every line of code in this blog post. Instead, I focus exclusively on my implementation of the grad() function, which calculates \\(\\mathbb{1} [s_i y_i &lt; 0] y_i \\mathbf{x}_i\\).\n\n# grad() function from perceptron.py\ndef grad(self, X, y):\n        s = X@self.w\n        return (s*y &lt; 0)*X*y\n\nBy taking advantage of our knowledge of linear algebra, the implementation of grad() becomes quite short! In the first line of grad(), we calculate the inner product \\(s_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\). The tensors X and self.w are shaped appropriately for us to compute this inner product with torch’s @ operator. In the second and final line of grad(), we use our result from the first line to calculate \\([s_i y_i &lt; 0] y_i \\mathbf{x}_i\\).\nTo verify that our implementation of perceptron was succussful, we run the “minimal training loop” provided in the assignment instructions. First, we need data to run the loop on, so we use some code generously provided by Professor Chodrow to generate and display our linearly separable data.\n\n# import packages\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom minibatch_perceptron import MinibatchPerceptron, MinibatchPerceptronOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nimport ipywidgets as wdg\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n# define function to create data\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n# define function to plot data\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# create linearly separable data\nX_ls, y_ls = perceptron_data(n_points = 50, noise = 0.3)\n\n# plot linearly separable data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X_ls, y_ls, ax)\nax.set_title(\"Our Linearly Separable Data\");\n\n\n\n\n\n\n\n\nVisually, it is clear that one could draw a line between the two different colors of points, so our data is linearly separable. This is important, because the perceptron algorithm will only converge to a solution with a loss of zero on linearly separable data. Since we have our data, we are now prepared to run the minimal training loop.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_ls, y_ls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_ls, y_ls)\n\nThe minimal training loop terminates, so our model must have converged to a solution with a loss of 0."
  },
  {
    "objectID": "posts/perceptron/index.html#part-b-fitting-and-evaluating-perceptron-models",
    "href": "posts/perceptron/index.html#part-b-fitting-and-evaluating-perceptron-models",
    "title": "Implementing Perceptron",
    "section": "Part B: Fitting and Evaluating Perceptron Models",
    "text": "Part B: Fitting and Evaluating Perceptron Models\nNow that we have a functional implementation of perceptron, we perform experiments and generate illustrations to reveal the strengths and weaknesses of the perceptron model.\n\nPart B.1: Evolution of the Model on Linearly Separable Data\nFirst, we illustrate how the loss function changes between iterations of the minimal training loop.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nAs you can see, with our data and our randomness seed, the perceptron algorithm converges to a solution with zero loss after 49 iterations. Notice that throughout this process, the weight vector only changes 6 times. In all 43 other iterations, the randomly selected point was correctly classified by the model at that stage.\nTo gain some insight as to the location and orientation of the decision boundary throughout this process, we plot the changes to the weight vector in the figure below. We include a subplot for every change to the weight vector, letting the dashed lines represent the previous weight vector and the solid lines represent the current weight vector. In each subplot, the circled point corresponds to the point \\(i\\) that was misclassified, leading to an update to the weight vector.\nThank you to Professor Chodrow for providing the code for creating this visualization.\n\n# define line function\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# set seed\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X_ls, y_ls)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i, local_loss = opt.step(X_ls, y_ls)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X_ls, y_ls, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X_ls, y_ls).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X_ls[i,0],X_ls[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y_ls[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThese figures illustrate how the model responds to the random selection of an incorrectly classified point. Generally, the correction slightly overcompensates for the misclassified point, leading the decision boundary to fluctuate between which class it incorrectly classifies, gradually adjusting until it classifies all points correctly.\n\n\nPart B.2: Evolution of the Model on Non-Linearly Separable Data\nUnfortunately, the perceptron algorithm will not converge to a decision boundary on data that is not linearly separable. To illustrate this, we first need to create data that cannot be perfectly divided using one separating line.\nTo create our non-linearly separable data, we use the same method as before but increase the amount of noise. As you can see in the graph below, the two classes have tendencies towards different regions in our resulting data, but it is impossible to draw a straight line that perfectly separates them.\n\n# set seed\ntorch.manual_seed(1234)\n\n# create non-linearly separable data\nX_nls, y_nls = perceptron_data(n_points = 50, noise = 0.8)\n\n# plot non-linearly separable data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\nax.set_title(\"Our Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nNow that we have created non-linearly separable data, we fit the perceptron model on our data. Since the perceptron will not converge to a solution on our data, we modify our code so the model terminates after 1000 iterations.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\n# for recording iteration number\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_nls, y_nls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_nls, y_nls)\n\n    # update iter\n    iter += 1\n\n    # maxiter condition\n    if iter &gt;= 1000:\n        break\n\nAs before, we begin by inspecting the change in loss over the iterations of our algorithm.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn the figure above, our model clearly does not converge to a solution. Rather, the perceptron algorithm is so sensitive to the individual data point under consideration at any given moment that the loss fluctuates wildly throughout the entire process, ranging from less than 0.1 to more than 0.7.\nIn the linearly separable case, we visually inspected every change to the weight vector, but because there are so many changes to the model in this case, it would be burdensome to inspect every change. Instead, we display the decision boundary in the final iteration of our model.\n\n# Plot final decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\ndraw_line(p.w, x_min = -1.5, x_max = 1.5, ax = ax, color = \"black\")\nax.set_title(\"Decision Boundary on Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nThe decision boundary could be worse, but it is far from perfect. Perceptron’s inadequate performance on non-linearly separable data is one of its major weaknesses.\n\n\nPart B.3: A 5-Dimensional Example\nUp until this point, we have only trained perceptron models on 2-dimensional data. Working with 2-dimensional data facilitates understanding and communication surrounding the model, as we can easily visualize data in two dimensions. However, we wrote our model to work in any finite number of dimensions. To demonstrate that our implementation works on multidimensional data, we now fit a perceptron model on data with 5 features.\n\n# set seed\ntorch.manual_seed(1234)\n\n# create data\nX_5d, y_5d = perceptron_data(n_points = 50, noise = 0.3, p_dims = 5)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\n# for recording iteration number\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_5d, y_5d) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_5d, y_5d)\n\n    iter += 1\n    if iter &gt;= 1000:\n        break\n\nWhile we cannot visualize our 5 dimensional data and weight vector, we can inspect the changes to our loss function over time using the plot below.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn the figure above, it appears that the perceptron algorithm terminated after 60 iterations, achieving a loss value of zero. The perceptron algorithm terminates if and only if its training data is linearly separable; thus, our data must be linearly separable."
  },
  {
    "objectID": "posts/perceptron/index.html#part-c-minibatch-perceptron",
    "href": "posts/perceptron/index.html#part-c-minibatch-perceptron",
    "title": "Implementing Perceptron",
    "section": "Part C: Minibatch Perceptron",
    "text": "Part C: Minibatch Perceptron\nIn this section, I implemented an extension to perceptron known as minibatch perceptron. The minibatch perceptron algorithm differs from the perceptron algorithm in that instead of updating the decision boundary using \\(1\\) random point, it updates the decision boundary using \\(k\\) random points. In more detail, the algorithm involves the following process:\n\nRandomly select an initial decision boundary \\(\\mathbf{w}^{(0)}\\).\nIteratively:\n\nSample \\(k\\) random integers \\(i_1, i_2, ..., i_k \\in \\{1,\\ldots,n\\}\\) without replacement.\nUpdate the decision boundary: \\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{\\alpha}{k} \\sum_{j=1}^k \\mathbb{1} [\\langle \\mathbf{w}^{(t)}, \\mathbf{x}_{i_j} \\rangle y_{i_j} &lt; 0] y_{i_j} \\mathbf{x}_{i_j} \\]\n\n\nYou can view my implementation of the algorithm at minibatch_perceptron.py, but I will not discuss all of the implementation details here. From the user’s perspective, fitting a minibatch perceptron model is exactly the same as fitting the perceptron model, with the addition of two parameters: \\(k\\) and \\(\\alpha\\). The \\(k\\) parameter simply refers to the number of points used in each iteration, while the \\(\\alpha\\) parameter is a learning rate that affects how much \\(\\mathbf{w}\\) changes when updated.\nIn the rest of this section, we perform a few experiments to illustrate the functionality of our algorithm and its similarities and differences in comparison to the regular perceptron algorithm. The experiments we perform in parts C.1, C.2, and C.3 all involve relatively similar operations. To reduce the volume of code in each section, we define the experiment() function below.\n\ndef experiment(X, y, k, alpha):  \n    # set seed\n    torch.manual_seed(1234567)\n\n    # instantiate a model and an optimizer\n    mb_p = MinibatchPerceptron() \n    mb_opt = MinibatchPerceptronOptimizer(mb_p)\n\n    # define loss variable\n    mb_loss = 1.0\n\n    # for keeping track of loss values\n    mb_loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while mb_loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        mb_loss = mb_p.loss(X, y) \n        mb_loss_vec.append(mb_loss)\n        \n        # perform a perceptron update using the random data point\n        mb_opt.step(X, y, k, alpha)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    # set seed\n    torch.manual_seed(1234567)\n\n    # instantiate a model and an optimizer\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    # define loss variable\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # perform a perceptron update using the random data point\n        opt.step(X, y)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    return loss_vec, mb_loss_vec, p.w, mb_p.w\n\n\nPart C.1: When k = 1\nTo illustrate the differences between the perceptron and minibatch perceptron algorithms, we begin by fitting the regular perceptron and the minibatch perceptron with \\(k = \\alpha = 1\\) on our linearly separable data.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_ls, y = y_ls, k = 1, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels \nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 1, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nThe two algorithms generated incredibly similar results! In fact, if the random point considered at every iteration had been the same, these graphs would be identical. In the regular perceptron algorithm, we generate a random number between \\(0\\) and \\(n\\), but in the minibatch perceptron algorithm, we generate a random permutation of the numbers \\(0\\) through \\(n-1\\) and select the first element of that permutation. This difference appears to result in different points being considered at each iteration, explaining why these graphs differ slightly.\n\n\nPart C.2: When k = 10\nOne advantage of the minibatch perceptron algorithm is that we can tune our model to different values of \\(k\\) and \\(\\alpha\\). As our first adjustment, let’s try changing \\(k\\) to \\(10\\).\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_ls, y = y_ls, k = 10, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 10, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nIt appears that this adjustment allowed our model to converge to a decision boundary in far fewer iterations! While our regular perceptron model requires 49 iterations to achieve perfect classification, the minibatch model with \\(k = 10\\) requires only 8 iterations.\n\n\nPart C.3: Convergence on Non-Linearly Separable Data\nAnother advantage of the minibatch perceptron model is that it can converge to a decision threshold on data that is not linearly separable. This does not mean that the model will perfectly classify every data point, but rather that the model will converge to a decision boundary with a loss value that is less than 0.5.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_nls, y = y_nls, k = 50, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 50, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nOh my! That certainly didn’t work. The minibatch perceptron model seems to have an even more chaotic loss function than the regular one! This is why we have the hyperparameter \\(\\alpha\\). Let’s try decreasing \\(\\alpha\\) to 0.01.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_nls, y = y_nls, k = 50, alpha = 0.01)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 50, \\\\alpha = 0.01$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nPhew, that looks much better! Whereas the loss function of the regular perceptron algorithm would oscillate for an eternity, the loss function of the minibatch algorithm converged to a value between 0.1 and 0.15 by the 600th iteration. Let’s take a look at the resulting decision boundary.\n\n# Plot final decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\ndraw_line(mb_p_w, x_min = -1.5, x_max = 2.5, ax = ax, color = \"black\")\n# ax.set(xlim = (-1, 2), ylim = (-1, 2))\nax.set_title(\"Decision Boundary on Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nThis decision boundary looks great! Obviously, it is impossible to perfectly separate the two categories of data in this example with a straight line. The decision boundary that our minibatch perceptron algorithm converged to appears to be about as good as we can do."
  },
  {
    "objectID": "posts/perceptron/index.html#part-d-runtime-complexity",
    "href": "posts/perceptron/index.html#part-d-runtime-complexity",
    "title": "Implementing Perceptron",
    "section": "Part D: Runtime Complexity",
    "text": "Part D: Runtime Complexity\nWhat is the runtime complexity of perceptron and minibatch perceptron? To answer this question, let us consider the three lines of code that we run in each iteration of the model:\nloss = p.loss(X, y) \nloss_vec.append(loss)\nopt.step(X, y)\nThe first two lines store a record of the loss function for tracking purposes. They are not actually required for fitting the perceptron, so we exclude them from our consideration of the algorithm’s runtime complexity. Thus evaluating the runtime complexity of a single iteration of perceptron amounts to evaluating the runtime complexity of opt.step(X, y).\nIn our implementation of perceptron (see perceptron.py), opt.step() involves the following operations:\n\nn = X.size()[0]: determines the number of rows in X in constant time\ni = torch.randint(n, size = (1,)): selects a random integer in constant time\nx_i = X[[i],:] and y_i = y[i]: subsets X and y in constant time\ncurrent_loss = self.model.loss(X, y): calculates the current loss, which involves a dot product between two \\(1 \\times p\\) vectors, an operation with linear time complexity O(\\(p\\))\nself.model.w += torch.reshape(self.model.grad(x_i, y_i),(self.model.w.size()[0],)): updates \\(\\mathbf{w}\\) in linear time O(\\(p\\)) due to a dot product in the grad function\nnew_loss = self.model.loss(X, y): calculates the updated loss in linear time O(\\(p\\)) due to a dot product\nreturn i, abs(current_loss - new_loss): returns values for visualization in constant time\n\nSince the operations with the largest time complexity were O(\\(p\\)), the overall runtime complexity of a single iteration of the perceptron algorithm is O(\\(p\\)). This runtime depends on the number of features \\(p\\) rather than the number of observations \\(n\\), because the dot product is between the weight vector \\(\\mathbf{w}\\) and a single row of \\(X\\).\nIn our implementation of minibatch perceptron (see minibatch_perceptron.py), opt.step() involves largely similar operations. The only difference in runtime occurs in the grad() function. Whereas in the regular perceptron, this involved computing a single dot product between two \\(1 \\times p\\) vectors, in the minibatch perceptron, this involves computing a matrix product between \\(X^{k \\times p}\\) and \\(w^{p \\times 1}\\). Calculating this matrix product is equivalent to calculating \\(k\\) dot products, each of which are O(\\(p\\)). Thus the runtime of the matrix product, and therefore the big O runtime of the minibatch perceptron algorithm, is O(\\(kp\\))."
  },
  {
    "objectID": "posts/perceptron/index.html#conclusion",
    "href": "posts/perceptron/index.html#conclusion",
    "title": "Implementing Perceptron",
    "section": "Conclusion",
    "text": "Conclusion\nIn this assignment, we investigated the perceptron algorithm, implementing it from scratch within the object-oriented framework provided by Professor Chodrow. We illustrated that the algorithm converges to a solution on linearly separable data of any finite number of dimensions but fails to converge to a solution on non-linearly separable data. To address this shortcoming, we implemented the minibatch perceptron algorithm. We found that when \\(k = 1\\), minibatch perceptron is similar to regular perceptron, and as we increase \\(k\\), the algorithm continues to find decision boundaries on linearly separable data. Furthermore, we discovered that when \\(k = n\\) and our learning rate \\(\\alpha\\) is adjusted appropriately, minibatch perceptron can converge to a (albeit imperfect) solution on data that is not linearly separable. Overall, this assignment provided me with a great opportunity to write up my first machine learning model from scratch and investigate its strengths and weaknesses."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Logistic Regression\n\n\n\n\n\nImplementing logistic regression using gradient descent with momentum.\n\n\n\n\n\nApr 3, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\nImplementing one of the oldest machine learning models from scratch.\n\n\n\n\n\nApr 1, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nBlack box classification with the Palmer Penguins dataset.\n\n\n\n\n\nMar 7, 2024\n\n\nLiam Smith\n\n\n\n\n\n\nNo matching items"
  }
]