[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! My name is Liam and I’m a senior at Middlebury College studying geography and mathematics. This blog documents my work in CSCI 0451: Machine Learning."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset contains anatomical measurements for \\(344\\) Penguins of \\(3\\) different species and is commonly used when introducing students to new data science concepts. In this blog post, we construct our first classification model in Python, attempting to accurately predict the species of a given observation in the Palmer Penguins dataset. Constrained to one qualitative and two quantitative predictors, we create our model using the scikit-learn package’s out-of-the-box logistic regression functionality. We select features by conducting 5-fold cross-validation on logistic regression models constructed from every possible combination of one qualitative and two quantitative variables. We implement our two models with the highest cross-validation score, selecting one that achieves 100% test data accuracy as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#abstract",
    "href": "posts/palmer-penguins/index.html#abstract",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset contains anatomical measurements for \\(344\\) Penguins of \\(3\\) different species and is commonly used when introducing students to new data science concepts. In this blog post, we construct our first classification model in Python, attempting to accurately predict the species of a given observation in the Palmer Penguins dataset. Constrained to one qualitative and two quantitative predictors, we create our model using the scikit-learn package’s out-of-the-box logistic regression functionality. We select features by conducting 5-fold cross-validation on logistic regression models constructed from every possible combination of one qualitative and two quantitative variables. We implement our two models with the highest cross-validation score, selecting one that achieves 100% test data accuracy as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-preparation",
    "href": "posts/palmer-penguins/index.html#data-preparation",
    "title": "Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nMost of the code in this section was conveniently provided by Professor Chodrow.\n\n# Load packages \nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport warnings\n\n# Change pd display parameter\npd.set_option('display.max_colwidth', 100)\n\n# Supress warnings to improve web aesthetics\nwarnings.filterwarnings(\"ignore\")\n\n# Load training data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n# Data preparation\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  x = df.drop([\"Species\"], axis = 1)\n  x = pd.get_dummies(x)\n  return x, y, df\n\nX_train, y_train, viz_train = prepare_data(train)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-exploration",
    "href": "posts/palmer-penguins/index.html#data-exploration",
    "title": "Palmer Penguins",
    "section": "Data Exploration",
    "text": "Data Exploration\nBefore constructing a classification model, it is often helpful to get a better feel for the data you are working with. For this reason, we begin our analysis by creating and analyzing a few figures.\n\n# Display plots\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\n\nsns.scatterplot(viz_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", ax = ax[0]);\nsns.move_legend(ax[0], \"lower right\")\nsns.scatterplot(viz_train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\", hue = \"Species\", ax = ax[1]);\nsns.move_legend(ax[1], \"lower right\")\n\n\n\n\n\n\n\n\nIn the first scatterplot, we have almost perfectly separated the three species of penguin using just two variables: flipper length and culmen length. The Adelie penguins tend to have low values of both variables, the Chinstrap penguins tend to have large culmen lengths but small flipper lengths, and the Gentoo penguins tend to have medium culmen lengths and large flipper lengths. One can imagine drawing straight lines in this graph that almost perfectly separate the three species of penguin. The fact that we can visually distinguish between the different species of penguins makes me very confident about our prospects for classification.\nIn the second scatterplot, the Gentoo penguins tend to have small culmen depths and large body masses, separating them from the other two species with some white-space to spare. However, the distributions of Adelie and Chinstrap points look almost identical, occupying the same lower right corner of the plot. If our goal was simply to predict whether a penguin is a Gentoo penguin, then I would imagine that these two variables could help make a strong classifier. However, since we seek to distinguish between all three species, these variables might not be the best choice.\nWhile body mass (or body mass and culmen depth) alone is insufficient to distinguish between the species, perhaps accounting for a categorical variable like sex will reveal some patterns. To explore this possibility, we include the following table of median body mass.\n\n# Display table\nviz_train.groupby(['Sex', 'Species'])[['Body Mass (g)']].median().unstack()\n\n\n\n\n\n\n\n\nBody Mass (g)\n\n\nSpecies\nAdelie\nChinstrap\nGentoo\n\n\nSex\n\n\n\n\n\n\n\nFEMALE\n3350.0\n3525.0\n4700.0\n\n\nMALE\n4025.0\n4050.0\n5500.0\n\n\n\n\n\n\n\nFrom this table, it is clear that the median body masses of all three species vary by sex, with the males tending to weigh more than the females. Additionally, while we already knew from our scatterplot that the Gentoo penguins tend to weigh more than the other species, this table reinforces that conclusion and shows it to be true for both males and females. Unfortunately, accounting for sex appears to do little to further distinguish between Adelie and Chinstrap penguins. There is roughly a 200 gram difference between the median mass of female Adelie and Chinstrap penguins, so perhaps sex in can help distinguish between the two species given that a penguin is female. However, the median mass of male Adelie and Chinstrap penguins differs only by 25 grams. A classifier would almost certainly need more information in order to make accurate predictions."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-construction",
    "href": "posts/palmer-penguins/index.html#model-construction",
    "title": "Palmer Penguins",
    "section": "Model Construction",
    "text": "Model Construction\nSince the Palmer Penguins dataset is relatively small and we are given constraints on the number of features we may include in our model, we will select features by fitting a classifier for every possible combination of one qualitative and two quantitative variables. There may be more robust and less computationally expensive methods of feature selection, but for our purposes, this works.\nWe implement logistic regression using scikit-learn’s out-of-the-box logistic-regression classifier.\n\n# Choosing features\nfrom itertools import combinations\n\n# Distinguish between qualitative and quantitative variables\nall_qual_cols = ['Clutch Completion', 'Sex', 'Island', 'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n# Define classifier\nLR = LogisticRegression(max_iter=100000000)\n\n# Initialize dataframe to store data\nmodels = pd.DataFrame(columns = [\"CV Score\", \"Columns\"])\n\n# Find best-performing classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    cv_score = cv_scores_LR.mean()\n    \n    model = pd.DataFrame({\"CV Score\": [cv_score], \"Columns\": [cols]})\n\n    models = pd.concat([models, model])\n\n\n# Sort by CV Score and display best-performing models\nmodels = models.sort_values(\"CV Score\", ascending=False)\nmodels.head()\n\n\n\n\n\n\n\n\nCV Score\nColumns\n\n\n\n\n0\n0.988311\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\n\n\n0\n0.988311\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\n\n\n0\n0.980543\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Body Mass (g)]\n\n\n0\n0.968854\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Flipper Length (mm)]\n\n\n0\n0.968778\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Delta 13 C (o/oo)]\n\n\n\n\n\n\n\nLet’s fit the model with the highest cross-validation score! There are actually two models with equal performance, so we will try both.\n\n# Test the model\n\n# Import data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Adjust species label\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n# Data preparation\nX_test, y_test, viz_test = prepare_data(test)\n\n# Fit and output the performance of the model\nmodel1 = LR.fit(X_train[models.iloc[[0]]['Columns'][0]], y_train)\nmodel1_score = LR.score(X_test[models.iloc[[0]]['Columns'][0]], y_test)\nmodel1_score\n\n1.0\n\n\nVoila! Logistic regression using the Culmen Length (mm), Culmen Depth (mm), and Island features correctly predicts every observation in our test data, achieving our goal.\n\n# Test the next model\nmodel2 = LR.fit(X_train[models.iloc[[1]]['Columns'][0]], y_train)\nmodel2_score = LR.score(X_test[models.iloc[[1]]['Columns'][0]], y_test)\nmodel2_score\n\n0.9852941176470589\n\n\nOn the other hand, logistic regression using the Culmen Length (mm), Culmen Depth (mm), and Sex features does not achieve 100% accuracy on the test data. Since our first model achieved 100% test accuracy, we select it as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-evaluation",
    "href": "posts/palmer-penguins/index.html#model-evaluation",
    "title": "Palmer Penguins",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nAt this point, we have constructed a well-performing model, so we proceed to model evaluation. First, we visualize our model’s decision regions using the plot_regions() function provided by Professor Chodrow.\n\n# Plotting decision regions function, generously provided by Professor Chodrow\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (10, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout(rect = (0,0,.95,.95))\n\n\n# Reverse the order of columns for the purposes of visualization\nmodels.iloc[[0]]['Columns'][0].reverse()\n\n# Refit model\nLR.fit(X_train[models.iloc[[0]]['Columns'][0]], y_train)\n\n# Plot decision regions on training data\nplot_regions(LR, X_train[models.iloc[[0]]['Columns'][0]], y_train)\nplt.suptitle(\"Decision Regions and Training Data\", fontsize = 13)\n\n# # Plot decision regions on test data\nplot_regions(LR, X_test[models.iloc[[0]]['Columns'][0]], y_test)\nplt.suptitle(\"Decision Regions and Test Data\", fontsize = 13);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the upper row of figures, we plot the training data in our decision regions, while in the lower row, we plot the test data in the decision regions. We trained a model on our training data and implemented it on our test data, so the decision regions are identical in the two rows of figures. The only difference between the rows is the points within the decision regions. Since most observations are part of the training data, there are many more points in the first row of figures. It is interesting to note that Torgersen Island only contains Gentoo penguins, Dream Island does not contain any Adelie penguins, and Biscoe Island does not contain any Chinstrap penguins. These patterns makes it substantially easier to predict a given penguin’s species. One could imagine that the model would always predict Gentoo penguins on Torgersen Island. Similarly, one could imagine the model never predicting Adelie penguins on Dream Island and never predicting Chinstrap penguins on Biscoe Island. Interestingly, the model actually fits decision regions for all three penguin varieties in each island. The decision region for a given species tends to be larger on islands where we would expect to find them, but the model allows for the possibility that there might be a penguin on an island where we would not expect one. Overall, the logistic regression model does a great job of incorporating both qualitative and quantitative variables into its decision regions.\n\n# Predict\nmodel1_pred = model1.predict(X_test[models.iloc[[0]]['Columns'][0]])\n\n# Confusion matrix\nC = confusion_matrix(y_test, model1_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nSince all of the predictions are on the main diagonal of the confusion matrix, we conclude that we did not misclassify any observations – but we already knew this, since our model achieved 100% test accuracy! If our model had not perfectly classified the observations in our test data, a confusion matrix would be more helpful for assessing our model’s misclassification tendencies."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#discussion",
    "href": "posts/palmer-penguins/index.html#discussion",
    "title": "Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, we began by exploring the Palmer Penguins dataset. Our scatterplots and table illustrated that for some variables, there is substantial variation between different species of penguin, while for others, there is very little. Determining which variables would be most valuable for classification required more than simple visualizations. In order to robustly determine the best combination of one qualitative and two quantitative varibles, I fit a logistic regression model on every possible combination. This process was relatively slow and may not work at scale, but it was successful in generating a highly accurate model in this scenario. My final model used the Culmen Length (mm), Culmen Depth (mm), and Island features, achieving 100% accurate predictions on the test data.\nThis assignment furthered my understanding of the entire model-construction procedure, including data preparation, data exploration, feature selection, model fitting, and model evaluation. After completing this blog post, I feel more confident about my ability to implement standard machine learning procedures from Python packages like scikit-learn. While it is relatively easy to implement out-of-the-box classifiers from scikit-learn, it may be more difficult to understand their theoretical underpinnings. As I finish this assignment, I am eager to learn more about the mathematics happening under the hood."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Palmer Penguins\n\n\n\n\n\nBlack box classification with the Palmer Penguins dataset.\n\n\n\n\n\nMar 7, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]