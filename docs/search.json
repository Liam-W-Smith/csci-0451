[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! My name is Liam and I’m a senior at Middlebury College studying geography and mathematics. This blog documents my work in CSCI 0451: Machine Learning."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset contains anatomical measurements for \\(344\\) Penguins of \\(3\\) different species and is commonly used when introducing students to new data science concepts. In this blog post, we construct our first classification model in Python, attempting to accurately predict the species of a given observation in the Palmer Penguins dataset. Constrained to one qualitative and two quantitative predictors, we create our model using the scikit-learn package’s out-of-the-box logistic regression functionality. We select features by conducting 5-fold cross-validation on logistic regression models constructed from every possible combination of one qualitative and two quantitative variables. We implement our two models with the highest cross-validation score, selecting one that achieves 100% test data accuracy as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#abstract",
    "href": "posts/palmer-penguins/index.html#abstract",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset contains anatomical measurements for \\(344\\) Penguins of \\(3\\) different species and is commonly used when introducing students to new data science concepts. In this blog post, we construct our first classification model in Python, attempting to accurately predict the species of a given observation in the Palmer Penguins dataset. Constrained to one qualitative and two quantitative predictors, we create our model using the scikit-learn package’s out-of-the-box logistic regression functionality. We select features by conducting 5-fold cross-validation on logistic regression models constructed from every possible combination of one qualitative and two quantitative variables. We implement our two models with the highest cross-validation score, selecting one that achieves 100% test data accuracy as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-preparation",
    "href": "posts/palmer-penguins/index.html#data-preparation",
    "title": "Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nMost of the code in this section was conveniently provided by Professor Chodrow.\n\n# Load packages \nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport warnings\n\n# Change pd display parameter\npd.set_option('display.max_colwidth', 100)\n\n# Supress warnings to improve web aesthetics\nwarnings.filterwarnings(\"ignore\")\n\n# Load training data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n# Data preparation\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  x = df.drop([\"Species\"], axis = 1)\n  x = pd.get_dummies(x)\n  return x, y, df\n\nX_train, y_train, viz_train = prepare_data(train)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-exploration",
    "href": "posts/palmer-penguins/index.html#data-exploration",
    "title": "Palmer Penguins",
    "section": "Data Exploration",
    "text": "Data Exploration\nBefore constructing a classification model, it is often helpful to get a better feel for the data you are working with. For this reason, we begin our analysis by creating and analyzing a few figures.\n\n# Display plots\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\n\nsns.scatterplot(viz_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", ax = ax[0]);\nsns.move_legend(ax[0], \"lower right\")\nsns.scatterplot(viz_train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\", hue = \"Species\", ax = ax[1]);\nsns.move_legend(ax[1], \"lower right\")\n\n\n\n\n\n\n\n\nIn the first scatterplot, we have almost perfectly separated the three species of penguin using just two variables: flipper length and culmen length. The Adelie penguins tend to have low values of both variables, the Chinstrap penguins tend to have large culmen lengths but small flipper lengths, and the Gentoo penguins tend to have medium culmen lengths and large flipper lengths. One can imagine drawing straight lines in this graph that almost perfectly separate the three species of penguin. The fact that we can visually distinguish between the different species of penguins makes me very confident about our prospects for classification.\nIn the second scatterplot, the Gentoo penguins tend to have small culmen depths and large body masses, separating them from the other two species with some white-space to spare. However, the distributions of Adelie and Chinstrap points look almost identical, occupying the same lower right corner of the plot. If our goal was simply to predict whether a penguin is a Gentoo penguin, then I would imagine that these two variables could help make a strong classifier. However, since we seek to distinguish between all three species, these variables might not be the best choice.\nWhile body mass (or body mass and culmen depth) alone is insufficient to distinguish between the species, perhaps accounting for a categorical variable like sex will reveal some patterns. To explore this possibility, we include the following table of median body mass.\n\n# Display table\nviz_train.groupby(['Sex', 'Species'])[['Body Mass (g)']].median().unstack()\n\n\n\n\n\n\n\n\nBody Mass (g)\n\n\nSpecies\nAdelie\nChinstrap\nGentoo\n\n\nSex\n\n\n\n\n\n\n\nFEMALE\n3350.0\n3525.0\n4700.0\n\n\nMALE\n4025.0\n4050.0\n5500.0\n\n\n\n\n\n\n\nFrom this table, it is clear that the median body masses of all three species vary by sex, with the males tending to weigh more than the females. Additionally, while we already knew from our scatterplot that the Gentoo penguins tend to weigh more than the other species, this table reinforces that conclusion and shows it to be true for both males and females. Unfortunately, accounting for sex appears to do little to further distinguish between Adelie and Chinstrap penguins. There is roughly a 200 gram difference between the median mass of female Adelie and Chinstrap penguins, so perhaps sex in can help distinguish between the two species given that a penguin is female. However, the median mass of male Adelie and Chinstrap penguins differs only by 25 grams. A classifier would almost certainly need more information in order to make accurate predictions."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-construction",
    "href": "posts/palmer-penguins/index.html#model-construction",
    "title": "Palmer Penguins",
    "section": "Model Construction",
    "text": "Model Construction\nSince the Palmer Penguins dataset is relatively small and we are given constraints on the number of features we may include in our model, we will select features by fitting a classifier for every possible combination of one qualitative and two quantitative variables. There may be more robust and less computationally expensive methods of feature selection, but for our purposes, this works.\nWe implement logistic regression using scikit-learn’s out-of-the-box logistic-regression classifier.\n\n# Choosing features\nfrom itertools import combinations\n\n# Distinguish between qualitative and quantitative variables\nall_qual_cols = ['Clutch Completion', 'Sex', 'Island', 'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n# Define classifier\nLR = LogisticRegression(max_iter=100000000)\n\n# Initialize dataframe to store data\nmodels = pd.DataFrame(columns = [\"CV Score\", \"Columns\"])\n\n# Find best-performing classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    cv_score = cv_scores_LR.mean()\n    \n    model = pd.DataFrame({\"CV Score\": [cv_score], \"Columns\": [cols]})\n\n    models = pd.concat([models, model])\n\n\n# Sort by CV Score and display best-performing models\nmodels = models.sort_values(\"CV Score\", ascending=False)\nmodels.head()\n\n\n\n\n\n\n\n\nCV Score\nColumns\n\n\n\n\n0\n0.988311\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\n\n\n0\n0.988311\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\n\n\n0\n0.980543\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Body Mass (g)]\n\n\n0\n0.968854\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Flipper Length (mm)]\n\n\n0\n0.968778\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Delta 13 C (o/oo)]\n\n\n\n\n\n\n\nLet’s fit the model with the highest cross-validation score! There are actually two models with equal performance, so we will try both.\n\n# Test the model\n\n# Import data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Adjust species label\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n# Data preparation\nX_test, y_test, viz_test = prepare_data(test)\n\n# Fit and output the performance of the model\nmodel1 = LR.fit(X_train[models.iloc[[0]]['Columns'][0]], y_train)\nmodel1_score = LR.score(X_test[models.iloc[[0]]['Columns'][0]], y_test)\nmodel1_score\n\n1.0\n\n\nVoila! Logistic regression using the Culmen Length (mm), Culmen Depth (mm), and Island features correctly predicts every observation in our test data, achieving our goal.\n\n# Test the next model\nmodel2 = LR.fit(X_train[models.iloc[[1]]['Columns'][0]], y_train)\nmodel2_score = LR.score(X_test[models.iloc[[1]]['Columns'][0]], y_test)\nmodel2_score\n\n0.9852941176470589\n\n\nOn the other hand, logistic regression using the Culmen Length (mm), Culmen Depth (mm), and Sex features does not achieve 100% accuracy on the test data. Since our first model achieved 100% test accuracy, we select it as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-evaluation",
    "href": "posts/palmer-penguins/index.html#model-evaluation",
    "title": "Palmer Penguins",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nAt this point, we have constructed a well-performing model, so we proceed to model evaluation. First, we visualize our model’s decision regions using the plot_regions() function provided by Professor Chodrow.\n\n# Plotting decision regions function, generously provided by Professor Chodrow\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (10, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout(rect = (0,0,.95,.95))\n\n\n# Reverse the order of columns for the purposes of visualization\nmodels.iloc[[0]]['Columns'][0].reverse()\n\n# Refit model\nLR.fit(X_train[models.iloc[[0]]['Columns'][0]], y_train)\n\n# Plot decision regions on training data\nplot_regions(LR, X_train[models.iloc[[0]]['Columns'][0]], y_train)\nplt.suptitle(\"Decision Regions and Training Data\", fontsize = 13)\n\n# # Plot decision regions on test data\nplot_regions(LR, X_test[models.iloc[[0]]['Columns'][0]], y_test)\nplt.suptitle(\"Decision Regions and Test Data\", fontsize = 13);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the upper row of figures, we plot the training data in our decision regions, while in the lower row, we plot the test data in the decision regions. We trained a model on our training data and implemented it on our test data, so the decision regions are identical in the two rows of figures. The only difference between the rows is the points within the decision regions. Since most observations are part of the training data, there are many more points in the first row of figures. It is interesting to note that Torgersen Island only contains Gentoo penguins, Dream Island does not contain any Adelie penguins, and Biscoe Island does not contain any Chinstrap penguins. These patterns makes it substantially easier to predict a given penguin’s species. One could imagine that the model would always predict Gentoo penguins on Torgersen Island. Similarly, one could imagine the model never predicting Adelie penguins on Dream Island and never predicting Chinstrap penguins on Biscoe Island. Interestingly, the model actually fits decision regions for all three penguin varieties in each island. The decision region for a given species tends to be larger on islands where we would expect to find them, but the model allows for the possibility that there might be a penguin on an island where we would not expect one. Overall, the logistic regression model does a great job of incorporating both qualitative and quantitative variables into its decision regions.\n\n# Predict\nmodel1_pred = model1.predict(X_test[models.iloc[[0]]['Columns'][0]])\n\n# Confusion matrix\nC = confusion_matrix(y_test, model1_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nSince all of the predictions are on the main diagonal of the confusion matrix, we conclude that we did not misclassify any observations – but we already knew this, since our model achieved 100% test accuracy! If our model had not perfectly classified the observations in our test data, a confusion matrix would be more helpful for assessing our model’s misclassification tendencies."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#discussion",
    "href": "posts/palmer-penguins/index.html#discussion",
    "title": "Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, we began by exploring the Palmer Penguins dataset. Our scatterplots and table illustrated that for some variables, there is substantial variation between different species of penguin, while for others, there is very little. Determining which variables would be most valuable for classification required more than simple visualizations. In order to robustly determine the best combination of one qualitative and two quantitative varibles, I fit a logistic regression model on every possible combination. This process was relatively slow and may not work at scale, but it was successful in generating a highly accurate model in this scenario. My final model used the Culmen Length (mm), Culmen Depth (mm), and Island features, achieving 100% accurate predictions on the test data.\nThis assignment furthered my understanding of the entire model-construction procedure, including data preparation, data exploration, feature selection, model fitting, and model evaluation. After completing this blog post, I feel more confident about my ability to implement standard machine learning procedures from Python packages like scikit-learn. While it is relatively easy to implement out-of-the-box classifiers from scikit-learn, it may be more difficult to understand their theoretical underpinnings. As I finish this assignment, I am eager to learn more about the mathematics happening under the hood."
  },
  {
    "objectID": "posts/AAG-GeoAI/index.html",
    "href": "posts/AAG-GeoAI/index.html",
    "title": "GeoAI at the American Association of Geographers’ 2024 Annual Meeting",
    "section": "",
    "text": "Last week, I travelled to Honolulu, Hawaii, where I presented my summer research at the American Association of Geographers’ Annual Meeting. This was my first academic conference, and it was exciting to connect with others in the field and learn from their work. I am particularly interested in the applications of machine learning towards remote sensing, so I made an effort to attend presentations on the topic.\nWith the emergence of large language models and the hype surrounding AI, it appears that a new buzzword – GeoAI – has taken root amongst geographers. There were a whole slew of sessions surrounding different topics in GeoAI, and on April 17, I attended the session on GeoAI for Feature Detection and Recognition. In this session, graduate students and researchers presented on how they are using machine learning and deep learning methods to identify objects and perform classification on remote sensing imagery. In the following sections, I provide a summary of each presentation, as well as my personal reflections on the topic."
  },
  {
    "objectID": "posts/AAG-GeoAI/index.html#introduction",
    "href": "posts/AAG-GeoAI/index.html#introduction",
    "title": "GeoAI at the American Association of Geographers’ 2024 Annual Meeting",
    "section": "",
    "text": "Last week, I travelled to Honolulu, Hawaii, where I presented my summer research at the American Association of Geographers’ Annual Meeting. This was my first academic conference, and it was exciting to connect with others in the field and learn from their work. I am particularly interested in the applications of machine learning towards remote sensing, so I made an effort to attend presentations on the topic.\nWith the emergence of large language models and the hype surrounding AI, it appears that a new buzzword – GeoAI – has taken root amongst geographers. There were a whole slew of sessions surrounding different topics in GeoAI, and on April 17, I attended the session on GeoAI for Feature Detection and Recognition. In this session, graduate students and researchers presented on how they are using machine learning and deep learning methods to identify objects and perform classification on remote sensing imagery. In the following sections, I provide a summary of each presentation, as well as my personal reflections on the topic."
  },
  {
    "objectID": "posts/AAG-GeoAI/index.html#context-enhanced-small-object-detection-through-high-resolution-remote-sensing-imagery",
    "href": "posts/AAG-GeoAI/index.html#context-enhanced-small-object-detection-through-high-resolution-remote-sensing-imagery",
    "title": "GeoAI at the American Association of Geographers’ 2024 Annual Meeting",
    "section": "Context-Enhanced Small Object Detection through High-Resolution Remote Sensing Imagery",
    "text": "Context-Enhanced Small Object Detection through High-Resolution Remote Sensing Imagery\nThe first presentation was by Tao Jiang, who received her PhD in Biomathematics, Bioinformatics, and Computational Biology from the Chinese University of Hong Kong in 2023. Her research goal was to detect individual cars using high-resolution satellite imagery. To do so, with the assumption that all cars are found on roads, she first performed semantic segmentation on her satellite imagery to distinguish roads from non-roads. With a binary image of roads vs non-roads in hand, she then fit deep learning algorithms to search for cars exclusively within roads. As a part of her study, she developed a new deep learning methodology specifically for car identification in satellite imagery and compared the performance of her algorithm and preexisting algorithms. Her model achieved a minor improvement in the overall accuracy rate of car identification, but she intends to continue improving her model. She also wants to expand upon her current work to track changes in traffic over time. Unfortunately, I have been unable to find a publication on her work for further reading.\nPersonally, I think it’s somewhat incredible that we can now detect individual vehicles using instruments that are not even on our planet. While there is a certain wow-factor inherent to Jiang’s research, I couldn’t help but wonder about (1) the practical applications of detecting cars on roads and (2) whether these applications would actually benefit society.\nThere were a few minutes for questions at the end of the presentation, and one person actually asked about the applications. Jiang’s response was that in near-real-time, her model could in theory be used to identify traffic conditions. In response to that, another member of the audience chimed in, asking how that would be any improvement over the preexisting methods for assessing traffic conditions. For example, Google Maps and other services already provide relatively high quality traffic information using location data from smartphones. Jiang did not have a satisfactory response to that point. Personally, I do not see how an imagery-based traffic report would be an improvement, as smartphone GPS data is updated constantly, while satellites need to be physically above the region of interest on relatively cloudless days to be of any use.\nWhile her algorithm may not provide much improvement in identifying traffic conditions, I would imagine that her area of research would be of interest to militaries and intelligence agencies. With transfer learning, it is not difficult to imagine extending her research towards military interests like identifying specific military vehicles or various military developments. For me, this raises concerns regarding the morality of this research and the various military actors that could exploit object identification models. And more generally, the capability to identify objects as small as a car raises a whole host of privacy concerns. Gone are the days where the best available satellite imagery had 30x30 meter pixels. With imagery of high enough resolution to identify individual cars, anybody with access to high resolution imagery could snoop in your backyard from outer space without you ever knowing."
  },
  {
    "objectID": "posts/AAG-GeoAI/index.html#classification-of-usgs-3dep-lidar-point-clouds-using-deep-learning-models",
    "href": "posts/AAG-GeoAI/index.html#classification-of-usgs-3dep-lidar-point-clouds-using-deep-learning-models",
    "title": "GeoAI at the American Association of Geographers’ 2024 Annual Meeting",
    "section": "Classification of USGS 3DEP LiDAR Point Clouds Using Deep Learning Models",
    "text": "Classification of USGS 3DEP LiDAR Point Clouds Using Deep Learning Models\nIn the second presentation, Jung-Kuan Liu described his efforts to create a deep learning model to classify 3DEP LiDAR point cloud data. In order to discuss his work, I should first provide some necessary context regarding 3DEP and LiDAR.\nFirst of all, 3DEP is the USGS’s 3D Elevation Program, which seeks to provide open, consistent, and high-quality LiDAR elevation data across the entire United States. LiDAR stands for light detection and ranging, and LiDAR data is collected by repeatedly bouncing a laser beam from an aircraft to the ground and back, determining elevation using the amount of time it takes for the reflected beam to return to the aircraft. Combined with other information such as the scan angle, calibration, and the aircraft’s coordinates, this process is used to develop a LiDAR point cloud, which is a set of X, Y, and Z-coordinates corresponding to the locations that each laser beam struck a surface. X and Y correspond to longitude and latitude coordinates, respectively, while Z corresponds to elevation. These point clouds can be processed to develop models of the Earth’s surface such as Digital Elevation Models (DEM), which are images with a single band where each pixel’s value represents the elevation at that location. A more detailed discussion of elevation data is beyond the scope of this blog post, but for further information, a good place to start is NOAA’s article on LiDAR.\nLiu works as a Physical Research Scientist at the USGS’s Center of Excellence for Geospatial Information Science, and his presentation revolved around his efforts to improve the landcover classification of USGS’s point cloud data. In particular, the USGS wants to automatically detect whether points represent vegetation, water, buildings, bridges, or other landcover. Current 3DEP standards classify points as ground, non-ground, and unclassified. This is sufficient for extracting “bare earth” DEMs, which are images where each pixel represents the elevation of the ground at that location, and Digital Surface Models (DSM), which are images where each pixel represents the elevation of the very first object that a laser would hit when beamed down from above (for example, trees and buildings). However, these current standards are insufficient for more detailed analyses surrounding the type of landcover each point represents. Liu is seeking to improve upon current standards by training deep learning models on point cloud data, and he has achieved substantial improvement in the variety of landcover classes identified via point cloud classification models. A current weak point of his model is that it is not very successful in its identification of water and bridge features. Liu indicated that water tends to absorb the wavelengths typically used in aerial LiDAR data collection, and that its low reflectance has been a particular challenge to his work.\nAfter improving upon these current weaknesses, Liu hopes to make his model available for public use. In fact, his end-goal is to develop an open-source tool that others can use to perform their own point cloud classification of 3DEP’s point cloud data without actually writing any code. The audience was interested in this product and I think it will be a welcome addition to the current 3DEP program."
  },
  {
    "objectID": "posts/AAG-GeoAI/index.html#detecting-particular-types-of-agriculture-with-satellite-imagery",
    "href": "posts/AAG-GeoAI/index.html#detecting-particular-types-of-agriculture-with-satellite-imagery",
    "title": "GeoAI at the American Association of Geographers’ 2024 Annual Meeting",
    "section": "Detecting Particular Types of Agriculture With Satellite Imagery",
    "text": "Detecting Particular Types of Agriculture With Satellite Imagery\nIn this section, I will actually discuss two presentations due to the similarity in their content and methods. These presentations were delivered by two PhD students at the University of Florida, Torit Chakraborty and Mashoukur Rahaman, who conduct research for the same lab.\nThe first of these presentations was titled From Grapes to Pixels: Advanced Deep Learning Techniques in Stellenbosch Vineyard Management, and Chakraborty’s goal in the project was to detect certain types of vineyards in Stellenbosch, South Africa. Chakraborty fit several different deep learning models, including Convolutional Neural Networks (CNN), Long Short Term Memory (LSTM), and Convo-LSTM, and assessed their performance in order to determine which model was best for the task. In addition to accurately locating vineyards, Chakraborty analyzed changes to the vineyards over time in order to assess the sustainability of Stellenbosch’s wine-tourism economy. While the level of accuracy he achieved was impressive, it was unclear to me what benefit remote sensing had over traditional methods. To my knowledge, Chakraborty has not yet published a paper on the topic, making it difficult to learn more about his methods and rationale.\nThe second presentation was titled Examining Machine and Deep Learning Techniques to Predict Almond Crop Locations in Central Valley California Using Remote Sensing Technologies. Completed in 2023, this study was Rahaman’s Master’s Thesis, and access to his paper appears to be restricted to University of Florida faculty and students at the moment. Rahaman’s goal was to identify locations of almond agriculture in Central Valley, California. To do so, he fit a handful of traditional machine learning models including random forest, K-nearest neighbor, and logistic regression, as well as several deep learning models including Unet, MAnet, and DeepLabv3+. He then compared the performance of his models to determine which model was best for the job. He found that all of the algorithms performed with at least 90% accuracy, with the deep learning models outperforming the machine learning models by one or two percentage points. With simpler models like logistic regression and random forest performing almost as well as complex and black box deep learning architectures, Rahaman spoke to the continued value of simpler models. When a model that is explainable and interpretable has almost the same predictive power as the most sophisticated model available, perhaps it is more valuable to use the simpler model.\nIn both of these presentations, I did not feel satisfied with the discussion of the applications of their work. Yes, it is cool that you can identify a very particular subclass of agriculture with 95% accuracy, but why does that matter? Is it just easier to perform a large-scale remote sensing analysis than to ask the farmers what they are growing? Or is the practical impact actually increased surveillance, like identifying illicit drug plantations? What benefit will this research have on humanity? There may be encouraging answers to these questions, but the presenters failed to point them out. To me, this highlights the importance of communicating not only the technical components of one’s research, but also the tangible impact one’s work could have on the world."
  },
  {
    "objectID": "posts/AAG-GeoAI/index.html#conclusion",
    "href": "posts/AAG-GeoAI/index.html#conclusion",
    "title": "GeoAI at the American Association of Geographers’ 2024 Annual Meeting",
    "section": "Conclusion",
    "text": "Conclusion\nIn this session on GeoAI for Feature Detection and Recognition, I gained exposure to three different yet related areas of research within remote sensing. In the first presentation, I learned that semantic segmentation and deep learning is being used to automate detection of individual cars from satellite imagery. In the second presentation, I learned how deep learning is improving the data quality of 3DEP’s elevation data. And in the third and fourth presentations, I learned about the power of machine learning and deep learning for detecting particular subclasses of agriculture from satellite imagery. Personally, I was impressed with the level of accuracy of these models and the potential of deep learning for automating object identification.\nOn the other hand, I was somewhat disappointed by the extent to which this research appears to be by trial and error. I’m the type of person who wants to understand exactly why something works, from the most basic assumptions to the overarching framework, as well as why a particular algorithm outperforms its alternatives. When you throw a dozen models at a problem and simply select the one with the highest accuracy, research feels less like scientific inquiry and more like an art of algorithm-matching, especially when there is no discussion of the theory behind why one model might perform better than others.\nFurthermore, as I mentioned earlier, I felt that some of the researchers failed to communicate why their work was relevant or important. I understand the fascination surrounding the power of deep learning, but I’m more interested in how these tools can be used to solve tangible and meaningful problems. One of my main takeaways from my first conference experience is how important it is to inform the audience not only about one’s methods, but also about the potential impacts of one’s research."
  },
  {
    "objectID": "posts/final-project/index.html",
    "href": "posts/final-project/index.html",
    "title": "Final Project",
    "section": "",
    "text": "Our project takes on the challenge of predicting population density in regions lacking data. Leveraging landcover image data and tract geometry, our approach involves computing zonal statistics and employing machine learning models. With this problem in mind, we employ satellite data from Connecticut due to its completeness and its potential to also be applied to other Northeastern States within the US. We create Linear Regression models and Spatial Autoregression models with our zonal statistics and tract data. We gauge their efficacy based on their mean-squared error and \\(R^2\\) value. Through this, we find that Linear Regression with No Penalty works best out of our Linear Regression models and our Endogenous Spatial Autoregression model works better than the Exogenous model. Furthermore, we conclude that Spatial Autoregression is more effective at predicting population density than Linear Regression. In regions without adequate census data, the Exogenous model would improve estimations of population density by taking into account the landcover of a given region and its neighbors. Our code and datasets are available through our Github\n\n\n\nIn countries such as the US, there is a large and accurate amount of census data. However there are many ountries in areas where the resources for gathering census data is lesser. This is potentially due to geographic inaccessibility, political conflict, administrative failiure, and as mentioned previously, a lack of resources. Thus, we want a way to predict human populations around the world with the data of the land itself, satellite imagery. With this imaging, the geography is divided into classes which we can then use as variables for our model. Research into this topic has stagnated to a degree, however Tian et al. (2005) produced a hallmark paper which tested the effectivity of modeling population with land cover data. It found that a similar model could have “feasible” and can have “high accuracy”. They utilized Linear Regression, and also manually broke down China into even 1km by 1km cells. Because of availablity of census data, we instead used census tracts, but we continued with the idea of utilizing Linear Regression. With some exploratory graphs of Connecticut, we discovered there might be a Spatial Pattern within our data. In order to take this into account during modeling, we started researching into machine learning algorithms with a spatial component. We came across a paper by Liu, Kounadi, and Zurita-Milla (2022), which concluded that models with a spatial component, such as spatial lag, garner better results than those without. They used spatial lag, and eigvenvectors spatial filtering to predict things beyond our datasets such as soil types. Thus, we sought to create Linear Regression Models and Spatial Autoregressive models, and compare the them to see which is more effective in predicting population density based on land cover.\n\n\n\nNASA in a webinar session called “Humanitarian Applications Using NASA Earth Observations” presented how satellite remote-sensing data could be useful in monitoring humanitarian conditions at refugee settlements. Human settlements could be detected through remote sensing images and therefore could be used to predict the population in a region. This talk alerted us that we still lack necessary population data in many parts of the world, but also demonstrated how remote sensing could be a powerful tool in tackling this problem and solving lack of population data in different countries. Thus, we decide to investigate the connection between remote sensing land cover data and population density in a context with better data coverage.\nThis type of model would be most beneficial by governments and government organizations. These users would most likely be hospital contractors, policy makers, emergency services providers such as ambulances and firefighers, and sociologists. Population census data is crucial for policy makers as it assists in city management so that the equitable distribution of resources can be better calculated.\nThe implications extend beyond helping users. Real people would be affected by this technology. Those who are workers in fields such as emergency service work, or school teachers who might have been over-worked previously may be relieved by the building of new hospitals and schools to compensate for population changes. However, the negative effects are also extremely real.\nImagining that this model expanded beyond the barriers of Connecticut and is being used in countries with much lower census data such as Brazil, there might be a calculation for a forestry company to continue harvesting wood from the Amazon, but they do not want to affect populations. Our algorithm calculates there are very few people in the area, as there is very dense land cover in the Amazon. This company starts to cut down trees and discovers that they are in an area of Indigenous peoples. A minority group that is already negatively affected continues to be disenfranchised. The issue of undercalculating the population density in an area can also affect the amount of resources a policymaker might provide to a region with a much greater population and lacking resources. This would also continue to negatively impact an already negatively impacted area.\nUltimately, the world would be a more equitable and sustainable place if this type of technology could assist countries lacking population data. The positive aspects of providing data where there is none provides the potential for great resource partioning, and better understanding of a countries population.\n\n\n\n\n\nWith this project being the entire state of Connecticut, we utilized landcover data, population, shape files for graphing, and synthesized data which combined our various data sets into manageable datasets suitable for modeling.\nThe bread and butter of our data stems from a 1-meter resolution landcover imagery covering the entire state of Connecticut. Derived from NAIP, the data has already been processed such that every pixel represents a certain class of landcover.\nAt over 800 MB, the dataset is too large to share via GitHub, and is downloadable by clicking on the first option at this link. This landcover dataset was one of the most complete datsets we could find, which is why we wanted to use it for our modelling.\nOur other data sources are the geometries and population data on the Census tract level for the state of Connecticut. We downloaded tract geometries directly into our Jupyter Notebook final_project.ipynb using the Pygris package, and we downloaded the population data from Social Explorer, storing it at data/population.csv.\n\n\n\nFirst, we clean and prepare our data for the model. We start by combining our Tract Geometry of CT with the Population Data of CT to form a new dataset. We utilize both the CT Landcover Data and the Tracts Data in a calculation of Zonal Statistics. This means we calculate the proportion of pixels within each tract that are of a given landcover class. This then is saved as a combined dataset which we then continue to clean by imputing values, performing more advanced Zonal Statistics, and dropping any NA Columns. From there, we are left with data ready to be used in a model.\nThe flowchart below more elegantly outlines this process\n\n\n\n\n\nflowchart LR\n  A(Population Data) --&gt; B(Tracts Data)\n  C(Tracts Geometry Data) --&gt; B(Tracts Data)\n  B --&gt; D{Zonal Statistics}\n  E(CT Landcover Data) --&gt; D{Zonal Statistics}\n  D{Zonal Statistics} --&gt; F(Combined Data)\n  F(Combined Data) --&gt; |Impute Data| G[Ready for Model]\n  F --&gt; |Additional Landcover Statistics| G[Ready for Model]\n  F --&gt; |Drop Uncommon Landcover| G[Cleaned Data]\n\n\n\n\n\n\nWe then implement three types of Linear Regression:\n\nLinear Regression with No Penalty Term\nLinear Regression with \\(\\ell_1\\) Regularization (Lasso Regression)\nLinear Regression with \\(\\ell_1\\) Regularization (Ridge Regression)\n\nBy utilizing the \\(R^2\\) and Mean Squared Error, we quantified the success of each of our models against one another as well as comparing them to sci-kit learn’s own implementations of each of these Linear Regression Models.\nFollowing Linear Regression, we then wanted to implement two types of Spatial AutoRegression:\n\nEndogenous Spatial Autoregression\nExogenous Spatial Autoregression\n\nAs our data can be plotted on a map of Connecticut, we felt it would be amiss to not explore Spatial Autogression. Through this style of model, we can take into account the spatial aspect of each tract when we are predicting. We chose both Endogenous and Exogenous Models. Endogenous Models take into account the neighboring tract population densities of a given tract. Exogenous Models take into account the zonal statistics of a given tract’s neighbors.\nWe merge our data with shape file and calculate the spatial lag of a each tract’s neighbors. The spatial lag is this case is the average population density of a given tracts of land. We also calculate the average landcover types of a given’s tracts neighbors.\nIn total, we create 8 models which we compare in order to determine the best way to predict population density with landcover data\n\n\n\n\n\nflowchart \nA[Cleaned Data] --&gt; B{No Penalty LR}\nA --&gt; C{Lasso LR}\nB --&gt; K{ours}\nB --&gt; L{sci-kit learn}\nC --&gt; G{ours}\nC --&gt; H{sci-kit learn}\nA --&gt; D{Ridge LR}\nD --&gt; I{ours}\nD --&gt; J{sci-kit learn}\nA --&gt; |Spatial Lag Pop Density| E{Endogenous}\nA --&gt; |Spatial Lag Landcover| F{Exogenous}"
  },
  {
    "objectID": "posts/final-project/index.html#abstract",
    "href": "posts/final-project/index.html#abstract",
    "title": "Final Project",
    "section": "",
    "text": "Our project takes on the challenge of predicting population density in regions lacking data. Leveraging landcover image data and tract geometry, our approach involves computing zonal statistics and employing machine learning models. With this problem in mind, we employ satellite data from Connecticut due to its completeness and its potential to also be applied to other Northeastern States within the US. We create Linear Regression models and Spatial Autoregression models with our zonal statistics and tract data. We gauge their efficacy based on their mean-squared error and \\(R^2\\) value. Through this, we find that Linear Regression with No Penalty works best out of our Linear Regression models and our Endogenous Spatial Autoregression model works better than the Exogenous model. Furthermore, we conclude that Spatial Autoregression is more effective at predicting population density than Linear Regression. In regions without adequate census data, the Exogenous model would improve estimations of population density by taking into account the landcover of a given region and its neighbors. Our code and datasets are available through our Github"
  },
  {
    "objectID": "posts/final-project/index.html#introduction",
    "href": "posts/final-project/index.html#introduction",
    "title": "Final Project",
    "section": "",
    "text": "In countries such as the US, there is a large and accurate amount of census data. However there are many ountries in areas where the resources for gathering census data is lesser. This is potentially due to geographic inaccessibility, political conflict, administrative failiure, and as mentioned previously, a lack of resources. Thus, we want a way to predict human populations around the world with the data of the land itself, satellite imagery. With this imaging, the geography is divided into classes which we can then use as variables for our model. Research into this topic has stagnated to a degree, however Tian et al. (2005) produced a hallmark paper which tested the effectivity of modeling population with land cover data. It found that a similar model could have “feasible” and can have “high accuracy”. They utilized Linear Regression, and also manually broke down China into even 1km by 1km cells. Because of availablity of census data, we instead used census tracts, but we continued with the idea of utilizing Linear Regression. With some exploratory graphs of Connecticut, we discovered there might be a Spatial Pattern within our data. In order to take this into account during modeling, we started researching into machine learning algorithms with a spatial component. We came across a paper by Liu, Kounadi, and Zurita-Milla (2022), which concluded that models with a spatial component, such as spatial lag, garner better results than those without. They used spatial lag, and eigvenvectors spatial filtering to predict things beyond our datasets such as soil types. Thus, we sought to create Linear Regression Models and Spatial Autoregressive models, and compare the them to see which is more effective in predicting population density based on land cover."
  },
  {
    "objectID": "posts/final-project/index.html#values-statement",
    "href": "posts/final-project/index.html#values-statement",
    "title": "Final Project",
    "section": "",
    "text": "NASA in a webinar session called “Humanitarian Applications Using NASA Earth Observations” presented how satellite remote-sensing data could be useful in monitoring humanitarian conditions at refugee settlements. Human settlements could be detected through remote sensing images and therefore could be used to predict the population in a region. This talk alerted us that we still lack necessary population data in many parts of the world, but also demonstrated how remote sensing could be a powerful tool in tackling this problem and solving lack of population data in different countries. Thus, we decide to investigate the connection between remote sensing land cover data and population density in a context with better data coverage.\nThis type of model would be most beneficial by governments and government organizations. These users would most likely be hospital contractors, policy makers, emergency services providers such as ambulances and firefighers, and sociologists. Population census data is crucial for policy makers as it assists in city management so that the equitable distribution of resources can be better calculated.\nThe implications extend beyond helping users. Real people would be affected by this technology. Those who are workers in fields such as emergency service work, or school teachers who might have been over-worked previously may be relieved by the building of new hospitals and schools to compensate for population changes. However, the negative effects are also extremely real.\nImagining that this model expanded beyond the barriers of Connecticut and is being used in countries with much lower census data such as Brazil, there might be a calculation for a forestry company to continue harvesting wood from the Amazon, but they do not want to affect populations. Our algorithm calculates there are very few people in the area, as there is very dense land cover in the Amazon. This company starts to cut down trees and discovers that they are in an area of Indigenous peoples. A minority group that is already negatively affected continues to be disenfranchised. The issue of undercalculating the population density in an area can also affect the amount of resources a policymaker might provide to a region with a much greater population and lacking resources. This would also continue to negatively impact an already negatively impacted area.\nUltimately, the world would be a more equitable and sustainable place if this type of technology could assist countries lacking population data. The positive aspects of providing data where there is none provides the potential for great resource partioning, and better understanding of a countries population."
  },
  {
    "objectID": "posts/final-project/index.html#materials-and-methods",
    "href": "posts/final-project/index.html#materials-and-methods",
    "title": "Final Project",
    "section": "",
    "text": "With this project being the entire state of Connecticut, we utilized landcover data, population, shape files for graphing, and synthesized data which combined our various data sets into manageable datasets suitable for modeling.\nThe bread and butter of our data stems from a 1-meter resolution landcover imagery covering the entire state of Connecticut. Derived from NAIP, the data has already been processed such that every pixel represents a certain class of landcover.\nAt over 800 MB, the dataset is too large to share via GitHub, and is downloadable by clicking on the first option at this link. This landcover dataset was one of the most complete datsets we could find, which is why we wanted to use it for our modelling.\nOur other data sources are the geometries and population data on the Census tract level for the state of Connecticut. We downloaded tract geometries directly into our Jupyter Notebook final_project.ipynb using the Pygris package, and we downloaded the population data from Social Explorer, storing it at data/population.csv.\n\n\n\nFirst, we clean and prepare our data for the model. We start by combining our Tract Geometry of CT with the Population Data of CT to form a new dataset. We utilize both the CT Landcover Data and the Tracts Data in a calculation of Zonal Statistics. This means we calculate the proportion of pixels within each tract that are of a given landcover class. This then is saved as a combined dataset which we then continue to clean by imputing values, performing more advanced Zonal Statistics, and dropping any NA Columns. From there, we are left with data ready to be used in a model.\nThe flowchart below more elegantly outlines this process\n\n\n\n\n\nflowchart LR\n  A(Population Data) --&gt; B(Tracts Data)\n  C(Tracts Geometry Data) --&gt; B(Tracts Data)\n  B --&gt; D{Zonal Statistics}\n  E(CT Landcover Data) --&gt; D{Zonal Statistics}\n  D{Zonal Statistics} --&gt; F(Combined Data)\n  F(Combined Data) --&gt; |Impute Data| G[Ready for Model]\n  F --&gt; |Additional Landcover Statistics| G[Ready for Model]\n  F --&gt; |Drop Uncommon Landcover| G[Cleaned Data]\n\n\n\n\n\n\nWe then implement three types of Linear Regression:\n\nLinear Regression with No Penalty Term\nLinear Regression with \\(\\ell_1\\) Regularization (Lasso Regression)\nLinear Regression with \\(\\ell_1\\) Regularization (Ridge Regression)\n\nBy utilizing the \\(R^2\\) and Mean Squared Error, we quantified the success of each of our models against one another as well as comparing them to sci-kit learn’s own implementations of each of these Linear Regression Models.\nFollowing Linear Regression, we then wanted to implement two types of Spatial AutoRegression:\n\nEndogenous Spatial Autoregression\nExogenous Spatial Autoregression\n\nAs our data can be plotted on a map of Connecticut, we felt it would be amiss to not explore Spatial Autogression. Through this style of model, we can take into account the spatial aspect of each tract when we are predicting. We chose both Endogenous and Exogenous Models. Endogenous Models take into account the neighboring tract population densities of a given tract. Exogenous Models take into account the zonal statistics of a given tract’s neighbors.\nWe merge our data with shape file and calculate the spatial lag of a each tract’s neighbors. The spatial lag is this case is the average population density of a given tracts of land. We also calculate the average landcover types of a given’s tracts neighbors.\nIn total, we create 8 models which we compare in order to determine the best way to predict population density with landcover data\n\n\n\n\n\nflowchart \nA[Cleaned Data] --&gt; B{No Penalty LR}\nA --&gt; C{Lasso LR}\nB --&gt; K{ours}\nB --&gt; L{sci-kit learn}\nC --&gt; G{ours}\nC --&gt; H{sci-kit learn}\nA --&gt; D{Ridge LR}\nD --&gt; I{ours}\nD --&gt; J{sci-kit learn}\nA --&gt; |Spatial Lag Pop Density| E{Endogenous}\nA --&gt; |Spatial Lag Landcover| F{Exogenous}"
  },
  {
    "objectID": "posts/final-project/index.html#acquire-tract-geometries",
    "href": "posts/final-project/index.html#acquire-tract-geometries",
    "title": "Final Project",
    "section": "Acquire Tract Geometries",
    "text": "Acquire Tract Geometries\nAs a test of concept, lets utilize the pygris library to access the CT tracts information and then let’s do a simple plot to ensure it’s correct.\n\n# Download geometry\nct_tracts = tracts(state = \"CT\", cb = True, cache = True, year = 2016)\n\n# Display geometry\nfig, ax = plt.subplots()\nct_tracts.plot(ax = ax)\nplt.title(\"Tracts Cartographic Boundaries\");\n\nUsing FIPS code '09' for input 'CT'"
  },
  {
    "objectID": "posts/final-project/index.html#calculate-population-density",
    "href": "posts/final-project/index.html#calculate-population-density",
    "title": "Final Project",
    "section": "Calculate Population Density",
    "text": "Calculate Population Density\nBefore we begin our journey into zonal statistics and eventually creating a predictive model, we first want to understand what the population density looks like in Connecticut. We have some general hypotheses that the areas around New Haven and Hartford are going to have higher amounts of population, and we also expect to see some small pockets of communities around Connecticut.\n\n# Import tracts population data\npop = pd.read_csv(\"../data/population.csv\")\n\n# Convert data type so join key matches\nct_tracts[\"Geo_TRACT\"] = ct_tracts[\"TRACTCE\"].astype(int)\n\n# Join attributes to geometry\ntracts = ct_tracts.merge(pop, how = \"inner\", on='Geo_TRACT')\n\n# Project tracts\ntracts = tracts.to_crs(\"EPSG:3857\")\n\n# Calculate area in KM^2\ntracts[\"Area\"] = tracts.area/1000**2\n\n# Calculate population density\ntracts[\"PopDensity\"] = tracts[\"SE_A00001_001\"]/tracts[\"Area\"]\n\n# Create map\ntracts.plot(\"PopDensity\", legend = True);"
  },
  {
    "objectID": "posts/final-project/index.html#first-steps",
    "href": "posts/final-project/index.html#first-steps",
    "title": "Final Project",
    "section": "First steps",
    "text": "First steps\nHere we open our path to our file, and more importantly, we set up our data to be used in zonal statistics. .read turns our data into a Numpy Array. Following this we are going to .transform our data, which means we are going to take the pixel locations of our coordinates (row col) and map them to our spatial coordinates (x, y). These coordinate values are relative to the CRS (Coordinate Reference System) which we defined earlier as “EPSG:2234”\n\n%%script echo skipping\n#the data can be accessed from https://coastalimagery.blob.core.windows.net/ccap-landcover/CCAP_bulk_download/High_Resolution_Land_Cover/Phase_2_Expanded_Categories/Legacy_Land_Cover_pre_2024/CONUS/ct_2016_ccap_hires_landcover_20200915.zip\nraster_path = '../data/ct_2016_ccap_hires_landcover_20200915.tif'\nlandcover = rasterio.open(raster_path)\narr = landcover.read(1)\naffine = landcover.transform\n\nskipping"
  },
  {
    "objectID": "posts/final-project/index.html#performing-zonal-statistics",
    "href": "posts/final-project/index.html#performing-zonal-statistics",
    "title": "Final Project",
    "section": "Performing Zonal statistics",
    "text": "Performing Zonal statistics\nIt’s as simple as importing rasterstats. We have handled the important data manipulation, and now it’s basically plug and play! One function to note is .to_crs which takes in given coordinate reference system and transforms all the points in our dataframe to match that system.\nThe rasterstats library is very good at getting information from rasters, and we can in fact gain more information by using categorical = True. This allows to see the amount of each type of pixel at a given tract.\n\n%%script echo skipping\ndf_new = zonal_stats(zone, arr, affine=affine, categorical = True)\n\nskipping\n\n\nTaking a look at our dataframe, we can confirm that each column is a type of pixel and each row is a tract\n\n%%script echo skipping\ndf_categorical = pd.DataFrame(df_new)\ndf_categorical\n\nskipping"
  },
  {
    "objectID": "posts/final-project/index.html#visualizing-zonal-stats",
    "href": "posts/final-project/index.html#visualizing-zonal-stats",
    "title": "Final Project",
    "section": "Visualizing Zonal Stats",
    "text": "Visualizing Zonal Stats\nNow that we have information on the amount of each pixel at a given tract, we can find the most common pixel per tract by using the function .idxmax() which will through each row and find the column with the largest value.\n\n%%script echo skipping\ndf_categorical['max_type'] = df_categorical.idxmax(axis=1)\ncombined_df = pd.concat([tracts, df_categorical], axis=1)\ncombined_df['max_type'] = combined_df['max_type'].astype(str)\n\nskipping\n\n\n\n%%script echo skipping\ncombined_df.plot(\"max_type\", legend = True);\n\nskipping\n\n\n\nSaving this data\nThese statistics took quite a while to run, and it may be beneficial to save this data as a csv to continue running statistics in the future\n\n%%script echo skipping\n\ncombined_df.to_csv('../data/combined_data.csv', index=False)\n\nskipping"
  },
  {
    "objectID": "posts/final-project/index.html#data-preparation-1",
    "href": "posts/final-project/index.html#data-preparation-1",
    "title": "Final Project",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we import our data.\n\n# Import and display data\ndata = pd.read_csv(\"../data/combined_data.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n18\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n136572.0\n423692.0\n142589.0\n1378858.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\nNaN\nNaN\n27939.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n11\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\nNaN\nNaN\n13728.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\nNaN\n20584.0\n80161.0\n99956.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\nNaN\nNaN\n9940.0\n68655.0\n486.0\nNaN\nNaN\nNaN\nNaN\n11\n\n\n\n\n5 rows × 87 columns\n\n\n\nLooks like there is some missing data in tracts that contain no pixels of a certain class. Let’s impute 0 for all NaN values.\n\n# Impute 0 for missing data\nprint(\"Before imputation, there were\", pd.isnull(data.iloc[:,68:-1]).sum().sum(), \"NaN values.\")\ndata[pd.isnull(data.iloc[:,68:-1])] = 0\nprint(\"After imputation, there are\", pd.isnull(data.iloc[:,68:-1]).sum().sum(), \"NaN values.\")\ndata.head()\n\nBefore imputation, there were 5774 NaN values.\nAfter imputation, there are 0 NaN values.\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n18\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n136572.0\n423692.0\n142589.0\n1378858.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\n0.0\n0.0\n27939.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n11\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\n0.0\n0.0\n13728.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\n0.0\n20584.0\n80161.0\n99956.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\n0.0\n0.0\n9940.0\n68655.0\n486.0\n0.0\n0.0\n0.0\n0.0\n11\n\n\n\n\n5 rows × 87 columns\n\n\n\nNow that we have complete data, we can calculate the proportion of pixels belonging to each class.\n\n# Calculate total number of pixels in each tract\ndata[\"sum\"] = data.iloc[:,68:-1].sum(axis = 1)\n\n# Calculate proportion of pixels belonging to each class\ndata.iloc[:,68:-2] = data.iloc[:,68:-2].div(data['sum'], axis=0)\n\n# View data\ndata.head()\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\nsum\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n0.069327\n0.023331\n0.225616\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n6111530.0\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\n0.000000\n0.012054\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n11\n2317904.0\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\n0.000000\n0.008350\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n1644135.0\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\n0.013289\n0.051753\n0.064533\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n1548918.0\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\n0.000000\n0.001484\n0.010249\n0.000073\n0.0\n0.0\n0.0\n0.0\n11\n6698858.0\n\n\n\n\n5 rows × 88 columns\n\n\n\n\n# Separate predictors and outcome\nX = data.iloc[:,68:-2]\ny = data[\"PopDensity\"]\n\nWe had an issue where our results were not quite matching those of scikit-learn and we discovered that this was due to a way we set up our dataset. Since we have calculated the proportion of pixels in each tract belonging to each landcover class, the landcovers sum to 1 in every row. Since we create an additional column of ones in order to calculate a y-intercept for linear regression with gradient descent, this means that our y-intercept column is equal to the sum of our other columns. In other words, the constant column is linearly dependent on our other predictor columns. To address this issue, we drop some columns that seem unimportant. Specifically, these columns are mostly zero, meaning that they are not very common in Connecticut anyway.\n\n# Drop some landcovers to address issue of linear combination \nX = X[['2', '5', '11', '12', '8', '13', '14', '15', '20', '21']]"
  },
  {
    "objectID": "posts/final-project/index.html#linear-regression-with-no-penalty-term",
    "href": "posts/final-project/index.html#linear-regression-with-no-penalty-term",
    "title": "Final Project",
    "section": "Linear Regression with No Penalty Term",
    "text": "Linear Regression with No Penalty Term\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit a linear regression model with scikit-learn. We do this simply to verify against our own implementation of linear regression.\n\n# Fit model\n# Doing this just for the purpose of seeing what it looks like\n# We can use the results from this package to verify that our implementation is working properly\n\n#Train and test split creation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nLR_s = LinearRegression() \n\nm = LR_s.fit(X_train, y_train)\n\nLinear regression seeks to minimize the mean squared error, so we report the mean square error from scikit-learn’s model here.\n\n# MSE\nmean_squared_error(y_train, LR_s.predict(X_train))\n\n227396.12768129486\n\n\nLet’s check the \\(R^2\\) value of our model. Recall that \\(R^2\\) is also known as the coefficient of determination, and it represents the proportion of variation in one’s outcome variable that is explained by one’s model.\n\n# R^2 value\nm.score(X_train, y_train)\n\n0.7723901708932351\n\n\nWith an \\(R^2\\) value of roughly \\(0.772\\), our ordinary least squares regression model accounts for about \\(77.2\\)% of the variation of the population densities in Connecticut’s tracts.\nLet’s inspect the y-intercept and coefficients to verify that our coefficients seem logical.\n\n# Y-intercept\nprint(\"Intercept:\", m.intercept_)\n\n# Min and max population density\nprint(\"Population Density Min:\", y_train.min())\nprint(\"Population Density Max:\", y_train.max())\n\nIntercept: 983.2395073145441\nPopulation Density Min: 0.0\nPopulation Density Max: 6084.305602883675\n\n\nSince our predictions are proportions of pixels in a tract of a given landcover, it is impossible for all of our predictors to be zero. Basically this means that no tract will be in the situation where all variables are equal to zero, leaving the y-intercept as its population density. However, in theory, in the absence of any landcover pixels, the population density would be \\(983\\) people per square kilometer. With y_train ranging from 0 to 6084, this seems somewhat reasonable.\n\n# Variable coefficients\nm.coef_\n\narray([  3409.40801231,  -2942.65854175,   -917.38563842,  -4525.6598175 ,\n          668.32452458,  -2125.96537456,  -1746.52921947,  -1576.35637606,\n       -13652.09857612,  -1417.12360532])\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nMost of these coefficients are negative, indicating that as the proportion of pixels representing a given landcover type increases, the population density of the tract decreases. The only positive values are the coefficient of 2, which represents developed impervious landcover, and the coefficient of 8, which represents grassland/herbaceous landcover. We definitely anticipated a positive coefficient for 2, as impervious developed surfaces like buildings and roads are a major marker of human presence. The documentation indicates that while this landcover cannot be used for tilling, it can be used for grazing, so perhaps the positive coefficient is indicative of population density associated with farming. Also, Connecticut is generally forested in rural areas, so grassy areas are likely in suburbia or near urban areas. The magnitude of 2 is much larger than 8, however, indicating that developed impervious landcover is the most important factor increasing population density.\nThe negative coefficients correspond to developed open space, mixed forest, shrub, palustrine forested wetland, palustrine scrub/shrub wetland, palustrine emergent wetland, barren land, and open water. With the exception of developed open space, these landcovers are generally not associated with population density. And developed open space does not necessitate people living in that location – people could live in one tract and commute to a tract with developed open space for recreational purposes, for example. Thus it makes sense that increased values of these variables contribute to less population density.\n\n\nTest Model\nNow that we have evaluated the basic interpretation of our model on our training data, let us check the performance of our model on our testing data. First, we calculate our predictions.\n\n# Create predictions (on test data)\npreds = LR_s.predict(X_test)\n\nLet us inspect the mean square error of our model on the testing data.\n\n# MSE\nmean_squared_error(y_test, preds)\n\n373799.85511504946\n\n\nAt \\(373,800\\), the mean squared error of our model on the testing data is much larger than the mean squared error on the training data, which was \\(227,396\\). This makes sense as our model was fit specifically to the tendencies of the training data.\nTo evaluate the explanatory power of our model, let’s also calculate the \\(R^2\\) value on our testing data.\n\n# Test R^2 value\nr2_score(y_test, preds)\n\n0.7086666350845903\n\n\nAs one might anticipate, the \\(R^2\\) value of the testing data is lower than the training data. However, at \\(0.709\\), the \\(R^2\\) of the testing data is only \\(0.064\\) lower than the \\(R^2\\) of the training data. In other words, our model explains \\(6.4\\)% less of the variation of the population density in our testing data. This is not a negligible amount, but we are still relatively satisfied with a model that explains over \\(70\\)% of the variation in population density.\n\n\n\nOur Implementation\nWe implemented ordinary linear regression with gradient descent in linear_regression.py. Let us train the model using our implementation and verify that our results roughly match those of scikit-learn.\n\nTrain Model\nFirst, we need to convert our training and testing data to the torch.tensor format to match the expected input of our model. We also add a column of ones at the end of the X training and testing data for the purposes of training our y-intercept.\n\n# convert to torch tensors\n# add column of ones for y-intercept\nX_train_torch = torch.cat((torch.tensor(X_train.values), torch.ones((X_train.shape[0], 1))), 1)\ny_train_torch = torch.tensor(y_train.values)\nX_test_torch = torch.cat((torch.tensor(X_test.values), torch.ones((X_test.shape[0], 1))), 1)\ny_test_torch = torch.tensor(y_test.values)\n\nNow that we have our data in the appropriate format, we can train our model.\n\n# fit linear regression model\nLR = LinearRegress()\nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec = []\n\n# fit model\nfor i in range(500000): \n    # update model\n    opt.step(X_train_torch, y_train_torch, alpha = 0.01)\n\n    # calculate and record loss\n    loss = LR.loss(X_train_torch, y_train_torch) \n    loss_vec.append(loss)\n\nLet’s inspect the evolution of our loss function (mean squared error) to verify that our model has converged to a solution.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n# MSE\nprint(\"Mean squared error after training:\", LR.mse(X_train_torch, y_train_torch).item())\n\nMean squared error after training: 227396.1319467965\n\n\n\n\n\n\n\n\n\nGreat! After \\(500,000\\) iterations, our mean squared error is \\(227,396.132\\), which is essentially equivalent to the mean squared error of \\(227,396.128\\) found by scikit-learn.\nLet’s inspect the y-intercept and coefficients to verify that they are similar to scikit-learn’s solution.\n\n# Y-intercept\nLR.w[-1]\n\ntensor(983.0291, dtype=torch.float64)\n\n\nThis y-intercept is also similar to the figure of \\(983.2395\\) reported by scikit-learn.\n\n# Variable coefficients\nprint(\"Coefficients:\", LR.w[:-1])\n\n# Differences in signs\nprint(\"Differences in sign:\", (torch.tensor(m.coef_)*LR.w[:-1]&lt; 0).sum().item())\n\n# Maximum difference in coefficient\nprint(\"Maximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR.w[:-1])).max().item())\n\nCoefficients: tensor([  3409.6334,  -2942.3605,   -917.2350,  -4527.3370,    669.5791,\n         -2126.6514,  -1721.1996,  -1578.8776, -13651.8922,  -1416.8399],\n       dtype=torch.float64)\nDifferences in sign: 0\nMaximum coefficient difference: 25.329603726808955\n\n\nOur coefficients are very similar to those from scikit-learn’s solution! All coefficients have the same sign and the maximum difference between a coefficient in our two models is \\(25\\). Considering the magnitude of the coefficients, this difference is relatively small. Thus the interpretation of our model matches the interpretation of scikit-learn’s model, making us confident that we have implemented linear regression correctly.\n\n# Compute R^2 score\nLR.r2(X_train_torch, y_train_torch)\n\ntensor(0.7724, dtype=torch.float64)\n\n\nOur \\(R^2\\) value is the same as scikit-learn’s.\n\n\nTest Model\nNow we inspect our model’s performance on the testing data.\n\n# MSE\nLR.mse(X_test_torch, y_test_torch)\n\ntensor(373801.8165, dtype=torch.float64)\n\n\nAt \\(373,802\\), our implementation’s testing MSE is very similar to scikit-learn’s \\(373,800\\), indicating similar performance. Once again, this is substantially larger than the training MSE, indicating that our model did not generalize perfectly.\n\n# R^2 value\nLR.r2(X_test_torch, y_test_torch)\n\ntensor(0.7087, dtype=torch.float64)\n\n\nScikit-learn’s testing \\(R^2\\) value was also \\(0.7087\\)! Overall, it appears that we have succesfully implemented linear regression in a manner that achieves similar results to scikit-learn."
  },
  {
    "objectID": "posts/final-project/index.html#linear-regression-with-ell_1-regularization",
    "href": "posts/final-project/index.html#linear-regression-with-ell_1-regularization",
    "title": "Final Project",
    "section": "Linear Regression with \\(\\ell_1\\) Regularization",
    "text": "Linear Regression with \\(\\ell_1\\) Regularization\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit the model with scikit-learn Lasso and inspect the resulting model. As before, we do this simply to verify against our own implementation.\n\n# Fit model\nLR_s_l1 = Lasso(alpha = 1)\n\nm = LR_s_l1.fit(X_train, y_train)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_train, LR_s_l1.predict(X_train)),\n      \"\\nR^2:\", m.score(X_train, y_train),\n      \"\\nY-intercept:\", m.intercept_,\n      \"\\nCoefficients:\\n\", m.coef_)\n\nMSE: 236944.40360579122 \nR^2: 0.7628329217280919 \nY-intercept: -191.72480712350455 \nCoefficients:\n [ 4358.88007237 -1696.29039686   224.06934601    -0.\n     0.            -0.            -0.            -0.\n -6028.66936275  -279.44578393]\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nThe training MSE is slightly larger and the training \\(R^2\\) is slightly smaller than linear regression with no regularizer, which makes sense as we have applied a penalty to help prevent overfitting. The y-intercept is closer to \\(0\\), and many of the coefficients are equal to exactly \\(0\\), making them more interpretable: some coefficients simply do not matter! In this model, landcover 2 (developed impervious) again has a positive coefficient, and with a large magnitude, it remains the main driver in high population density. There is one other variable, 11 (mixed forest), which has a positive coefficient. Interestingly, it was negative in the other model, leading to confusion in its interpretation. But with a somewhat small magnitude, this variable overall has a minor impact on population density, only changing the population density by 224 people per square kilometer as its value increases from 0 to 1. With the \\(\\ell_1\\) regularizer, the landcovers of shrub, grassland/herbaceous, palustrine forested wetland, palustrine scrub/shrub wetland, and palustrine emergent wetland are now equal to zero. These coefficients must not have been that important to the model, as our regularizer made them have zero impact on population density. Variables with negative coefficients are developed open space, barren land, and open water, probably for the same reasons that they were negative earlier.\n\n\nTest Model\nNext, we discover whether the \\(\\ell_1\\) regularizer actually made the model generalize better to the testing data.\n\n# Create predictions (on test data)\npreds = LR_s_l1.predict(X_test)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_test, preds),\n      \"\\nR^2:\", r2_score(y_test, preds))\n\nMSE: 390639.95954704983 \nR^2: 0.6955417389066836\n\n\nOur new MSE of \\(390,639\\) is actually larger than the MSE of \\(373,800\\) with no regularizer, indicating that the \\(\\ell_1\\) regularizer did not help our model generalize to the testing data. Furthermore, the \\(R^2\\) value was larger in the previous model, meaning that the model with no regularizer explained more variation in the outcome variable.\n\n\n\nOur Implementation\nLet’s fit linear regression with the \\(\\ell_1\\) norm with our own implementation and verify that our results match those of scikit-learn. Note that scikit-learn uses an algorithm known as coordinate descent to find their solution, but we learned about gradient descent in this class. Coordinate descent is better suited for lasso regression because it allows some coefficients to equal exactly zero. Gradient descent with the \\(\\ell_1\\) norm makes some coefficients much smaller, but does not cause any of them to equal exactly zero. To mimick their results, in our implementation we set our coefficients equal to zero if they are below a selected threshold. We allow our model \\(5000\\) iterations to begin learning the coefficients before applying this threshold.\n\nTrain Model\n\n# fit linear regression model\nLR_l1 = LinearRegress(penalty = \"l1\", lam = 1) # 1 in scikit-learn\nopt_l1 = GradientDescentOptimizer(LR_l1)\n\n# initialize vector to record loss values\nloss_vec_l1 = []\n\n# fit model\nfor i in range(50000):\n    # update model\n    opt_l1.step(X_train_torch, y_train_torch, alpha = 0.001)\n\n    # set coefs equal to zero after model has had enough learning time\n    if i &gt; 5000:\n        LR_l1.w[torch.abs(LR_l1.w) &lt; 500] = 0\n\n    # calculate and record loss\n    loss = LR_l1.loss(X_train_torch, y_train_torch) \n    loss_vec_l1.append(loss)\n\n# plot the changes in loss \nplt.plot(loss_vec_l1, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIt appears that our model converged to a solution with a similar loss function value! Note that the small upwards blip in the loss function occured at iteration \\(5000\\) when we began allowing our model to set some coefficients equal to zero. Let us inspect our results and compare them to scikit-learn’s output.\n\n# Report results\nprint(\"MSE:\", LR_l1.mse(X_train_torch, y_train_torch).item(),\n      \"\\nR^2:\", LR_l1.r2(X_train_torch, y_train_torch),\n      \"\\nY-intercept:\", LR_l1.w[-1],\n      \"\\nCoefficients:\\n\", LR_l1.w[:-1],\n      \"\\nDifferences in sign:\", (torch.tensor(m.coef_)*LR_l1.w[:-1]&lt; 0).sum().item(),\n      \"\\nMaximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR_l1.w[:-1])).max().item())\n\nMSE: 233736.21710488532 \nR^2: tensor(0.7660, dtype=torch.float64) \nY-intercept: tensor(0., dtype=torch.float64) \nCoefficients:\n tensor([ 4257.6646, -1977.7857,     0.0000,     0.0000,     0.0000,     0.0000,\n            0.0000,     0.0000, -7781.5773,  -551.3119], dtype=torch.float64) \nDifferences in sign: 0 \nMaximum coefficient difference: 1752.9079262978257\n\n\nOur model’s MSE of \\(233,736\\) is slightly smaller than scikit-learn’s MSE of \\(236,944\\) and our model’s \\(R^2\\) of \\(0.7660\\) is slightly larger than scikit-learn’s \\(R^2\\) of \\(0.7628\\), indicating that our linear regression model with the \\(\\ell_1\\) norm performed marginally better than theirs. This difference could have occured due to differences in the optimizer and the number of training iterations. Additionally, these MSE and \\(R^2\\) metrics are both slightly worse than what our implementation achieved with no regularizer, which makes sense as we are attempting to prevent overfitting.\nOne should note that our workaround for setting coefficients equal to zero is not ideal for several reasons. First, we hard-coded a certain threshold for choosing coefficients to set equal to zero, as well as a certain number of iterations at which to begin checking for these low-magnitude coefficients. Most users probably do not want to decide on such a threshold. Second, our method did not exactly replicate the output from scikit-learn. Adjusting our parameters to exactly reproduce the coefficients set to zero proved difficult, and the best we were able to do involved setting the y-intercept and landcover 11 equal to zero, while they were nonzero in scikit-learn’s solution. Landcover 11 represents mixed forest and was the one coefficient with a somewhat counterintuitive value in scikit-learn’s model, so in terms of interpretation, our new model still makes sense. All coefficients have the same sign as scikit-learn’s model with similar magnitudes, making us confident that our model is successfully describing the situation, despite the minor discrepancies.\n\n\nTest Model\n\n# Report results\nprint(\"MSE:\", LR_l1.mse(X_test_torch, y_test_torch),\n      \"\\nR^2:\", LR_l1.r2(X_test_torch, y_test_torch))\n\nMSE: tensor(384765.6761, dtype=torch.float64) \nR^2: tensor(0.7001, dtype=torch.float64)\n\n\nThese values are pretty similar to the ones we have seen already. At \\(384,766\\), our implementation’s MSE is less than scikit-learn’s \\(390,640\\), and at \\(0.7001\\), our implementation’s \\(R^2\\) is slightly more than scikit-learn’s \\(0.6955\\). This means that our model generalized slightly better to the testing data, in addition to performing better on the training data. Again, this can likely be explained by differences in the optimization method and the number of training iterations.\nFurthermore, this MSE is slightly larger than the \\(373,802\\) figure returned by our implementation of linear regression with no penalty term, and this \\(R^2\\) is slighly smaller than the \\(0.7087\\) figure, indicating that linear regression with the \\(\\ell_1\\) penalty did not generalize better to the testing data."
  },
  {
    "objectID": "posts/final-project/index.html#linear-regression-with-ell_2-regularization",
    "href": "posts/final-project/index.html#linear-regression-with-ell_2-regularization",
    "title": "Final Project",
    "section": "Linear Regression with \\(\\ell_2\\) Regularization",
    "text": "Linear Regression with \\(\\ell_2\\) Regularization\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit the model with scikit-learn Ridge and inspect the resulting model. As before, we do this to assess the validity of our own implementation.\n\n# Fit model\nLR_s_l2 = Ridge(alpha = .1)\n\nm = LR_s_l2.fit(X_train, y_train)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_train, LR_s_l2.predict(X_train)),\n      \"\\nR^2:\", m.score(X_train, y_train),\n      \"\\nY-intercept:\", m.intercept_,\n      \"\\nCoefficients:\\n\", m.coef_)\n\nMSE: 235049.69085940512 \nR^2: 0.7647294150800621 \nY-intercept: 69.8570955883946 \nCoefficients:\n [ 4146.87047622 -2058.81157194     6.57006522 -1039.66258053\n   107.03266863  -815.93549227  -127.78253829  -231.19197573\n -6438.07336424  -692.08973348]\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nThe training MSE is larger and the training \\(R^2\\) is smaller than scikit-learn’s linear regression with no regularizer. We anticipated this would be true in comparison to the no regularizer model as the penalty term helps prevent overfitting. It appears that with our chosen parameters, lasso regression performed better than ridge regression in terms of both MSE and \\(R^2\\), but this will change depending on the selected value for parameters.\nThe y-intercept and all coefficients except for landcover 2 are smaller than they were under linear regression without regularization, indicating that the regularization method has been successful in decreasing the magnitude of our coefficients. None of the coefficients are equal to exactly zero, but that is to be expected when working with the \\(\\ell_2\\) penalty.\nThe sign of every coefficient in this model is the same as in the original linear regression model except for landcover 11 (mixed forest), which is now positive and was also positive under lasso regression. However, the magnitude of this coefficient is really small; at 6.57, a location’s population density only changes by 6.57 people per square kilometer as the proportion of pixels represented by mixed forest increases from 0 to 1.\n\n\nTest Model\n\n# Create predictions (on test data)\npreds = LR_s_l2.predict(X_test)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_test, preds),\n      \"\\nR^2:\", r2_score(y_test, preds))\n\nMSE: 387635.2059385676 \nR^2: 0.6978835936921313\n\n\nOn the testing data, our MSE of \\(387,635\\) is similar to the result of \\(390,640\\) with the \\(\\ell_1\\) regularizer but larger than the MSE of \\(373,800\\) with no regularizer, indicating that the \\(\\ell_2\\) regularizer also did not help our model generalize to the testing data better than unregularized linear regression. The \\(R^2\\) value was also larger in linear regression, meaning that the model without regularization explained more variation in the outcome variable.\n\n\n\nOur Implementation\n\nTrain Model\nLet’s fit linear regression with the \\(\\ell_1\\) norm with our own implementation and verify that our results are reasonably similar to those of scikit-learn.\n\n# fit linear regression model\nLR_l2 = LinearRegress(penalty = \"l2\", lam = .1/X_train_torch.shape[0]) \nopt_l2 = GradientDescentOptimizer(LR_l2)\n\n# initialize vector to record loss values\nloss_vec_l2 = []\n\n# fit model\nfor i in range(1000000): \n    # update model\n    opt_l2.step(X_train_torch, y_train_torch, alpha = 0.00001)\n\n    # calculate and record loss\n    loss = LR_l2.loss(X_train_torch, y_train_torch) \n    loss_vec_l2.append(loss)\n\n# plot the changes in loss \nplt.plot(loss_vec_l2, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\n\nm.coef_\n\narray([ 4146.87047622, -2058.81157194,     6.57006522, -1039.66258053,\n         107.03266863,  -815.93549227,  -127.78253829,  -231.19197573,\n       -6438.07336424,  -692.08973348])\n\n\n\n# Report results\nprint(\"MSE:\", LR_l2.mse(X_train_torch, y_train_torch).item(),\n      \"\\nR^2:\", LR_l2.r2(X_train_torch, y_train_torch),\n      \"\\nY-intercept:\", LR_l2.w[-1],\n      \"\\nCoefficients:\\n\", LR_l2.w[:-1],\n      \"\\nDifferences in sign:\", (torch.tensor(m.coef_)*LR_l2.w[:-1]&lt; 0).sum().item(),\n      \"\\nMaximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR_l2.w[:-1])).max().item())\n\nMSE: 246031.0025521462 \nR^2: tensor(0.7537, dtype=torch.float64) \nY-intercept: tensor(-258.6768, dtype=torch.float64) \nCoefficients:\n tensor([ 4417.6162, -1821.4942,   373.5831,  -372.9745,  -251.1601,  -436.8005,\n          -43.2542,  -109.9865, -2181.9550,  -541.2076], dtype=torch.float64) \nDifferences in sign: 1 \nMaximum coefficient difference: 4256.118320893194\n\n\n\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nFirst of all, our implementation does not generate identical output to scikit-learn’s implementation. In order to make our implementation converge to a solution, we needed to make \\(\\lambda\\) far smaller than in their implementation. This may have occurred because we are using the optimization technique of gradient descent, but scikit-learn has implemented a number of more complex techniques and automatically detects which one to use depending on the dataset it receives as input. It is also possible that they implemented their loss function as the sum of squared error rather than the mean squared error. If this is the case, then dividing our \\(\\lambda\\) by the number of observations should theoretically produce identical results. In the code above, we opt for this implementation; however, it should be noted that we have not confirmed whether scikit-learn actually uses the sum of squares in their loss function. Even with this modification, our model has converged to a different solution than theirs, for reasons we have not uncovered.\nAlthough our results are different, they are not drastically different. Our MSE is \\(246,030\\) rather than \\(235,050\\) and our \\(R^2\\) is \\(0.7537\\) rather than \\(0.7647\\), differences that are not ideal but also not terrible. All coefficients have the same sign as their solution except for 8 (grassland/herbaceous). In all prior models, the coefficient of landcover 8 has been positive or zero, but in this model, it is negative! This confuses the interpretation of landcover 8, but with only one discrepancy it does not necessarily ring alarm bells. Perhaps if we had the time to confirm scikit-learn’s loss function and implement the same optimization method we would achive more similar results.\n\n\nTest Model\n\n# Report results\nprint(\"MSE:\", LR_l2.mse(X_test_torch, y_test_torch),\n      \"\\nR^2:\", LR_l2.r2(X_test_torch, y_test_torch))\n\nMSE: tensor(399693.2117, dtype=torch.float64) \nR^2: tensor(0.6885, dtype=torch.float64)\n\n\nAt \\(399,692\\), our implementation’s MSE is more than scikit-learn’s \\(387,635\\) as well as all prior results. And at \\(0.6885\\), our implementation’s \\(R^2\\) is less than scikit-learn’s \\(0.6979\\) and all other results. We could achieve better results by modifying our parameter values, but we were unable to identically reproduce the output of scikit-learn. Overall, our results indicate that for this problem, regularization does not lead to improved performance on the testing data, although it may facilitate interpretation of coefficients."
  },
  {
    "objectID": "posts/final-project/index.html#discussion-of-linear-regression",
    "href": "posts/final-project/index.html#discussion-of-linear-regression",
    "title": "Final Project",
    "section": "Discussion of Linear Regression",
    "text": "Discussion of Linear Regression\nIn linear regression, a major assumption is that all observations are independent of each other. However, when working with spatial data, nearby observations are often similar, such that observations are not independent if they are in close proximity to each other. In order to determine whether our model suffers from such spatial dependence, we will fit a linear regression model on the entire dataset and produce a map of our model’s residuals. We opt for linear regression without regularization due to its higher performance in the work above.\n\n# convert to torch tensors\n# add column of ones for y-intercept\nX_torch = torch.cat((torch.tensor(X.values), torch.ones((X.shape[0], 1))), 1)\ny_torch = torch.tensor(y.values)\n\n# fit linear regression model\nLR_full = LinearRegress()\nopt_full = GradientDescentOptimizer(LR_full)\n\n# fit model\nfor i in range(500000): \n    # update model\n    opt_full.step(X_torch, y_torch, alpha = 0.01)\n\n    # calculate and record loss\n    loss = LR_full.loss(X_torch, y_torch) \n\n\n# calculate residuals\nresid = (y_torch - LR_full.pred(X_torch))\n\n# add residual column to tracts\ntracts[\"resid\"] = resid\n\n# specify that color ramp should be centered at 0\ndivnorm = TwoSlopeNorm(vmin=-3000, vcenter=0., vmax = 3000)\n\n# create map\nresid_map = tracts.plot(\"resid\", legend = True, cmap = \"seismic\", norm = divnorm, figsize = (8,8))\nplt.title(\"Residual Map\");\n\n\n\n\n\n\n\n\nIn an ideal scenario with spatially independent observations, the values of residuals would be distributed randomly throughout the map. However, with clear clusters of red and blue, our model visually appears to be making similar errors in nearby places. In other words, our residuals suffer from spatial autocorrelation. This may occur because the population density in one tract influences the population density in another tract; similarly, the landcover in one tract may influence the population density in a neighboring tract. Fortunately, there exists an entire field of spatial statistics dedicated to addressing issues of spatial autocorrelation. In the following section, we will employ one technique, known as spatial lag regression, in order to account for spatial dependence and hopefully improve our results. Before continuing to our section on spatial autoregression, we first perform cross-validation on linear regression and report the average root mean squared error (RMSE) in order to compare our results to our autoregressive results. We will opt for scikit-learn’s linear regression class since it is faster and achieves identical results to ours.\n\n# define model\nLR = LinearRegression() \n\n# define scoring function\n# this is just required in order to use scikit-learn's cross_val_score function\n# basically they multiply the MSE by -1, so we need to account for that afterwards\nmse_score = make_scorer(mean_squared_error, greater_is_better = False)\n\n# cross validation\ncv_scores_LR = cross_val_score(estimator = LR, X = X, y = y, scoring = mse_score, cv = 4)\n\n# compute average RMSE\nnp.sqrt(-1*cv_scores_LR).mean()\n\n503.0545511056982\n\n\nWith regular linear regression, we have achieved an average cross-validation RMSE of \\(503\\) people per square kilometer. Let’s see if accounting for space can improve our results!"
  },
  {
    "objectID": "posts/final-project/index.html#data-processing-and-exploration",
    "href": "posts/final-project/index.html#data-processing-and-exploration",
    "title": "Final Project",
    "section": "Data Processing and Exploration",
    "text": "Data Processing and Exploration\nIn this spatial autoregression model, we adopt queen criterion to construct spatial continuity weight matrix. The queen criterion defines neighbors as spatial units sharing a common edge or a common vertex. This means that in our model, we will add the features and characteristics of the neighboring tracts as part of the prediction variables.\nTo find the weight matrix, we need to introduce geometry to our dataset. Here, I am merging the csv file to a shapefile and convert the merged data to a GeoDataFrame format. Later, I calculate the queen spatial continuity matrix using the libpysal pacakge. Using the spatial weight continuity matrix, we can then calculate the spatial lag data of population density, which is the mean population density of the neighboring tracts.\n\n# import shapefile\n# need separate shapefile because the one form pygris didn't cooperate with the weights matrix functions\ndata = pd.read_csv(\"../data/combined_data.csv\")\ngdf = gpd.read_file('../data/tl_2016_09_tract.shp')\n\n# create merge columns\ngdf['TRACTCE'] = gdf['TRACTCE'].astype(int)\ndata['TRACTCE'] = data['TRACTCE'].astype(int)\n\n# merge csv with shapfile using TRACTCE\nmerged_gdf = gdf.merge(data, on='TRACTCE', how='left')\n\n# make merged_gdf into geo dataframe\nmerged_gdf = gpd.GeoDataFrame(merged_gdf)\n\n# drop out all rows that have no population density\nmerged_gdf = merged_gdf.dropna(subset=['PopDensity'], axis=0)\n\n# clean tracts that have truncated data on population density\nmerged_gdf = merged_gdf[merged_gdf['PopDensity'] != 0]\nmerged_gdf = merged_gdf[merged_gdf['TRACTCE'] != 194202]\n\n# define the geometry_x column to be the geometry feature \nmerged_gdf.set_geometry(\"geometry_x\", inplace=True)\n\n# calculate Queen's neighbor weights for each tracts\nw = lp.weights.Queen.from_dataframe(merged_gdf)\nw.transform = 'R'\n\n# compute spatial lag of population density\nmerged_gdf['spatial_lag_PopDens'] = lp.weights.lag_spatial(w, merged_gdf['PopDensity'])\n\n# calculate the mean pop density of each tract's neighbors\n#merged_gdf['avg_neighbor_density'] = merged_gdf.groupby('TRACTCE')['spatial_lag'].transform('mean')\nmerged_gdf['PopDensity'] = merged_gdf['PopDensity'].astype(float)\n\n# download merged_gdf to csv file\nmerged_gdf.to_csv('../data/merged_gdf.csv', index=False)\n\n/tmp/ipykernel_18572/2255761594.py:27: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n  w = lp.weights.Queen.from_dataframe(merged_gdf)\n\n\nNext, we want to perform a spatial autocorrelation evaluation using Global Moran’s I index. This evaluation assesses the spatial distribution characteristic of the entire region. We plot the scatter plot between the population denisty and mean population denisty of tract’s neighbors. The Global Moran’s I index, if we do not delve into its mathematical details, is the slope of the best fit line between these two numbers. In our case, we calculated the Moran’s I index to be 0.6. Together with the distribution of the scatter plot, we believe that population density of the neighboring tracts are dependent. We also want to inspect the spatial association at a local scale. The color of each tract is based on its own population density and the population density of its surrounding tracts.\nMoran’s Scatterplot has four categories: High-High, High-Low, Low-High, Low-Low. High/low before the dash means whether the tract has a populuation density that is higher/lower than the mean overall population density. High/low after the dash means whether the tract’s neighbors population denisty is above/below the average population density. After categorization, we map the tracts to inspect the distribution of the tracts’ categories. We find that High-High tracts are usually in urban areas, Low-High tracts are usually suburbs, High-Low tracts are typically towns in the rural area, and Low-Low are rural tracts. Therefore, we believe that by taking into account the characteristics of the target tract’s neighboring tract, we are able to predict population density better than ordinary least square regression.\n\n# read data\nmerged_csv_moran = pd.read_csv(\"../data/merged_gdf.csv\", usecols=['PopDensity', 'spatial_lag_PopDens', \"Geo_NAME\"]).dropna()\n\n# Extract x and y columns from the DataFrame\nx = merged_csv_moran['PopDensity'].values.reshape(-1, 1)  # Reshape to make it a 2D array for scikit-learn\ny = merged_csv_moran['spatial_lag_PopDens'].values\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_csv_moran['spatial_lag_PopDens'].mean()\nq = merged_csv_moran['PopDensity'].mean()\n\n# Categorize the rows based on conditions\nmerged_csv_moran['category'] = 0  # Initialize category column\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-High'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &lt; q), 'category'] = 'Low-High'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &lt; p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-Low'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &lt; p) & (merged_csv_moran['PopDensity'] &lt; q), 'category'] = 'Low-Low'\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_csv_moran['spatial_lag_PopDens'].mean()\nq = merged_csv_moran['PopDensity'].mean()\n\n# Define custom colors for categories\ncolors = {'High-High': '#F47E3E', 'Low-Low': '#0FA3B1', 'Low-High': '#D9E5D6', 'High-Low': '#DCC156'}\n\n# Create a scatter plot of x vs y\nscatter = plt.scatter(x, y, color=merged_csv_moran['category'].map(colors))\n\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(x, y)\n\n# Get the slope and intercept of the fitted line\nslope = model.coef_[0]\nintercept = model.intercept_\n\n# Plot the fitted line\nplt.plot(x, model.predict(x), color='red', label=f'Linear Regression (y = {slope:.2f}x + {intercept:.2f})')\n\n# Add labels and title\nplt.xlabel('Population Density')\nplt.ylabel('Spatial Lag Density')\nplt.title(\"Moran's I = 0.60\")\n\n# Create legend entries manually\nlegend_patches = [\n    Patch(color=color, label=label) for label, color in colors.items()\n]\n\n# Add the legend with custom entries and regression equation\nplt.legend(handles=legend_patches + [scatter, plt.Line2D([0], [0], color='red', label=f'(y = {slope:.2f}x + {intercept:.2f})')])\n\n# Draw horizontal and vertical dashed line at y = p\nplt.axhline(y=p, color='gray', linestyle='--')\nplt.axvline(x=q, color='gray', linestyle='--')\n\n# Show plot\nplt.show()\n\n/tmp/ipykernel_18572/4162410205.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'High-High' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  merged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-High'\n/tmp/ipykernel_18572/4162410205.py:51: MatplotlibDeprecationWarning: An artist whose label starts with an underscore was passed to legend(); such artists will no longer be ignored in the future.  To suppress this warning, explicitly filter out such artists, e.g. with `[art for art in artists if not art.get_label().startswith('_')]`.\n  plt.legend(handles=legend_patches + [scatter, plt.Line2D([0], [0], color='red', label=f'(y = {slope:.2f}x + {intercept:.2f})')])\n\n\n\n\n\n\n\n\n\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_gdf['spatial_lag_PopDens'].mean()\nq = merged_gdf['PopDensity'].mean()\n\n# Categorize the rows based on conditions\nmerged_gdf['category'] = 0  # Initialize category column\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-High'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &lt; q), 'category'] = 'Low-High'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &lt; p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-Low'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &lt; p) & (merged_gdf['PopDensity'] &lt; q), 'category'] = 'Low-Low'\n\n# Define custom colors for categories\ncolors = {'High-High': '#F47E3E', 'Low-Low': '#0FA3B1', 'Low-High': '#D9E5D6', 'High-Low': '#DCC156'}\n\n# Plot the map using custom colors\nfig, ax = plt.subplots(figsize=(10, 10))\nmerged_gdf.plot(column='category', ax=ax, color=merged_gdf['category'].map(colors), legend=True)\nplt.title('Map of Moran Scatterplot Quadrants')\nplt.show()\n\n/tmp/ipykernel_18572/3545356962.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'High-High' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  merged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-High'\n/tmp/ipykernel_18572/3545356962.py:17: UserWarning: Only specify one of 'column' or 'color'. Using 'color'.\n  merged_gdf.plot(column='category', ax=ax, color=merged_gdf['category'].map(colors), legend=True)\n\n\n\n\n\n\n\n\n\nInstead of using all possible land cover types, we are going to use land cover types that are more common among all tracts in CT for density prediction. The land cover types we selected are the same as the ones in linear regression section.\n\n# All landcover types\nall_landcover = ['2', '5', '8', '11', '12', '13', '14', '15', '17', '18', '19', '20', '21', '22', '7', '6', '0', '23']\nall_landcover_pct = ['2pct', '5pct', '8pct', '11pct', '12pct', '13pct', '14pct', '15pct', '17pct', '18pct', '19pct', '20pct', '21pct', '22pct', '7pct', '6pct', '0pct', '23pct']\n\n# Select landcover types\nlandcover_types = ['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'] #, '22', '7', '8', '13', '14', '15', '20', '21'\nlandcover_pct = ['2pct', '5pct', '11pct', '12pct', '8pct', '13pct', '14pct', '15pct', '20pct', '21pct'] # , '22pct', '7pct', '8pct', '13pct', '14pct', '15pct', '20pct', '21pct'\n\n# Merge them into our data\nmerged_gdf['sum'] = merged_gdf[all_landcover].sum(axis=1)\nmerged_gdf[all_landcover_pct] = merged_gdf[all_landcover].div(merged_gdf['sum'], axis=0).multiply(100).astype(float)\n\n# Download merged_gdf to csv file optionally \n#merged_gdf.to_csv('merged_gdf_saved.csv', index=False)"
  },
  {
    "objectID": "posts/final-project/index.html#spatial-lag-regression",
    "href": "posts/final-project/index.html#spatial-lag-regression",
    "title": "Final Project",
    "section": "Spatial Lag Regression",
    "text": "Spatial Lag Regression\n\nEndogenous vs. Exogenous: What’s the Difference?\nThere are two types of spatially lagged regression models. The first one is spatially lagged endogenous regression model. The endogenous model includes the spatial lagged value of the target variable as one of the explanatory variables for regression. In our case, the population density of a tract’s neighbor is part of the variables we use to predict the population density of the tract.\nThe second type of spatially lagged regression model is spatially lagged exogenous regression model. Instead of taking into account the population density, our target variable, of the neighboring tracts, the exogenous model considers the explanatory variables of the tract’s surroundings. In our case, the spatially lagged exogenous model adds neighbors’ land type information to the model. We will calculate the spatial lagged value of each land cover type for all tracts and include them as part of the predictor variables.\nWe first fit both models to the entirety of CT and map their residuals on each tract. First, we fit the endogenous model.\n\n# Endogenous model: consider spatial lag population denisty\npredictor = landcover_pct + ['spatial_lag_PopDens']\n\n# Get explanatory variables and target variable\nX_merged_gdf = merged_gdf[predictor].values\ny_merged_gdf = merged_gdf['PopDensity'].values.reshape(-1, 1)\n\n# Create, fit, and predict with Linear Regression\nmodel = LinearRegression()\nmodel.fit(X_merged_gdf, y_merged_gdf)\ny_pred = model.predict(X_merged_gdf)\n\n# Calculate residuals \nresiduals = y_merged_gdf - y_pred\nmerged_gdf['residuals'] = residuals\n\n# Remove Spatial lag so that our Exogenous model does not take this into account\nmerged_gdf.drop(columns=['spatial_lag_PopDens'], inplace=True)\n\nNext, we fit the exogenous model.\n\n# Exogenous model: consider\nexo_predictor = landcover_pct + ['lag_2pct', 'lag_5pct', 'lag_11pct', 'lag_12pct', 'lag_8pct', 'lag_13pct', 'lag_14pct', 'lag_15pct', 'lag_20pct', 'lag_21pct'] \n\nfor i in range(len(landcover_pct)):\n        merged_gdf['lag_' + landcover_pct[i]] = lp.weights.lag_spatial(w, merged_gdf[landcover_pct[i]])\n\n# Get explanatory variables and target variable\nX_merged_gdf_exo = merged_gdf[exo_predictor].values\ny_merged_gdf_exo = merged_gdf['PopDensity'].values.reshape(-1, 1)\n\n#Create, fit, and predict with Linear Regression\nmodel_exo = LinearRegression()\nmodel_exo.fit(X_merged_gdf_exo, y_merged_gdf_exo)\ny_pred_exo = model_exo.predict(X_merged_gdf_exo)\n\n#Calculate Residuals and make new column\nresiduals_exo = y_merged_gdf_exo - y_pred_exo\nmerged_gdf['residuals_exo'] = residuals_exo\n\nNow, we visualize the map of residuals for both models.\n\n# Define the colors for the custom colormap\ncolors = [(0, 'brown'), (0.5, 'white'), (1, 'green')]  # Position 0 is brown, position 0.5 is white, position 1 is green\n\n# Create the colormap\ncmap = LinearSegmentedColormap.from_list('custom_cmap', colors)\n\n# Determine the range of residuals to be used for normalization\nresiduals_max = max(abs(merged_gdf['residuals_exo'].max()), abs(merged_gdf['residuals'].max()))\nvmax = residuals_max * 0.75  # Adjust the factor as needed\n\n# Create a normalization object\nnorm = Normalize(vmin=-vmax, vmax=vmax)\n\n# First graph\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))  # Create a figure with 1 row and 2 columns\n\n# Graph 1 - Exogenous variables\nmerged_gdf.plot(column='residuals_exo', cmap=cmap, legend=True, ax=axes[0], vmax=vmax, norm=norm)\naxes[0].set_title('Spatial Distribution of Residuals (Exogenous)')\naxes[0].set_xlabel('Longitude')\naxes[0].set_ylabel('Latitude')\n\n# Graph 2 - Spatial lag of PopDensity\nmerged_gdf.plot(column='residuals', cmap=cmap, legend=True, ax=axes[1], vmax=vmax, norm=norm)\naxes[1].set_title('Spatial Distribution of Residuals (Endogenous)')\naxes[1].set_xlabel('Longitude')\naxes[1].set_ylabel('Latitude')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nnum_bins = 50\nhist_range = (0, 2000)\n\n# Create subplots with two columns\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot the first histogram\naxs[0].hist(merged_gdf['residuals_exo'], bins=num_bins, range=hist_range, color='green')\naxs[0].set_xlabel('Absolute Residual')\naxs[0].set_ylabel('Number of Rows')\naxs[0].set_title('Distribution of Absolute Residuals (exogenous)')\n\n# Plot the second histogram\naxs[1].hist(merged_gdf['residuals'], bins=num_bins, range=hist_range, color='green')\naxs[1].set_xlabel('Absolute Residual')\naxs[1].set_ylabel('Number of Rows')\naxs[1].set_title('Distribution of Absolute Residuals (endogenous)')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show plots\nplt.show()\n\n\n\n\n\n\n\n\nThe exogenous spatial lag residual map is on the left and the endogenous spatial lag residual map is on the right. Qualitatively assessing the these two maps, we see both models tend to underestimate the population density in urban areas. It is reasonable as land cover data is only two dimensional and does not account for the vertical height of the buildings. We also see slightly differences in prediction of rural areas between Exogenous and Endogenous. Endogenous is more accurate in rural areas as the landcover is not a sole factor of prediction, and it instead takes into account the population density of the tracts around it. Exogenous is slightly more inaccurate in these regions for the lack of this parameter. Both models tend to have a better performance at predicting density in less populated areas (Low-Low tracts).\nContinuing to explore, we created two residual histograms. We noted that they are similar to our map of CT and do not present any new pattern.\nTo explore a bit deeper into our dataset, we will look at a snapshot of our map around Hartford and its neighboring cities.\n\n# Create a normalization object\nnorm = Normalize(vmin=-2000, vmax=2000)\n\n# First graph\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))  # Create a figure with 1 row and 2 columns\n\n# Graph 2 - Exogenous variables\nmerged_gdf.plot(column='residuals_exo', cmap=cmap, legend=True, ax=axes[0], vmax=vmax, norm=norm)\naxes[0].set_title('Spatial Distribution of Residuals Near Hartford (Exogenous)')\naxes[0].set_xlabel('Longitude')\naxes[0].set_ylabel('Latitude')\n\n# Graph 1 - Spatial lag of PopDensity\nmerged_gdf.plot(column='residuals', cmap=cmap, legend=True, ax=axes[1], vmax=vmax, norm=norm)\naxes[1].set_title('Spatial Distribution of Residuals Near Hartford (Endogenous)')\naxes[1].set_xlabel('Longitude')\naxes[1].set_ylabel('Latitude')\n\naxes[0].set_ylim([41.6, 41.8])\naxes[0].set_xlim([-72.9, -72.5])\naxes[1].set_ylim([41.6, 41.8])\naxes[1].set_xlim([-72.9, -72.5])\n\nplt.show()\n\n\n\n\n\n\n\n\nOne area we were particularly curious about was Hartford as it is a large urban hub in the central of Connecticut. We noticed that we were grossly underestimating densely populated areas, which makes sense as they are relatively large outliers from the rest of the relatively lower population and spread out suburban areas of Connecticut. However, we were better at calculating more densely populated areas with the Endogenous model. We hypothesize this is due to the fact that Endogenous Models inherently take into account the population densities of neighboring tracts. Thus, there is a greater likelihood that the model will “self-correct” by knowing the population of its neighbors."
  },
  {
    "objectID": "posts/final-project/index.html#training-and-testing-cross-validation",
    "href": "posts/final-project/index.html#training-and-testing-cross-validation",
    "title": "Final Project",
    "section": "Training and Testing Cross Validation",
    "text": "Training and Testing Cross Validation\nFor training and testing, we need to separate the data into two. Due to the spatial dependence of tracts, we cannot randomly select tracts from the dataset and assign them to either training or testing data because neighboring tracts will not be in the same dataset. Therefore, to minimize the rupture of spatial relations, we decide to separate training and testing data by neighboring counties to ensure that all tracts in training and testiing data are countinuous. Later, we perform for loops on each set of training and testing data and calculate their mean RMSE for each training and testing set for both endogenous and exogenous model.\n\nmerged_csv = pd.read_csv(\"../data/merged_gdf.csv\")\n\n# Extract the county name from the the Geo_NAME column. \nmerged_gdf['County'] = merged_gdf['Geo_NAME'].str.split(',').str[1].str.strip().str.replace(' ', '')\nmerged_gdf = merged_gdf.dropna(subset=['County'])\n\n\n# Spatially lagged endogenous regressor\nodd_counties = ['NewLondonCounty', 'NewHavenCounty', 'LitchfieldCounty', 'TollandCounty']\neven_counties = ['MiddlesexCounty', 'FairfieldCounty','HartfordCounty', 'WindhamCounty']\n\nrmse = []\n\nfor i in range(4):\n    # Splitting training and testing counties\n    train_1 = merged_gdf[(merged_gdf['County'] != odd_counties[i]) & (merged_gdf['County'] != even_counties[i])]\n    test_1 = merged_gdf[(merged_gdf['County'] == odd_counties[i]) | (merged_gdf['County'] == even_counties[i])]\n\n    # Queen weight matrix for each train and test\n    train_1_w = lp.weights.Queen.from_dataframe(train_1)\n    test_1_w = lp.weights.Queen.from_dataframe(test_1)\n    \n    # Regularize the weights\n    train_1_w.transform = 'R'\n    test_1_w.transform = 'R'\n    \n    # Calculate the spatial lag pop density\n    train_1['spatial_lag_PopDens'] = lp.weights.lag_spatial(train_1_w, train_1['PopDensity'])\n    test_1['spatial_lag_PopDens'] = lp.weights.lag_spatial(test_1_w, test_1['PopDensity'])\n    \n    y_train = np.array(train_1['PopDensity']).reshape((-1,1))\n    x_train = np.array(train_1[predictor])\n\n    y_test = np.array(test_1['PopDensity'])\n    x_test = np.array(test_1[predictor])\n\n    # Fit linear regression model using scikit-learn \n    model = LinearRegression()\n    model.fit(x_train, y_train)\n\n    # Predict on test data\n    y_pred_test = model.predict(x_test)\n\n    # Calculate RMSE\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\n    rmse.append(test_rmse)\n\n\nnp.mean(rmse)\n\n382.27553702535505\n\n\nThe average root mean square error of the spatially lagged endogenous regression model is 382.28. The endogenous model is more advantageous when we have a relatively higher coverage of census data and we need to predict the population density of small region surrounded by regions with good census.\nNext, we do training and testing cross validation for the exogenous spatial lagged model.\n\n# Spatially lagged exogenous regressors\n\nrmse_exo = []\n\n# Set loops for each set of different counties\nfor i in range(4):\n\n    train_1 = merged_gdf[(merged_gdf['County'] != odd_counties[i]) & (merged_gdf['County'] != even_counties[i])]\n    test_1 = merged_gdf[(merged_gdf['County'] == odd_counties[i]) | (merged_gdf['County'] == even_counties[i])]\n\n    train_1_w = lp.weights.Queen.from_dataframe(train_1)\n    test_1_w = lp.weights.Queen.from_dataframe(test_1)\n\n    train_1_w.transform = 'R'\n    test_1_w.transform = 'R'\n\n    # Calculate spatial lag \n    for j in range(len(landcover_pct)):\n        train_1['lag_' + landcover_pct[j]] = lp.weights.lag_spatial(train_1_w, train_1[landcover_pct[j]])\n        test_1['lag_' + landcover_pct[j]] = lp.weights.lag_spatial(test_1_w, test_1[landcover_pct[j]])\n    \n    # Extract training and test data \n    y_train = np.array(train_1['PopDensity']).reshape((-1,1))\n    x_train = np.array(train_1[exo_predictor])\n\n    y_test = np.array(test_1['PopDensity'])\n    x_test = np.array(test_1[exo_predictor])\n\n    # Fit linear regression model using scikit-learn \n    model = LinearRegression()\n    model.fit(x_train, y_train)\n\n    # Predict on test data\n    y_pred_test = model.predict(x_test)\n\n    # Calculate RMSE\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\n    rmse_exo.append(test_rmse)\n\n\nnp.mean(rmse_exo)\n\n391.66561692553\n\n\nThe average RMSE of the spatially lagged endogenous regression model cross validation is 391.67, which is slightly larger than the RMSE of the endogenous model. The exogenous model is more applicable to scenarios when we have good satellite data but sparse census data.\nComparing our Spatial Autoregression to our Linear regression, it is clear that our Spatial Regression yields better results."
  },
  {
    "objectID": "posts/final-project/index.html#concluding-discussion",
    "href": "posts/final-project/index.html#concluding-discussion",
    "title": "Final Project",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nThrough this project, we were able to implement three different forms of Linear Regression, as well as create Spatial Autoregression models, and we determined the efficacy of each of these models both mathematically and graphically. Our results were relatively similar to Tian et al. (2005) in that they underpredicted the population density in densely populated urban areas more frequently than other plots of land, and over-predicted population density in rural areas. Overall, we accomplished a few key things with project. Through our models, we were able to predict population density with only landcover with relatively strong accuracy. We successfully compared different machine learning models and concluded that Spatial Autoregression was more accurate than Linear Regression. With more time, we would have liked to implement Poisson Regression and performed analysis at the block group level instead of tract level. With more computational power, we would have liked to calculate a larger dataset, representing a larger spatial region. Overall, we are proud of our work!"
  },
  {
    "objectID": "posts/final-project/index.html#group-contributions-statement",
    "href": "posts/final-project/index.html#group-contributions-statement",
    "title": "Final Project",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nLiam helped with data acquisition and preparation, wrote our implementation of linear regression with gradient descent in linear_regression.py, and compared the output of our class with that of scikit-learn. Alex acquired landcover data from Conus, and shapefile data of Connecticut. He then implemented zonal statistics with Manny. Alex explained the differences in Spatial Autoregression methods, trained the models, and utilized cross validation. Manny created visualizations of our models to represent graphically the residuals of each model. He proof-read all of our code, making corrections, rewriting descriptions, and ensuring cohesive and consistent writing styles. He also contributed to code in the Spatial Auto Regression section."
  },
  {
    "objectID": "posts/final-project/index.html#personal-reflection",
    "href": "posts/final-project/index.html#personal-reflection",
    "title": "Final Project",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nAs my contributions to this project were primarily related to linear regression, I learned a lot about the implementation of linear regression with gradient descent. I learned to implement lasso regression with the \\(\\ell_1\\) norm and ridge regression with the \\(\\ell_2\\) norm and gained an understanding of the differences between the two penalties. I also learned that there are a lot of fancier optimization methods implemented by scikit-learn that run faster and in some cases like coordinate descent actually yield different output. In collaboration with my project partners, I also learned about spatial autoregression and its advantages when modeling spatial phenomena. I feel proud of our work, as we successfully implemented linear regression from scratch and conducted a spatial regression extension that we had not originally planned. I think it is interesting how accounting for landcover at a location’s neighbors can yield more accurate predictions than considering the landcover at that location alone. As I hope to pursue a career in spatial data science, gaining this deeper understanding of linear regression and exposure to spatial autoregression will be helpful as I move into the professional world."
  },
  {
    "objectID": "posts/newtons-method/index.html",
    "href": "posts/newtons-method/index.html",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "In this blog post, we expand upon our previous implementation of logistic regression to add another optimization option. In our updated implementation of logistic regression, available at logistic.py, we can now optimize our logistic regression model with both gradient descent (with momentum) and Newton’s method. After implementing this extension, we demonstrate with an example that logistic regression with both optimizers converges to the same solution. Using the same example, we reveal that logistic regression with Newton’s method commonly converges to a solution in fewer training iterations than gradient descent. We also illustrate that selecting an appropriate value for \\(\\alpha\\) is critical to Newton’s method, as the optimizer will not converge to a solution if \\(\\alpha\\) is too large. Finally, we count the number of operations required in each iteration of model training for both gradient descent and Newton’s method, revealing that Newton’s method is more computationally expensive than gradient descent, especially when working with many features."
  },
  {
    "objectID": "posts/newtons-method/index.html#abstract",
    "href": "posts/newtons-method/index.html#abstract",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "In this blog post, we expand upon our previous implementation of logistic regression to add another optimization option. In our updated implementation of logistic regression, available at logistic.py, we can now optimize our logistic regression model with both gradient descent (with momentum) and Newton’s method. After implementing this extension, we demonstrate with an example that logistic regression with both optimizers converges to the same solution. Using the same example, we reveal that logistic regression with Newton’s method commonly converges to a solution in fewer training iterations than gradient descent. We also illustrate that selecting an appropriate value for \\(\\alpha\\) is critical to Newton’s method, as the optimizer will not converge to a solution if \\(\\alpha\\) is too large. Finally, we count the number of operations required in each iteration of model training for both gradient descent and Newton’s method, revealing that Newton’s method is more computationally expensive than gradient descent, especially when working with many features."
  },
  {
    "objectID": "posts/newtons-method/index.html#generating-training-data",
    "href": "posts/newtons-method/index.html#generating-training-data",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Generating Training Data",
    "text": "Generating Training Data\nFirst, we import the packages we will need for this assignment.\n\n# Import packages\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nNext, we generate the data that we will use to train our model. Thank you to Professor Chodrow for providing the functions for generating and visualizing training data.\n\n# define function to generate data\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n# define function to plot data\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -0.5, vmax = 1.5, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# generate data\nX, y = classification_data(n_points = 500, noise = 0.6)\n\n# plot data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_data(X, y, ax)\nax.set_title(\"Training Data\");"
  },
  {
    "objectID": "posts/newtons-method/index.html#convergence-to-an-appropriate-decision-boundary",
    "href": "posts/newtons-method/index.html#convergence-to-an-appropriate-decision-boundary",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Convergence to an Appropriate Decision Boundary",
    "text": "Convergence to an Appropriate Decision Boundary\nTo begin evaluating the efficacy of our model, we fit logistic regression on our training data with regular gradient descent and with Newton’s method, illustrating that both options converge to the same solution.\n\n# set seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and gradient descent optimizer\nLR_gradient = LogisticRegression() \nopt_gradient = GradientDescentOptimizer(LR_gradient)\n\n# initialize vector to record loss values\nloss_vec_gradient = [10, 5]\n\n# fit model\nwhile loss_vec_gradient[-2] - loss_vec_gradient[-1] &gt; 0.00001:\n\n    # update model\n    opt_gradient.step(X, y, alpha = 0.07, beta = 0)\n\n    # calculate and record loss\n    loss = LR_gradient.loss(X, y) \n    loss_vec_gradient.append(loss)\n\n# remove initialization values\nloss_vec_gradient = loss_vec_gradient[2:]\n\n# reset seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and newton's optimizer\nLR_newton = LogisticRegression() \nopt_newton = NewtonOptimizer(LR_newton)\n\n# initialize vector to record loss values\nloss_vec_newton = [10, 5]\n\n# fit model\nwhile loss_vec_newton[-2] - loss_vec_newton[-1] &gt; 0.00001:\n\n    # update model\n    opt_newton.step(X, y, alpha = 10)\n\n    # calculate and record loss\n    loss = LR_newton.loss(X, y) \n    loss_vec_newton.append(loss)\n\n# remove initialization values\nloss_vec_newton = loss_vec_newton[2:]\n\nNow that we have fit our models, let’s inspect the decision boundaries in the context of our training data.\n\n# define function to draw line\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# plot decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_data(X, y, ax)\ndraw_line(LR_gradient.w, x_min = -1.25, x_max = 2.25, ax = ax, color = \"black\", label = \"Gradient Descent\")\ndraw_line(LR_newton.w, x_min = -1.25, x_max = 2.25, ax = ax, color = \"red\", label = \"Newton's Optimizer\")\nax.legend(loc = \"lower left\")\nax.set_title(\"Training Data and Decision Boundaries\");\n\n\n\n\n\n\n\n\nThe decision boundaries for the gradient descent optimizer and Newton’s optimizer are virtually identical! This result makes me confident that we have correctly implemented logistic regression with Newton’s optimizer."
  },
  {
    "objectID": "posts/newtons-method/index.html#the-speed-of-newtons-optimizer",
    "href": "posts/newtons-method/index.html#the-speed-of-newtons-optimizer",
    "title": "Newton’s Method for Logistic Regression",
    "section": "The Speed of Newton’s Optimizer",
    "text": "The Speed of Newton’s Optimizer\nIn the previous section we saw that logistic regression with gradient descent and Newton’s method converge to the same solution. Naturally, one might wonder whether both options approach the solution at the same rate. As it turns out, when an appropriate value of \\(\\alpha\\) is selected, logistic regression with Newton’s optimizer converges to a solution in far fewer iterations than logistic regression with classic gradient descent. To illustrate this statement, we plot the change in loss over the iterations of both models’ training.\n\n# plot the changes in loss \nplt.plot(loss_vec_gradient, color = \"slategrey\", lw = 3, label = \"Gradient Descent\")\nplt.plot(loss_vec_newton, color = \"#A37933\", lw = 3, label = \"Newton's Optimizer\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.legend(loc = \"upper right\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nAs we would expect, the two optimizers begin with identical loss values. However, from the very beginning, the loss values of logistic regression with Newton’s optimizer decrease more rapidly than logistic regression with gradient descent. In fact, Newton’s approach achieves an incremental improvement to the loss function of less than our threshold of \\(0.00001\\) in just over \\(200\\) iterations, whereas the gradient descent approach requires over \\(1,100\\) iterations. For this data and this choice of \\(\\alpha\\), the Newton’s optimizer approach requires dramatically fewer iterations than regular gradient descent."
  },
  {
    "objectID": "posts/newtons-method/index.html#when-alpha-is-too-large",
    "href": "posts/newtons-method/index.html#when-alpha-is-too-large",
    "title": "Newton’s Method for Logistic Regression",
    "section": "When \\(\\alpha\\) is Too Large",
    "text": "When \\(\\alpha\\) is Too Large\nThe learning rate \\(\\alpha\\) impacts how far logistic regression with Newton’s method moves the decision boundary in any given iteration. When \\(\\alpha\\) is very small, each change to the decision boundary becomes similarly small, such that it takes many iterations to converge to an adequate solution. In the example above, \\(\\alpha\\) was large enough that logistic regression with Newton’s optimizer converged to a solution more rapidly than ordinary gradient descent. But what happens when \\(\\alpha\\) is substantially larger? To answer this question, we experiment with fitting a model where \\(\\alpha = 10,000\\) in the code chunk below.\n\n# reset seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and newton's optimizer\nLR_newton_10000 = LogisticRegression() \nopt_newton_10000 = NewtonOptimizer(LR_newton_10000)\n\n# initialize vector to record loss values\nloss_vec_newton_10000 = [10, 5]\n\n# initialize counter\ncounter = 0\n\n# fit model\nwhile loss_vec_newton_10000[-2] - loss_vec_newton_10000[-1] &gt; 0.00001:\n    # update model\n    opt_newton_10000.step(X, y, alpha = 10000)\n\n    # calculate and record loss\n    loss = LR_newton_10000.loss(X, y) \n    loss_vec_newton_10000.append(loss)\n    \n    # update counter\n    counter += 1\n\n# remove initialization values\nloss_vec_newton_10000 = loss_vec_newton_10000[2:]\n\n# print results\nprint(\"Total iterations:\", counter, \"\\nResulting loss vector\", loss_vec_newton_10000)\n\nTotal iterations: 1 \nResulting loss vector [tensor(nan)]\n\n\nApparently, by increasing \\(\\alpha\\) to \\(10,000\\), we prevented the logistic regression model with Newton’s optimizer from converging to a solution. In fact, with \\(\\alpha\\) set so high, the model produces a loss value of NaN, terminating our training loop in a single iteration. This likely occured because the large value of \\(\\alpha\\) caused the decision boundary to move so far that the loss function’s value exceeded the maximum value that torch can store."
  },
  {
    "objectID": "posts/newtons-method/index.html#operation-counting",
    "href": "posts/newtons-method/index.html#operation-counting",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Operation Counting",
    "text": "Operation Counting\nLet \\(C_{gd}\\) represent the total computational cost of fitting a logistic regression model with gradient descent and \\(C_{nm}\\) represent the total computational cost of fitting a logistic regression model with Newton’s method.\nSuppose that gradient descent converges to an adequate solution in \\(t_{gd}\\) iterations and Newton’s method converges to an adequate solution in \\(t_{nm}\\) iterations. Furthermore, per Professor Chodrow’s instructions, we assume that the following operations are associated with the following computational costs.\n\nComputing the loss: \\(c\\) computational units\nComputing the gradient: \\(2c\\) units\nComputing the Hessian: \\(pc\\) units\nInverting a \\(p \\times p\\) matrix: \\(k_1p^\\gamma\\) units\nPerforming the matrix-vector multiplication required by Newton’s method: \\(k_2p^2\\)\n\nIn each iteration of gradient descent, we calculate the loss and the gradient once, leading to the following total computational cost.\n\\(C_{gd} = t_{gd} (c + 2c) = t_{gd}(3c)\\)\nIn each iteration of Newton’s method, we calculate the loss, Hessian, inverse Hessian, matrix multiplication, and gradient once, leading to the following total computational cost.\n\\(C_{nm} = t_{nm}(c + pc + k_1p^\\gamma + k_2p^2 + 2c) = t_{nm}(3c + pc + k_1p^\\gamma + k_2p^2)\\)\nIf Newton’s method requires fewer computational units to complete than gradient descent, we have the following inequalities.\n\\[C_{nm} &lt; C_{gd}\\] \\[t_{nm}(3c + pc + k_1p^\\gamma + k_2p^2) &lt; t_{gd}(3c)\\] \\[\\frac{(3c + pc + k_1p^\\gamma + k_2p^2)}{3c} &lt; \\frac{t_{gd}}{t_{nm}}\\]\nThus, Newton’s method will only require less computational effort than gradient descent if the total number of iterations required to complete gradient descent \\(t_{gd}\\) is at least \\(\\frac{(3c + pc + k_1p^\\gamma + k_2p^2)}{3c}\\) times as much as the number of iterations required to complete Newton’s method.\nSince \\(p\\) is raised to the power of \\(2\\) and to the power of \\(\\gamma\\) in this formula, the ratio between \\(t_{gd}\\) and \\(t_{nm}\\) required for Newton’s method to be more efficient than gradient descent increases dramatically as \\(p\\) increases. Practically speaking, when working with datasets of many variables, this is unlikely to happen. For this reason, if working on a dataset with many variables, gradient descent will almost certainly be more efficient than Newton’s method."
  },
  {
    "objectID": "posts/newtons-method/index.html#conclusion",
    "href": "posts/newtons-method/index.html#conclusion",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nCompleting this assignment gave me the opportunity to understand the strengths and weaknesses of two different optimization methods for logistic regression. By implementing and performing experiments with both gradient descent and Newton’s method, I learned that Newton’s method may converge to an adequate solution in fewer training iterations than gradient descent. By counting the number of operations required for both optimizers, I discovered that fewer iterations does not necessarily mean lower computational complexity. In fact, when working with a large number of features, gradient descent is almost certainly less computationally expensive than Newton’s method. In summary, if given the choice between gradient descent and Newton’s method, choose gradient descent!"
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In this blog post, we implement logistic regression via empirical risk minimization and perform a few experiments to highlight the model’s strengths and weaknesses. As a part of our experimentation, we implement gradient descent with momentum and investigate the evolution of our loss function in comparison to ordinary gradient descent. We also test our model on data with more dimensions than observations in order to illustrate the perils of overfitting. To see my implementation of logistic regression, please visit logistic.py."
  },
  {
    "objectID": "posts/logistic-regression/index.html#abstract",
    "href": "posts/logistic-regression/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In this blog post, we implement logistic regression via empirical risk minimization and perform a few experiments to highlight the model’s strengths and weaknesses. As a part of our experimentation, we implement gradient descent with momentum and investigate the evolution of our loss function in comparison to ordinary gradient descent. We also test our model on data with more dimensions than observations in order to illustrate the perils of overfitting. To see my implementation of logistic regression, please visit logistic.py."
  },
  {
    "objectID": "posts/logistic-regression/index.html#generating-training-data",
    "href": "posts/logistic-regression/index.html#generating-training-data",
    "title": "Implementing Logistic Regression",
    "section": "Generating Training Data",
    "text": "Generating Training Data\nFirst, we import the packages we will need for this assignment.\n\n# Import packages\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nNext, we generate the data that we will use to train our model. Thank you to Professor Chodrow for providing the functions for generating and visualizing training data.\n\n# Define function to generate data\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n# define function to plot data\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -0.5, vmax = 1.5, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# generate data\nX, y = classification_data(n_points = 500, noise = 0.6)\n\n# plot data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_data(X, y, ax)\nax.set_title(\"Training Data\");"
  },
  {
    "objectID": "posts/logistic-regression/index.html#vanilla-gradient-descent",
    "href": "posts/logistic-regression/index.html#vanilla-gradient-descent",
    "title": "Implementing Logistic Regression",
    "section": "Vanilla Gradient Descent",
    "text": "Vanilla Gradient Descent\nTo begin evaluating the efficacy of our model, we fit logistic regression on our training data with regular gradient descent. We actually only implemented gradient descent with momentum in logistic.py, but if we set \\(\\beta = 0\\), as we do below, this is equivalent to regular gradient descent.\n\n# set seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec_vanilla = []\n\n# fit model\nwhile len(loss_vec_vanilla) &lt; 2 or loss_vec_vanilla[-2] - loss_vec_vanilla[-1] &gt; 0.00001:\n\n    # update model\n    opt.step(X, y, alpha = 0.07, beta = 0)\n\n    # calculate and record loss\n    loss = LR.loss(X, y) \n    loss_vec_vanilla.append(loss)\n\nNow that we have fit our model, let’s inspect our decision boundary in the context of our training data.\n\n# define function to draw line\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# plot decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_data(X, y, ax)\ndraw_line(LR.w, x_min = -1.25, x_max = 2.25, ax = ax, color = \"black\")\nax.set_title(\"Training Data and Decision Boundary\");\n\n\n\n\n\n\n\n\nVisually, this line appears to be an intelligent choice. There are some misclassified points on either side of the line, but our data is not linearly separable, so it is impossible to correctly classify all points using a linear decision boundary. This visual check leads me to believe that our classifier is performing well. As another check, we plot the evolution of our loss function below to verify that it decreases monotonically.\n\n# plot the changes in loss \nplt.plot(loss_vec_vanilla, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn this graph, every iteration has a lower loss value than the iteration that precedes it. In other words, our loss function decreases monotonically, as guaranteed by gradient descent.\n\nThe Benefits of Momentum\nOur model appears to be behaving as expected, leading us to the natural question: can we do better if we use gradient descent with momentum? More specifically, can the model converge to an appropriate weight vector in fewer iterations under gradient descent with momentum? To address this question, we fit a logistic regression model using the same values for \\(\\alpha\\) and our random seed, but with the modification that \\(\\beta = 0.9\\) rather than \\(0\\).\n\n# set seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec_momentum = []\n\n# fit model\nwhile len(loss_vec_momentum) &lt; 2 or loss_vec_momentum[-2] - loss_vec_momentum[-1] &gt; 0.00001:\n\n    # update model\n    opt.step(X, y, alpha = 0.07, beta = 0.9)\n\n    # calculate and record loss\n    loss = LR.loss(X, y) \n    loss_vec_momentum.append(loss)\n\nTo identify any improvements due to gradient descent with momentum, we plot the evolution of the loss function under both scenarios on the same graph.\n\n# plot the changes in loss \nplt.plot(loss_vec_vanilla, color = \"slategrey\", label = \"Gradient Descent\")\nplt.plot(loss_vec_momentum, color = \"#A37933\", label = \"Gradient Descent with Momentum\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\")\nplt.legend(loc = \"lower left\");\n\n\n\n\n\n\n\n\nIn this scenario, the loss function of gradient descent with momentum decreased slightly more rapidly than regular gradient descent. To be fully transparent, producing a scenario with this improvement involved some fishing for appropriate data and \\(\\alpha\\). In this case, gradient descent with momentum performed slightly better than regular gradient descent, but this is by no means a guarantee.\n\n\nThe Perils of Overfitting\nIn this section, we construct a scenario where logistic regression overfits to the training data. To accomplish this task, we fit a model on data with substantially more dimensions than observations. Specifically, we generate training data and test data which both contain 30 dimensions and 20 observations, using a function for generating classification data generously provided by Professor Chodrow. Then we fit our logistic regression model on our training data, resulting in the evolution of our loss function as illustrated below.\n\n# set seed\ntorch.manual_seed(1234)\n\n# generate data\nX_train, y_train = classification_data(n_points = 20, noise = 0.5, p_dims = 30)\nX_test, y_test = classification_data(n_points = 20, noise = 0.5, p_dims = 30)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec_overfit = []\n\n# fit model\nfor _ in range(100):\n\n    # update model\n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\n    # calculate and record loss\n    loss = LR.loss(X_train, y_train) \n    loss_vec_overfit.append(loss)\n    if loss == 0:\n        break\n\n# plot the changes in loss \nplt.plot(loss_vec_overfit, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn this figure, we find that our loss function decreased in the same monotonic manner as before, converging to a decision boundary with a loss value of less than 0.1. Below, we calculate the training accuracy of our model.\n\n# Compute training accuracy\ny_hat = LR.predict(X_train)\ntrain_accuracy = (1.0*(y_hat == y_train)).mean().item()\ntrain_accuracy\n\n1.0\n\n\nAt 100% training accuracy, our model is correctly predicting the outcome variable of every observation in our training data. But what about testing accuracy?\n\n# Compute testing accuracy\ny_hat = LR.predict(X_test)\ntest_accuracy = (1.0*(y_hat == y_test)).mean().item()\ntest_accuracy\n\n0.6499999761581421\n\n\nAt roughly 65%, our testing accuracy is not nearly as high as our training accuracy. Our model appears to have been overfit to our training data, failing to generalize to test data that was generated in a similar manner. Why might this have happened?\nI suspect that 20 observations of training points is simply insufficient for training a model in 30 dimensional space. There are so many possibilities for where a point can be located in 30 dimensional space, and 20 observations barely scratches the surface of these possibilities. Our model would need substantially more training data in order to have been exposed to enough of these possibilities. In the absence of sufficient training data, our model reflects the noise of our training data rather than the underlying pattern."
  },
  {
    "objectID": "posts/logistic-regression/index.html#conclusion",
    "href": "posts/logistic-regression/index.html#conclusion",
    "title": "Implementing Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nIn this assignment, we implemented logistic regression via empirical risk minimization within the object-oriented framework provided by Professor Chodrow. We investigated the differences between regular gradient descent and gradient descent with momentum, discovering that the latter option converges to a solution more rapidly in some cases. We also illustrated that logistic regression tends to overfit when the training data has more dimensions than observations. Implementing logistic regression and experimenting with the model’s parameters furthered my understanding of logistic regression in particular and gradient descent more broadly."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html",
    "href": "posts/sparse-kernel-machines/index.html",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "",
    "text": "In this blog post, we extend our previous implementation of standard logistic regression to a kernelized setting, allowing our model to detect nonlinear decision boundaries. Specifically, we implement a sparse kernel machine using the \\(\\ell_1\\) regulizer to ensure that the vast majority of \\(\\mathbf{a}\\)’s entries are indistinguishable from zero. After implementing sparse kernelized logistic regression, we perform a few experiments to investigate the properties of the algorithm. We test different values of \\(\\lambda\\) to investigate how this parameter impacts the number of points indistinguishable from zero, we test different values of \\(\\gamma\\) to reveal how this parameter impacts the wiggliness of our decision boundary, and we test the model on nonlinear data from scikit-learn’s make_moons() function to illustrate that the model is robust to nonlinear patterns. Finally, we fit a model with a large \\(\\gamma\\)-value and compute ROC curves to reveal how this parameter may lead to overfitting. To see my implementation of sparse kernelized logistic regression, please visit sparse_kernel_logistic.py."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#abstract",
    "href": "posts/sparse-kernel-machines/index.html#abstract",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "",
    "text": "In this blog post, we extend our previous implementation of standard logistic regression to a kernelized setting, allowing our model to detect nonlinear decision boundaries. Specifically, we implement a sparse kernel machine using the \\(\\ell_1\\) regulizer to ensure that the vast majority of \\(\\mathbf{a}\\)’s entries are indistinguishable from zero. After implementing sparse kernelized logistic regression, we perform a few experiments to investigate the properties of the algorithm. We test different values of \\(\\lambda\\) to investigate how this parameter impacts the number of points indistinguishable from zero, we test different values of \\(\\gamma\\) to reveal how this parameter impacts the wiggliness of our decision boundary, and we test the model on nonlinear data from scikit-learn’s make_moons() function to illustrate that the model is robust to nonlinear patterns. Finally, we fit a model with a large \\(\\gamma\\)-value and compute ROC curves to reveal how this parameter may lead to overfitting. To see my implementation of sparse kernelized logistic regression, please visit sparse_kernel_logistic.py."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#generating-training-data",
    "href": "posts/sparse-kernel-machines/index.html#generating-training-data",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "Generating Training Data",
    "text": "Generating Training Data\nFirst, we import the packages we will need for this assignment.\n\n# import packages\n%load_ext autoreload\n%autoreload 2\nfrom sparse_kernel_logistic import KernelLogisticRegression, GradientDescentOptimizer\nimport torch \nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_moons\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\nNext, we generate the data that we will use to train our model. Thank you to Professor Chodrow for providing the functions for generating and visualizing training data.\n\n# define function for creating classification data\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n# define function for plotting classification data\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1)\n\n# create classification data\nX, y = classification_data(n_points = 100, noise = 0.4)\n\n# plot classification data\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\nplt.title(\"Training Data\");"
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#fitting-our-first-model",
    "href": "posts/sparse-kernel-machines/index.html#fitting-our-first-model",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "Fitting Our First Model",
    "text": "Fitting Our First Model\nWith our training data in hand and our implementation of kernelized logistic regression in sparse_kernel_logistic.py, we are ready to fit our first model! In our first model, we are paying particular attention to whether our output matches our expected output, hoping to confirm that our implementation is adequate and bug-free.\nIn this model and for the remainder of this blog post, we will use the Gaussian radial basis function (RBF) kernel.\n\n# define kernel\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n\n# set seed\ntorch.manual_seed(1)\n\n# create kernel logistic regression model\nKR = KernelLogisticRegression(X, rbf_kernel, lam = .1, gamma = 0.1)\nopt = GradientDescentOptimizer(KR)\n\n# fit model\nfor i in range(100000):\n    # update model\n    opt.step(X, y, alpha = 0.0001)\n\nIn sparse kernelized logistic regression, the \\(\\ell_1\\) norm is used to make the model sparse – that is, to make most entries of \\(\\mathbf{a}\\) equal to zero. Let us confirm that must of our entries for \\(\\mathbf{a}\\) are indeed close to zero.\n\n# compute proportion of coefficients distinguishable from zero\n(1.0*(torch.abs(KR.a) &gt; 0.001)).mean()\n\ntensor(0.0500)\n\n\nSuccess! Finally, let us confirm that our model is scoring points appropriately. We will not actually implement a decision threshold in this blog post, but the important point here is whether the scores of one class tend to be different from the scores of the other class.\n\n# plot results using Professor Chodrow's provided code\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG_r\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\");\n\n\n\n\n\n\n\n\nIn our output, there is a dark brown ring around most of the brown points in the lower left side of the plot. Expanding outwards, each subsequent concentric ring is less and less likely to contain brown points. These rings correspond to scores of that region of the graph, so you can imagine the model picking an appropriate score cutoff that serves as a pretty accurate decision threshold."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#basic-experiments",
    "href": "posts/sparse-kernel-machines/index.html#basic-experiments",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "Basic Experiments",
    "text": "Basic Experiments\nIn this section, we tinker with our parameter values in order to see how they impact the output of our model.\n\nExperiment 1: Adjusting \\(\\lambda\\)\nIn our first experiment, we leave all of the parameters the same as our initial model except for \\(\\lambda\\), which we increase from \\(0.1\\) to \\(0.11\\). Recall that \\(\\lambda\\) is the coefficient of the \\(\\ell_1\\) regulizer. As the \\(\\ell_1\\) regulizer is responsible for making \\(\\mathbf{a}\\) sparse, increasing the weight of this regularization term will have the effect of making \\(\\mathbf{a}\\) even more sparse.\n\n# set seed\ntorch.manual_seed(1)\n\n# create kernel logistic regression model\nKR = KernelLogisticRegression(X, rbf_kernel, lam = .11, gamma = 0.1)\nopt = GradientDescentOptimizer(KR)\n\n# fit model\nfor i in range(100000):\n    # update model\n    opt.step(X, y, alpha = 0.0001)\n\nNow that we have trained our model, let us determine how many entries of \\(\\mathbf{a}\\) are distinguishable from zero. As before, we define distinguishable from zero to mean that a coefficient’s magnitude is greater than \\(0.001\\).\n\n# calculate total points distinguishable from zero\n(1.0*(torch.abs(KR.a) &gt; 0.001)).sum()\n\ntensor(1.)\n\n\nIn this case, by increasing \\(\\lambda\\) from \\(0.1\\) to just \\(0.11\\), our model went from having \\(5\\) points with weights distinguishable from zero to having only \\(1\\).\n\n\nExperiment 2: Adjusting \\(\\gamma\\)\nIn this experiment, we use the same parameters from our initial model except for \\(\\gamma\\), which controls the bandwidth of our RBF kernel. According to scikit-learn, this means that \\(\\gamma\\) controls how far the influence of a training point reaches. Smaller \\(\\gamma\\)-values give points influence over more space, while larger \\(\\gamma\\)-values give points influence over less space. In our first example, the shape of our score contours was very smooth. When points have influence over a small amount of space, the model becomes more sensitive to individual points, so I suspect that a larger \\(\\gamma\\) value will result in wigglier score contours. To see whether this is true, I increase \\(\\gamma\\) from \\(0.1\\) to \\(3\\) below.\n\n# set seed\ntorch.manual_seed(1)\n\n# create kernel logistic regression model\nKR = KernelLogisticRegression(X, rbf_kernel, lam = .1, gamma = 3)\nopt = GradientDescentOptimizer(KR)\n\n# fit model\nfor i in range(100000):\n    # update model\n    opt.step(X, y, alpha = 0.0001)\n\n\n# plot results using Professor Chodrow's provided code\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG_r\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\");\n\n\n\n\n\n\n\n\nAs I anticipated, the score contours are less smooth and less circular. In the lower left, rather than one set of concentric rings, we now have two sets. On the right hand side, we now witness some sharper curves than those present in the concentric rings from our first model. In this manner, our results confirm our expectations that larger values of \\(\\gamma\\) result in wigglier decision boundaries.\n\n\nExperiment 3: Detecting Nonlinear Patterns\nFirst, let us use scikit-learn’s make_moons() function to generate data with a nonlinear pattern.\n\n# set seed\ntorch.manual_seed(1)\n\n# Create nonlinear training data\nmoons_X, moons_y = make_moons(n_samples=100, noise = 0.1)\n\n# Convert to torch tensors\nmoons_X = torch.tensor(moons_X, dtype = torch.float32)\nmoons_y = torch.tensor(moons_y, dtype = torch.float32)\n\n# plot data\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(moons_X, moons_y, ax)\nplt.title(\"Training Data\");\n\n\n\n\n\n\n\n\nWhile the pattern of this data is clear to the human eye, it is also clear that the curve separating the two classes of data is not a straight line. Let us see whether our kernelized logistic regression model can detect this difference.\n\n# create kernel logistic regression model\nKR = KernelLogisticRegression(moons_X, rbf_kernel, lam = .075, gamma = 4)\nopt = GradientDescentOptimizer(KR)\n\n# fit model\nfor i in range(200000):\n    # update model\n    opt.step(moons_X, moons_y, alpha = 0.0001)\n\n\n# show results\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(moons_X[:,0].min() - 0.2, moons_X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(moons_X[:,1].min() - 0.2, moons_X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(moons_X, moons_y, ax)\nplt.scatter(moons_X[ix, 0],moons_X[ix, 1], facecolors = \"none\", edgecolors = \"black\");\n\n\n\n\n\n\n\n\nIt took me some time to discover parameter values that resulted in a relatively well-fit model, but with some tuning, it appears that kernelized logistic regression can indeed find nonlinear patterns in data such as the one above. The model has not performed perfectly, but on the top left there is a curve that generally follows the shape of the upper moon, and on the bottom right there is a curve that generally follows the shape of the lower moon. The most difficult region for the model to predict the correct label appears to be where one moon almost intersects the center of the other moon. While it is not perfect, I am overall impressed by how well the model has performed. The fact that kernelized logistic regression can find nonlinear patterns represents a substantial improvement over traditional logistic regression."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#overfitting",
    "href": "posts/sparse-kernel-machines/index.html#overfitting",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "Overfitting",
    "text": "Overfitting\nIn this section, we intentionally overfit a model with a poor choice of \\(\\gamma\\) in order to demonstrate the potential issues of overfitting. First, we generate training and testing data using the same function.\n\n# set seed\ntorch.manual_seed(1)\n\n# generate training data\nX_train, y_train = classification_data(n_points = 100, noise = 0.6)\n\n# generate testing data\nX_test, y_test = classification_data(n_points = 100, noise = 0.6)\n\n# show training and testing data side by side\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nplot_classification_data(X_train, y_train, ax[0])\nax[0].set_title(\"Training Data\")\nplot_classification_data(X_test, y_test, ax[1])\nax[1].set_title(\"Testing Data\");\n\n\n\n\n\n\n\n\nBecause we used Professor Chodrow’s classification_data() function for generating both datasets, we know that the training and testing data have the same underlying pattern. By inspecting the graphs above, it is obvious that one class tends to be located in the lower left of the graph, while the other class tends to be located in the upper right. By introducing noise into the generation of both datasets, we ensured that the two datasets are not identical. Now that we have our data, we train our model and inspect its score contours.\n\n# train model\nKR = KernelLogisticRegression(X_train, rbf_kernel, lam = .065, gamma = 50)\nopt = GradientDescentOptimizer(KR)\n\n# fit model\nfor i in range(100000):\n    # update model\n    opt.step(X_train, y_train, alpha = 0.0001)\n    \n# show results\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X_train[:,0].min() - 0.2, X_train[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X_train[:,1].min() - 0.2, X_train[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1,1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG_r\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X_train, y_train, ax)\nax.set_title(\"Training Data and Score Contours\");\n\n\n\n\n\n\n\n\nVisually, the score contours appear to reflect the individual data points relatively well. The background is blue, with a region of brown surrounding groups of and occasionally individual brown points. However, the underlying pattern used to generate this data is a roughly linear boundary between the upper right and lower left. These somewhat wiggly contours may reflect the noise of our training data more than the overall pattern in the data.\nLet us evaluate the performance of our model on our training data and testing data by computing ROC curves. Thank you to Professor Chodrow for providing some code for creating ROC curves in his lecture notes.\n\n# roc curve training data\n\n# store min and max scores\n    # train\ns_train = KR.score(X_train)\nscore_min_train = s_train.min()\nscore_max_train = s_train.max()\n    # test\ns_test = KR.score(X_test)\nscore_min_test = s_test.min()\nscore_max_test = s_test.max()\n\n# create vectors for TPR and FPR\nnum_thresholds = 101\n    #train\nFPR_train = np.zeros(num_thresholds)\nTPR_train = np.zeros(num_thresholds)\nT_train = np.linspace(score_min_train, score_max_train, num_thresholds)\n    # test\nFPR_test = np.zeros(num_thresholds)\nTPR_test = np.zeros(num_thresholds)\nT_test = np.linspace(score_min_test, score_max_test, num_thresholds)\n\n# calculate TPR and TNR\n    # train\nfor i in range(num_thresholds):\n    t = T_train[i]\n    preds    = s_train &lt;= t\n    FPR_train[i]   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR_train[i]   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n    # test\nfor i in range(num_thresholds):\n    t = T_test[i]\n    preds    = s_test &lt;= t\n    FPR_test[i]   = ((preds == 1) & (y_test == 0)).sum() / (y_test == 0).sum()\n    TPR_test[i]   = ((preds == 1) & (y_test == 1)).sum() / (y_test == 1).sum()\n\n# create figure\nfig, ax = plt.subplots(1, 1)\n\nax.plot(FPR_train, TPR_train, color = \"black\", label = \"Training ROC\")\nax.plot([0,1], [0,1], linestyle=\"--\", color = \"grey\")\nax.plot(FPR_test, TPR_test, color = \"red\", label = \"Testing ROC\")\nax.set_aspect('equal')\nplt.legend(loc = \"lower right\")\nlabs = ax.set(xlabel = \"False Positive Rate\", ylabel = \"True Positive Rate\", title = \"ROC Curve\")\n\n\n\n\n\n\n\n\nRecall that a perfect ROC curve reaches the top left corner of this graph with a false positive rate of 0 and a true positive rate of 1. Because data is rarely perfect, models rarely achieve this goal. Rather, the effectiveness of a model is represented by how close a model comes to achieving that. In our case, the training ROC curve is substantially above and to the left of the testing ROC curve, indicating that our model does not generalize well to the testing data. In fact, the distance between the training and testing ROC curves is roughly the same as the distance between the testing curve and the diagonal line that represents a model that randomly guesses at the classes. In this manner, it is clear that our model performs dramatically better on our training data than on our testing data. Because \\(\\gamma\\) is so large, the score contours reflect the noise in our data more than the underlying pattern of our data, leading our model to ineffectively generalize to new data."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#conclusion",
    "href": "posts/sparse-kernel-machines/index.html#conclusion",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we extended our work with logistic regression to a new situation: nonlinearity. In our prior work, we implemented standard logistic regression and performed experiments with several optimizers, including standard gradient descent, gradient descent with momentum, and Newton’s method. In this assignment, rather than searching for more efficient optimization algorithms, we implemented a kernelized model that can find a wide array of nonlinear patterns. We performed several experiments in order to deepen our understanding of the model, discovering that larger values of \\(\\lambda\\) result in fewer training points with non-zero coefficients, larger values of \\(\\gamma\\) result in wigglier decision boundaries, and these large \\(\\gamma\\) values can often lead to overfitting. We also illustrated that our model can find nonlinear decision boundaries by fitting our model on data created with scikit-learn’s make_moons() function. Overall, this assignment allowed me to learn the theory behind and implementation of an intruiging variation of an otherwise familiar machine learning algorithm."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "In this blog post, we implement the perceptron, one of the first machine learning models ever invented, and perform a few experiments to highlight the model’s strengths and weaknesses. The perceptron algorithm is used to predict a binary outcome on data with a continuous and finite-dimensional feature space. Through our experiments, we visually illustrate that perceptron converges to a solution that completely differentiates between the outcome classes on linearly separable data. However, the algorithm does not converge to a solution on non-linearly separable data. We then implement a generalization of perceptron known as minibatch perceptron, which uses \\(k\\) observations in each iteration rather than \\(1\\). We perform experiments that illustrate that minibatch perceptron behaves similarly to perceptron when \\(k = 1\\) and continues to find a decision boundary on linearly separable data as we increase \\(k\\) towards \\(n\\). Finally, we show that when \\(k = n\\), minibatch perceptron can converge to a solution on non-linearly separable data, although this solution will not correctly classify every data point. To see my implementation of the two algorithms, please visit perceptron.py and minibatch_perceptron.py."
  },
  {
    "objectID": "posts/perceptron/index.html#abstract",
    "href": "posts/perceptron/index.html#abstract",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "In this blog post, we implement the perceptron, one of the first machine learning models ever invented, and perform a few experiments to highlight the model’s strengths and weaknesses. The perceptron algorithm is used to predict a binary outcome on data with a continuous and finite-dimensional feature space. Through our experiments, we visually illustrate that perceptron converges to a solution that completely differentiates between the outcome classes on linearly separable data. However, the algorithm does not converge to a solution on non-linearly separable data. We then implement a generalization of perceptron known as minibatch perceptron, which uses \\(k\\) observations in each iteration rather than \\(1\\). We perform experiments that illustrate that minibatch perceptron behaves similarly to perceptron when \\(k = 1\\) and continues to find a decision boundary on linearly separable data as we increase \\(k\\) towards \\(n\\). Finally, we show that when \\(k = n\\), minibatch perceptron can converge to a solution on non-linearly separable data, although this solution will not correctly classify every data point. To see my implementation of the two algorithms, please visit perceptron.py and minibatch_perceptron.py."
  },
  {
    "objectID": "posts/perceptron/index.html#part-a-implementing-perceptron",
    "href": "posts/perceptron/index.html#part-a-implementing-perceptron",
    "title": "Implementing Perceptron",
    "section": "Part A: Implementing Perceptron",
    "text": "Part A: Implementing Perceptron\nRecall from Professor Chodrow’s lecture notes that the perceptron algorithm involves the following process:\n\nRandomly select an initial decision boundary \\(\\mathbf{w}^{(0)}\\).\nIteratively:\n\nPick a random integer \\(i \\in \\{1,\\ldots,n\\}\\).\nCompute the score for the point \\(i\\): \\(s_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\).\nUpdate the decision boundary: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbb{1} [s_i y_i &lt; 0] y_i \\mathbf{x}_i\\)\n\n\nI implement this entire process in perceptron.py, but for the sake of brevity, I will not discuss every line of code in this blog post. Instead, I focus exclusively on my implementation of the grad() function, which calculates \\(\\mathbb{1} [s_i y_i &lt; 0] y_i \\mathbf{x}_i\\).\n\n# grad() function from perceptron.py\ndef grad(self, X, y):\n        s = X@self.w\n        return (s*y &lt; 0)*X*y\n\nBy taking advantage of our knowledge of linear algebra, the implementation of grad() becomes quite short! In the first line of grad(), we calculate the inner product \\(s_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\). The tensors X and self.w are shaped appropriately for us to compute this inner product with torch’s @ operator. In the second and final line of grad(), we use our result from the first line to calculate \\([s_i y_i &lt; 0] y_i \\mathbf{x}_i\\).\nTo verify that our implementation of perceptron was succussful, we run the “minimal training loop” provided in the assignment instructions. First, we need data to run the loop on, so we use some code generously provided by Professor Chodrow to generate and display our linearly separable data.\n\n# import packages\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom minibatch_perceptron import MinibatchPerceptron, MinibatchPerceptronOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nimport ipywidgets as wdg\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n# define function to create data\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n# define function to plot data\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# create linearly separable data\nX_ls, y_ls = perceptron_data(n_points = 50, noise = 0.3)\n\n# plot linearly separable data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X_ls, y_ls, ax)\nax.set_title(\"Our Linearly Separable Data\");\n\n\n\n\n\n\n\n\nVisually, it is clear that one could draw a line between the two different colors of points, so our data is linearly separable. This is important, because the perceptron algorithm will only converge to a solution with a loss of zero on linearly separable data. Since we have our data, we are now prepared to run the minimal training loop.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_ls, y_ls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_ls, y_ls)\n\nThe minimal training loop terminates, so our model must have converged to a solution with a loss of 0."
  },
  {
    "objectID": "posts/perceptron/index.html#part-b-fitting-and-evaluating-perceptron-models",
    "href": "posts/perceptron/index.html#part-b-fitting-and-evaluating-perceptron-models",
    "title": "Implementing Perceptron",
    "section": "Part B: Fitting and Evaluating Perceptron Models",
    "text": "Part B: Fitting and Evaluating Perceptron Models\nNow that we have a functional implementation of perceptron, we perform experiments and generate illustrations to reveal the strengths and weaknesses of the perceptron model.\n\nPart B.1: Evolution of the Model on Linearly Separable Data\nFirst, we illustrate how the loss function changes between iterations of the minimal training loop.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nAs you can see, with our data and our randomness seed, the perceptron algorithm converges to a solution with zero loss after 49 iterations. Notice that throughout this process, the weight vector only changes 6 times. In all 43 other iterations, the randomly selected point was correctly classified by the model at that stage.\nTo gain some insight as to the location and orientation of the decision boundary throughout this process, we plot the changes to the weight vector in the figure below. We include a subplot for every change to the weight vector, letting the dashed lines represent the previous weight vector and the solid lines represent the current weight vector. In each subplot, the circled point corresponds to the point \\(i\\) that was misclassified, leading to an update to the weight vector.\nThank you to Professor Chodrow for providing the code for creating this visualization.\n\n# define line function\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# set seed\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X_ls, y_ls)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i, local_loss = opt.step(X_ls, y_ls)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X_ls, y_ls, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X_ls, y_ls).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X_ls[i,0],X_ls[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y_ls[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThese figures illustrate how the model responds to the random selection of an incorrectly classified point. Generally, the correction slightly overcompensates for the misclassified point, leading the decision boundary to fluctuate between which class it incorrectly classifies, gradually adjusting until it classifies all points correctly.\n\n\nPart B.2: Evolution of the Model on Non-Linearly Separable Data\nUnfortunately, the perceptron algorithm will not converge to a decision boundary on data that is not linearly separable. To illustrate this, we first need to create data that cannot be perfectly divided using one separating line.\nTo create our non-linearly separable data, we use the same method as before but increase the amount of noise. As you can see in the graph below, the two classes have tendencies towards different regions in our resulting data, but it is impossible to draw a straight line that perfectly separates them.\n\n# set seed\ntorch.manual_seed(1234)\n\n# create non-linearly separable data\nX_nls, y_nls = perceptron_data(n_points = 50, noise = 0.8)\n\n# plot non-linearly separable data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\nax.set_title(\"Our Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nNow that we have created non-linearly separable data, we fit the perceptron model on our data. Since the perceptron will not converge to a solution on our data, we modify our code so the model terminates after 1000 iterations.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\n# for recording iteration number\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_nls, y_nls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_nls, y_nls)\n\n    # update iter\n    iter += 1\n\n    # maxiter condition\n    if iter &gt;= 1000:\n        break\n\nAs before, we begin by inspecting the change in loss over the iterations of our algorithm.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn the figure above, our model clearly does not converge to a solution. Rather, the perceptron algorithm is so sensitive to the individual data point under consideration at any given moment that the loss fluctuates wildly throughout the entire process, ranging from less than 0.1 to more than 0.7.\nIn the linearly separable case, we visually inspected every change to the weight vector, but because there are so many changes to the model in this case, it would be burdensome to inspect every change. Instead, we display the decision boundary in the final iteration of our model.\n\n# Plot final decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\ndraw_line(p.w, x_min = -1.5, x_max = 1.5, ax = ax, color = \"black\")\nax.set_title(\"Decision Boundary on Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nThe decision boundary could be worse, but it is far from perfect. Perceptron’s inadequate performance on non-linearly separable data is one of its major weaknesses.\n\n\nPart B.3: A 5-Dimensional Example\nUp until this point, we have only trained perceptron models on 2-dimensional data. Working with 2-dimensional data facilitates understanding and communication surrounding the model, as we can easily visualize data in two dimensions. However, we wrote our model to work in any finite number of dimensions. To demonstrate that our implementation works on multidimensional data, we now fit a perceptron model on data with 5 features.\n\n# set seed\ntorch.manual_seed(1234)\n\n# create data\nX_5d, y_5d = perceptron_data(n_points = 50, noise = 0.3, p_dims = 5)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\n# for recording iteration number\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_5d, y_5d) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_5d, y_5d)\n\n    iter += 1\n    if iter &gt;= 1000:\n        break\n\nWhile we cannot visualize our 5 dimensional data and weight vector, we can inspect the changes to our loss function over time using the plot below.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn the figure above, it appears that the perceptron algorithm terminated after 60 iterations, achieving a loss value of zero. The perceptron algorithm terminates if and only if its training data is linearly separable; thus, our data must be linearly separable."
  },
  {
    "objectID": "posts/perceptron/index.html#part-c-minibatch-perceptron",
    "href": "posts/perceptron/index.html#part-c-minibatch-perceptron",
    "title": "Implementing Perceptron",
    "section": "Part C: Minibatch Perceptron",
    "text": "Part C: Minibatch Perceptron\nIn this section, I implemented an extension to perceptron known as minibatch perceptron. The minibatch perceptron algorithm differs from the perceptron algorithm in that instead of updating the decision boundary using \\(1\\) random point, it updates the decision boundary using \\(k\\) random points. In more detail, the algorithm involves the following process:\n\nRandomly select an initial decision boundary \\(\\mathbf{w}^{(0)}\\).\nIteratively:\n\nSample \\(k\\) random integers \\(i_1, i_2, ..., i_k \\in \\{1,\\ldots,n\\}\\) without replacement.\nUpdate the decision boundary: \\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{\\alpha}{k} \\sum_{j=1}^k \\mathbb{1} [\\langle \\mathbf{w}^{(t)}, \\mathbf{x}_{i_j} \\rangle y_{i_j} &lt; 0] y_{i_j} \\mathbf{x}_{i_j} \\]\n\n\nYou can view my implementation of the algorithm at minibatch_perceptron.py, but I will not discuss all of the implementation details here. From the user’s perspective, fitting a minibatch perceptron model is exactly the same as fitting the perceptron model, with the addition of two parameters: \\(k\\) and \\(\\alpha\\). The \\(k\\) parameter simply refers to the number of points used in each iteration, while the \\(\\alpha\\) parameter is a learning rate that affects how much \\(\\mathbf{w}\\) changes when updated.\nIn the rest of this section, we perform a few experiments to illustrate the functionality of our algorithm and its similarities and differences in comparison to the regular perceptron algorithm. The experiments we perform in parts C.1, C.2, and C.3 all involve relatively similar operations. To reduce the volume of code in each section, we define the experiment() function below.\n\ndef experiment(X, y, k, alpha):  \n    # set seed\n    torch.manual_seed(1234567)\n\n    # instantiate a model and an optimizer\n    mb_p = MinibatchPerceptron() \n    mb_opt = MinibatchPerceptronOptimizer(mb_p)\n\n    # define loss variable\n    mb_loss = 1.0\n\n    # for keeping track of loss values\n    mb_loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while mb_loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        mb_loss = mb_p.loss(X, y) \n        mb_loss_vec.append(mb_loss)\n        \n        # perform a perceptron update using the random data point\n        mb_opt.step(X, y, k, alpha)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    # set seed\n    torch.manual_seed(1234567)\n\n    # instantiate a model and an optimizer\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    # define loss variable\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # perform a perceptron update using the random data point\n        opt.step(X, y)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    return loss_vec, mb_loss_vec, p.w, mb_p.w\n\n\nPart C.1: When k = 1\nTo illustrate the differences between the perceptron and minibatch perceptron algorithms, we begin by fitting the regular perceptron and the minibatch perceptron with \\(k = \\alpha = 1\\) on our linearly separable data.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_ls, y = y_ls, k = 1, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels \nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 1, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nThe two algorithms generated incredibly similar results! In fact, if the random point considered at every iteration had been the same, these graphs would be identical. In the regular perceptron algorithm, we generate a random number between \\(0\\) and \\(n\\), but in the minibatch perceptron algorithm, we generate a random permutation of the numbers \\(0\\) through \\(n-1\\) and select the first element of that permutation. This difference appears to result in different points being considered at each iteration, explaining why these graphs differ slightly.\n\n\nPart C.2: When k = 10\nOne advantage of the minibatch perceptron algorithm is that we can tune our model to different values of \\(k\\) and \\(\\alpha\\). As our first adjustment, let’s try changing \\(k\\) to \\(10\\).\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_ls, y = y_ls, k = 10, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 10, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nIt appears that this adjustment allowed our model to converge to a decision boundary in far fewer iterations! While our regular perceptron model requires 49 iterations to achieve perfect classification, the minibatch model with \\(k = 10\\) requires only 8 iterations.\n\n\nPart C.3: Convergence on Non-Linearly Separable Data\nAnother advantage of the minibatch perceptron model is that it can converge to a decision threshold on data that is not linearly separable. This does not mean that the model will perfectly classify every data point, but rather that the model will converge to a decision boundary with a loss value that is less than 0.5.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_nls, y = y_nls, k = 50, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 50, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nOh my! That certainly didn’t work. The minibatch perceptron model seems to have an even more chaotic loss function than the regular one! This is why we have the hyperparameter \\(\\alpha\\). Let’s try decreasing \\(\\alpha\\) to 0.01.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_nls, y = y_nls, k = 50, alpha = 0.01)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 50, \\\\alpha = 0.01$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nPhew, that looks much better! Whereas the loss function of the regular perceptron algorithm would oscillate for an eternity, the loss function of the minibatch algorithm converged to a value between 0.1 and 0.15 by the 600th iteration. Let’s take a look at the resulting decision boundary.\n\n# Plot final decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\ndraw_line(mb_p_w, x_min = -1.5, x_max = 2.5, ax = ax, color = \"black\")\n# ax.set(xlim = (-1, 2), ylim = (-1, 2))\nax.set_title(\"Decision Boundary on Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nThis decision boundary looks great! Obviously, it is impossible to perfectly separate the two categories of data in this example with a straight line. The decision boundary that our minibatch perceptron algorithm converged to appears to be about as good as we can do."
  },
  {
    "objectID": "posts/perceptron/index.html#part-d-runtime-complexity",
    "href": "posts/perceptron/index.html#part-d-runtime-complexity",
    "title": "Implementing Perceptron",
    "section": "Part D: Runtime Complexity",
    "text": "Part D: Runtime Complexity\nWhat is the runtime complexity of perceptron and minibatch perceptron? To answer this question, let us consider the three lines of code that we run in each iteration of the model:\nloss = p.loss(X, y) \nloss_vec.append(loss)\nopt.step(X, y)\nThe first two lines store a record of the loss function for tracking purposes. They are not actually required for fitting the perceptron, so we exclude them from our consideration of the algorithm’s runtime complexity. Thus evaluating the runtime complexity of a single iteration of perceptron amounts to evaluating the runtime complexity of opt.step(X, y).\nIn our implementation of perceptron (see perceptron.py), opt.step() involves the following operations:\n\nn = X.size()[0]: determines the number of rows in X in constant time\ni = torch.randint(n, size = (1,)): selects a random integer in constant time\nx_i = X[[i],:] and y_i = y[i]: subsets X and y in constant time\ncurrent_loss = self.model.loss(X, y): calculates the current loss, which involves a dot product between two \\(1 \\times p\\) vectors, an operation with linear time complexity O(\\(p\\))\nself.model.w += torch.reshape(self.model.grad(x_i, y_i),(self.model.w.size()[0],)): updates \\(\\mathbf{w}\\) in linear time O(\\(p\\)) due to a dot product in the grad function\nnew_loss = self.model.loss(X, y): calculates the updated loss in linear time O(\\(p\\)) due to a dot product\nreturn i, abs(current_loss - new_loss): returns values for visualization in constant time\n\nSince the operations with the largest time complexity were O(\\(p\\)), the overall runtime complexity of a single iteration of the perceptron algorithm is O(\\(p\\)). This runtime depends on the number of features \\(p\\) rather than the number of observations \\(n\\), because the dot product is between the weight vector \\(\\mathbf{w}\\) and a single row of \\(X\\).\nIn our implementation of minibatch perceptron (see minibatch_perceptron.py), opt.step() involves largely similar operations. The only difference in runtime occurs in the grad() function. Whereas in the regular perceptron, this involved computing a single dot product between two \\(1 \\times p\\) vectors, in the minibatch perceptron, this involves computing a matrix product between \\(X^{k \\times p}\\) and \\(w^{p \\times 1}\\). Calculating this matrix product is equivalent to calculating \\(k\\) dot products, each of which are O(\\(p\\)). Thus the runtime of the matrix product, and therefore the big O runtime of the minibatch perceptron algorithm, is O(\\(kp\\))."
  },
  {
    "objectID": "posts/perceptron/index.html#conclusion",
    "href": "posts/perceptron/index.html#conclusion",
    "title": "Implementing Perceptron",
    "section": "Conclusion",
    "text": "Conclusion\nIn this assignment, we investigated the perceptron algorithm, implementing it from scratch within the object-oriented framework provided by Professor Chodrow. We illustrated that the algorithm converges to a solution on linearly separable data of any finite number of dimensions but fails to converge to a solution on non-linearly separable data. To address this shortcoming, we implemented the minibatch perceptron algorithm. We found that when \\(k = 1\\), minibatch perceptron is similar to regular perceptron, and as we increase \\(k\\), the algorithm continues to find decision boundaries on linearly separable data. Furthermore, we discovered that when \\(k = n\\) and our learning rate \\(\\alpha\\) is adjusted appropriately, minibatch perceptron can converge to a (albeit imperfect) solution on data that is not linearly separable. Overall, this assignment provided me with a great opportunity to write up my first machine learning model from scratch and investigate its strengths and weaknesses."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Final Project\n\n\n\n\n\nPredicting Population Density with Land Cover Using Linear Regression and Spatial Autoregression\n\n\n\n\n\nMay 17, 2024\n\n\nManuel Fors, Liam Smith, Yide (Alex) Xu\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Kernelized Logistic Regression\n\n\n\n\n\nExtending logistic regression to a sparse version that detects nonlinear decision boundaries.\n\n\n\n\n\nMay 5, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nGeoAI at the American Association of Geographers’ 2024 Annual Meeting\n\n\n\n\n\nModern research applications of machine learning and deep learning in remote sensing.\n\n\n\n\n\nApr 29, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method for Logistic Regression\n\n\n\n\n\nA comparison between two optimization algorithms for logistic regression.\n\n\n\n\n\nApr 21, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nImplementing logistic regression using gradient descent with momentum.\n\n\n\n\n\nApr 3, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\nImplementing one of the oldest machine learning models from scratch.\n\n\n\n\n\nApr 1, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nBlack box classification with the Palmer Penguins dataset.\n\n\n\n\n\nMar 7, 2024\n\n\nLiam Smith\n\n\n\n\n\n\nNo matching items"
  }
]